{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LISS Panel Data Loading\n",
    "\n",
    "This notebook loads all CSV files from the current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 CSV files (excluding intermediate results):\n",
      "  - Family and Household wave 16_23p_EN_1.0p.csv\n",
      "  - SHAP_Feature_Importance.csv\n",
      "  - social interation and leisure wave 16_23p_EN_1.0p.csv\n",
      "  - LISS_Trimmed_76_Features.csv\n",
      "  - Health wave 16_23p_EN_1.0p.csv\n",
      "  - Background Variables wave 16_202307_EN_1.0p.csv\n",
      "  - Work and Schooling wave 17_24q_EN_1.0p.csv\n",
      "  - LISS_Final_Y_M2.csv\n",
      "  - LISS_Final_Y_M3.csv\n",
      "  - LISS_Final_Y_M1.csv\n",
      "  - Personality wave 16_24p_EN_1.0p.csv\n",
      "  - income wave 16_23p_EN_1.0p.csv\n",
      "  - LISS_Final_Y_M4.csv\n",
      "  - LISS_Final_Y_M5.csv\n",
      "  - RQ1_Incremental_Contribution_Analysis.csv\n",
      "  - Rubins_Rule_SHAP_Importance_Detailed.csv\n",
      "  - income wave 17_24q_EN_1.0p.csv\n",
      "  - Health wave 17_24q_EN_1.0p.csv\n",
      "  - LISS_Final_X_M1.csv\n",
      "  - LISS_Final_X_M3.csv\n",
      "  - social interation and leisure wave 17_24q_EN_1.0p.csv\n",
      "  - LISS_Final_X_M2.csv\n",
      "  - Work and Schooling wave 16_23p_EN_1.0p.csv\n",
      "  - Rubins_Rule_SHAP_Importance.csv\n",
      "  - Subgroup_Error_Analysis_Results.csv\n",
      "  - LISS_Final_X_M5.csv\n",
      "  - LISS_Final_X_M4.csv\n",
      "  - Background Variables wave 17_202407_EN_1.0p.csv\n",
      "\n",
      "Excluded intermediate result files (4):\n",
      "  - merged_liss_data.csv\n",
      "  - LISS_Final_Features_X.csv\n",
      "  - LISS_Final_Features_X_with_ID.csv\n",
      "  - LISS_Final_Features_Y.csv\n",
      "\n",
      "Loading Family and Household wave 16_23p_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 5206 rows, 402 columns\n",
      "\n",
      "Loading SHAP_Feature_Importance.csv...\n",
      "  [OK] Loaded successfully: 22 rows, 3 columns\n",
      "\n",
      "Loading social interation and leisure wave 16_23p_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 5991 rows, 450 columns\n",
      "\n",
      "Loading LISS_Trimmed_76_Features.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 77 columns\n",
      "\n",
      "Loading Health wave 16_23p_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 6058 rows, 209 columns\n",
      "\n",
      "Loading Background Variables wave 16_202307_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 8626 rows, 34 columns\n",
      "\n",
      "Loading Work and Schooling wave 17_24q_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 5792 rows, 422 columns\n",
      "\n",
      "Loading LISS_Final_Y_M2.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 2 columns\n",
      "\n",
      "Loading LISS_Final_Y_M3.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 2 columns\n",
      "\n",
      "Loading LISS_Final_Y_M1.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 2 columns\n",
      "\n",
      "Loading Personality wave 16_24p_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 5477 rows, 159 columns\n",
      "\n",
      "Loading income wave 16_23p_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 4905 rows, 211 columns\n",
      "\n",
      "Loading LISS_Final_Y_M4.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 2 columns\n",
      "\n",
      "Loading LISS_Final_Y_M5.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 2 columns\n",
      "\n",
      "Loading RQ1_Incremental_Contribution_Analysis.csv...\n",
      "  [OK] Loaded successfully: 4 rows, 6 columns\n",
      "\n",
      "Loading Rubins_Rule_SHAP_Importance_Detailed.csv...\n",
      "  [OK] Loaded successfully: 10 rows, 7 columns\n",
      "\n",
      "Loading income wave 17_24q_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 5342 rows, 210 columns\n",
      "\n",
      "Loading Health wave 17_24q_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 5521 rows, 215 columns\n",
      "\n",
      "Loading LISS_Final_X_M1.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 23 columns\n",
      "\n",
      "Loading LISS_Final_X_M3.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 23 columns\n",
      "\n",
      "Loading social interation and leisure wave 17_24q_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 5581 rows, 450 columns\n",
      "\n",
      "Loading LISS_Final_X_M2.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 23 columns\n",
      "\n",
      "Loading Work and Schooling wave 16_23p_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 5139 rows, 422 columns\n",
      "\n",
      "Loading Rubins_Rule_SHAP_Importance.csv...\n",
      "  [OK] Loaded successfully: 10 rows, 4 columns\n",
      "\n",
      "Loading Subgroup_Error_Analysis_Results.csv...\n",
      "  [OK] Loaded successfully: 11 rows, 7 columns\n",
      "\n",
      "Loading LISS_Final_X_M5.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 23 columns\n",
      "\n",
      "Loading LISS_Final_X_M4.csv...\n",
      "  [OK] Loaded successfully: 12939 rows, 23 columns\n",
      "\n",
      "Loading Background Variables wave 17_202407_EN_1.0p.csv...\n",
      "  [OK] Loaded successfully (sep=';', encoding='utf-8'): 11700 rows, 34 columns\n",
      "\n",
      "============================================================\n",
      "Summary: Successfully loaded 28 dataframes\n",
      "============================================================\n",
      "\n",
      "Family and Household wave 16_23p_EN_1.0p:\n",
      "  Shape: (5206, 402)\n",
      "  Columns: ['nomem_encr', 'cf23p_m', 'cf23p001', 'cf23p002', 'cf23p536']...\n",
      "\n",
      "SHAP_Feature_Importance:\n",
      "  Shape: (22, 3)\n",
      "  Columns: ['Rank', 'Feature', 'Mean_Abs_SHAP']\n",
      "\n",
      "social interation and leisure wave 16_23p_EN_1.0p:\n",
      "  Shape: (5991, 450)\n",
      "  Columns: ['nomem_encr', 'cs23p_m', 'cs23p001', 'cs23p004', 'cs23p005']...\n",
      "\n",
      "LISS_Trimmed_76_Features:\n",
      "  Shape: (12939, 77)\n",
      "  Columns: ['Unnamed: 0', 'cp24p020', 'cp24p021', 'cp24p022', 'cp24p023']...\n",
      "\n",
      "Health wave 16_23p_EN_1.0p:\n",
      "  Shape: (6058, 209)\n",
      "  Columns: ['nomem_encr', 'ch23p_m', 'ch23p001', 'ch23p002', 'ch23p003']...\n",
      "\n",
      "Background Variables wave 16_202307_EN_1.0p:\n",
      "  Shape: (8626, 34)\n",
      "  Columns: ['nomem_encr', 'nohouse_encr', 'wave', 'geslacht', 'gender']...\n",
      "\n",
      "Work and Schooling wave 17_24q_EN_1.0p:\n",
      "  Shape: (5792, 422)\n",
      "  Columns: ['nomem_encr', 'cw24q_m', 'cw24q000', 'cw24q525', 'cw24q002']...\n",
      "\n",
      "LISS_Final_Y_M2:\n",
      "  Shape: (12939, 2)\n",
      "  Columns: ['Unnamed: 0', 'Delta_JobSatisfaction']\n",
      "\n",
      "LISS_Final_Y_M3:\n",
      "  Shape: (12939, 2)\n",
      "  Columns: ['Unnamed: 0', 'Delta_JobSatisfaction']\n",
      "\n",
      "LISS_Final_Y_M1:\n",
      "  Shape: (12939, 2)\n",
      "  Columns: ['Unnamed: 0', 'Delta_JobSatisfaction']\n",
      "\n",
      "Personality wave 16_24p_EN_1.0p:\n",
      "  Shape: (5477, 159)\n",
      "  Columns: ['nomem_encr', 'cp24p_m', 'cp24p010', 'cp24p011', 'cp24p012']...\n",
      "\n",
      "income wave 16_23p_EN_1.0p:\n",
      "  Shape: (4905, 211)\n",
      "  Columns: ['nomem_encr', 'ci23p_m', 'ci23p001', 'ci23p002', 'ci23p326']...\n",
      "\n",
      "LISS_Final_Y_M4:\n",
      "  Shape: (12939, 2)\n",
      "  Columns: ['Unnamed: 0', 'Delta_JobSatisfaction']\n",
      "\n",
      "LISS_Final_Y_M5:\n",
      "  Shape: (12939, 2)\n",
      "  Columns: ['Unnamed: 0', 'Delta_JobSatisfaction']\n",
      "\n",
      "RQ1_Incremental_Contribution_Analysis:\n",
      "  Shape: (4, 6)\n",
      "  Columns: ['Model Block', 'Included Features', 'Cumulative R² (Pooled Mean)', 'Cumulative R² (Std)', 'New ΔR² (Contribution)']...\n",
      "\n",
      "Rubins_Rule_SHAP_Importance_Detailed:\n",
      "  Shape: (10, 7)\n",
      "  Columns: ['Feature', 'Q_bar', 'SE_Pooled', 'U_bar', 'B']...\n",
      "\n",
      "income wave 17_24q_EN_1.0p:\n",
      "  Shape: (5342, 210)\n",
      "  Columns: ['nomem_encr', 'ci24q_m', 'ci24q001', 'ci24q002', 'ci24q326']...\n",
      "\n",
      "Health wave 17_24q_EN_1.0p:\n",
      "  Shape: (5521, 215)\n",
      "  Columns: ['nomem_encr', 'ch24q_m', 'ch24q001', 'ch24q002', 'ch24q003']...\n",
      "\n",
      "LISS_Final_X_M1:\n",
      "  Shape: (12939, 23)\n",
      "  Columns: ['Unnamed: 0', 'N_Score', 'E_Score', 'C_Score', 'A_Score']...\n",
      "\n",
      "LISS_Final_X_M3:\n",
      "  Shape: (12939, 23)\n",
      "  Columns: ['Unnamed: 0', 'N_Score', 'E_Score', 'C_Score', 'A_Score']...\n",
      "\n",
      "social interation and leisure wave 17_24q_EN_1.0p:\n",
      "  Shape: (5581, 450)\n",
      "  Columns: ['nomem_encr', 'cs24q_m', 'cs24q001', 'cs24q004', 'cs24q005']...\n",
      "\n",
      "LISS_Final_X_M2:\n",
      "  Shape: (12939, 23)\n",
      "  Columns: ['Unnamed: 0', 'N_Score', 'E_Score', 'C_Score', 'A_Score']...\n",
      "\n",
      "Work and Schooling wave 16_23p_EN_1.0p:\n",
      "  Shape: (5139, 422)\n",
      "  Columns: ['nomem_encr', 'cw23p_m', 'cw23p000', 'cw23p525', 'cw23p002']...\n",
      "\n",
      "Rubins_Rule_SHAP_Importance:\n",
      "  Shape: (10, 4)\n",
      "  Columns: ['Rank', 'Feature', 'Pooled Mean Abs SHAP', 'Pooled SE']\n",
      "\n",
      "Subgroup_Error_Analysis_Results:\n",
      "  Shape: (11, 7)\n",
      "  Columns: ['Category', 'Subgroup', 'MAE', 'MAE_Std', 'N_Evaluations']...\n",
      "\n",
      "LISS_Final_X_M5:\n",
      "  Shape: (12939, 23)\n",
      "  Columns: ['Unnamed: 0', 'N_Score', 'E_Score', 'C_Score', 'A_Score']...\n",
      "\n",
      "LISS_Final_X_M4:\n",
      "  Shape: (12939, 23)\n",
      "  Columns: ['Unnamed: 0', 'N_Score', 'E_Score', 'C_Score', 'A_Score']...\n",
      "\n",
      "Background Variables wave 17_202407_EN_1.0p:\n",
      "  Shape: (11700, 34)\n",
      "  Columns: ['nomem_encr', 'nohouse_encr', 'wave', 'geslacht', 'gender']...\n"
     ]
    }
   ],
   "source": [
    "# Cell 1:  Load All CSV Files\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Get current directory\n",
    "current_dir = Path('.')\n",
    "\n",
    "# Find all CSV files in current directory (excluding subdirectories)\n",
    "csv_files = list(current_dir.glob('*.csv'))\n",
    "\n",
    "# Exclude intermediate result files (from previous runs)\n",
    "excluded_files = [\n",
    "    'merged_liss_data.csv',\n",
    "    'LISS_Final_Features_X.csv',\n",
    "    'LISS_Final_Features_X_with_ID.csv',\n",
    "    'LISS_Final_Features_Y.csv'\n",
    "]\n",
    "\n",
    "csv_files = [f for f in csv_files if f.name not in excluded_files]\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files (excluding intermediate results):\")\n",
    "for f in csv_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "if excluded_files:\n",
    "    print(f\"\\nExcluded intermediate result files ({len(excluded_files)}):\")\n",
    "    for fname in excluded_files:\n",
    "        print(f\"  - {fname}\")\n",
    "\n",
    "# Dictionary to store all dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Load each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Use filename without extension as key\n",
    "    key = csv_file.stem\n",
    "    print(f\"\\nLoading {csv_file.name}...\")\n",
    "    \n",
    "    # Try different combinations of separator and encoding\n",
    "    loaded = False\n",
    "    \n",
    "    # First, try to detect separator by reading first line\n",
    "    try:\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            # Check if semicolon is used (common in European CSV files)\n",
    "            if ';' in first_line and first_line.count(';') > first_line.count(','):\n",
    "                sep = ';'\n",
    "            else:\n",
    "                sep = ','\n",
    "    except:\n",
    "        sep = ','  # Default to comma\n",
    "    \n",
    "    # Try different encodings and separators\n",
    "    for encoding in ['utf-8', 'latin-1']:\n",
    "        for separator in [sep, ';', ',']:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file, encoding=encoding, sep=separator, low_memory=False)\n",
    "                # Check if we got a reasonable number of columns (more than 1)\n",
    "                if df.shape[1] > 1:\n",
    "                    dataframes[key] = df\n",
    "                    sep_info = f\" (sep='{separator}', encoding='{encoding}')\" if separator != ',' or encoding != 'utf-8' else \"\"\n",
    "                    print(f\"  [OK] Loaded successfully{sep_info}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "                    loaded = True\n",
    "                    break\n",
    "                else:\n",
    "                    # If only 1 column, separator might be wrong\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if loaded:\n",
    "            break\n",
    "    \n",
    "    if not loaded:\n",
    "        print(f\"  [ERROR] Error: Could not load {csv_file.name} with any combination of separator and encoding\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Summary: Successfully loaded {len(dataframes)} dataframes\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Display summary of loaded dataframes\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Merging all dataframes using 'nomem_encr' as join key\n",
      "============================================================\n",
      "[OK] Family and Household wave 16_23p_EN_1.0p: has 'nomem_encr' column (5206 rows, 402 columns)\n",
      "[WARNING] SHAP_Feature_Importance: missing 'nomem_encr' column - will be skipped\n",
      "[OK] social interation and leisure wave 16_23p_EN_1.0p: has 'nomem_encr' column (5991 rows, 450 columns)\n",
      "[OK] LISS_Trimmed_76_Features: has 'nomem_encr' column (12939 rows, 77 columns)\n",
      "[OK] Health wave 16_23p_EN_1.0p: has 'nomem_encr' column (6058 rows, 209 columns)\n",
      "[OK] Background Variables wave 16_202307_EN_1.0p: has 'nomem_encr' column (8626 rows, 34 columns)\n",
      "[OK] Work and Schooling wave 17_24q_EN_1.0p: has 'nomem_encr' column (5792 rows, 422 columns)\n",
      "[WARNING] LISS_Final_Y_M2: missing 'nomem_encr' column - will be skipped\n",
      "[WARNING] LISS_Final_Y_M3: missing 'nomem_encr' column - will be skipped\n",
      "[WARNING] LISS_Final_Y_M1: missing 'nomem_encr' column - will be skipped\n",
      "[OK] Personality wave 16_24p_EN_1.0p: has 'nomem_encr' column (5477 rows, 159 columns)\n",
      "[OK] income wave 16_23p_EN_1.0p: has 'nomem_encr' column (4905 rows, 211 columns)\n",
      "[WARNING] LISS_Final_Y_M4: missing 'nomem_encr' column - will be skipped\n",
      "[WARNING] LISS_Final_Y_M5: missing 'nomem_encr' column - will be skipped\n",
      "[WARNING] RQ1_Incremental_Contribution_Analysis: missing 'nomem_encr' column - will be skipped\n",
      "[WARNING] Rubins_Rule_SHAP_Importance_Detailed: missing 'nomem_encr' column - will be skipped\n",
      "[OK] income wave 17_24q_EN_1.0p: has 'nomem_encr' column (5342 rows, 210 columns)\n",
      "[OK] Health wave 17_24q_EN_1.0p: has 'nomem_encr' column (5521 rows, 215 columns)\n",
      "[WARNING] LISS_Final_X_M1: missing 'nomem_encr' column - will be skipped\n",
      "[WARNING] LISS_Final_X_M3: missing 'nomem_encr' column - will be skipped\n",
      "[OK] social interation and leisure wave 17_24q_EN_1.0p: has 'nomem_encr' column (5581 rows, 450 columns)\n",
      "[WARNING] LISS_Final_X_M2: missing 'nomem_encr' column - will be skipped\n",
      "[OK] Work and Schooling wave 16_23p_EN_1.0p: has 'nomem_encr' column (5139 rows, 422 columns)\n",
      "[WARNING] Rubins_Rule_SHAP_Importance: missing 'nomem_encr' column - will be skipped\n",
      "[WARNING] Subgroup_Error_Analysis_Results: missing 'nomem_encr' column - will be skipped\n",
      "[WARNING] LISS_Final_X_M5: missing 'nomem_encr' column - will be skipped\n",
      "[WARNING] LISS_Final_X_M4: missing 'nomem_encr' column - will be skipped\n",
      "[OK] Background Variables wave 17_202407_EN_1.0p: has 'nomem_encr' column (11700 rows, 34 columns)\n",
      "\n",
      "Warning: 15 dataframes will be skipped due to missing 'nomem_encr' column\n",
      "\n",
      "============================================================\n",
      "Starting merge of 13 dataframes...\n",
      "============================================================\n",
      "\n",
      "[1/13] Starting with: Family and Household wave 16_23p_EN_1.0p\n",
      "    Initial shape: (5206, 402)\n",
      "\n",
      "[2/13] Merging: social interation and leisure wave 16_23p_EN_1.0p\n",
      "    Shape before merge: (5206, 402)\n",
      "    Shape of new dataframe: (5991, 450)\n",
      "    Shape after merge: (6340, 851)\n",
      "    Unique respondents (nomem_encr): 6340\n",
      "\n",
      "[3/13] Merging: LISS_Trimmed_76_Features\n",
      "    Shape before merge: (6340, 851)\n",
      "    Shape of new dataframe: (12939, 77)\n",
      "    Shape after merge: (12939, 927)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[4/13] Merging: Health wave 16_23p_EN_1.0p\n",
      "    Shape before merge: (12939, 927)\n",
      "    Shape of new dataframe: (6058, 209)\n",
      "    Shape after merge: (12939, 1135)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[5/13] Merging: Background Variables wave 16_202307_EN_1.0p\n",
      "    Shape before merge: (12939, 1135)\n",
      "    Shape of new dataframe: (8626, 34)\n",
      "    Shape after merge: (12939, 1168)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[6/13] Merging: Work and Schooling wave 17_24q_EN_1.0p\n",
      "    Shape before merge: (12939, 1168)\n",
      "    Shape of new dataframe: (5792, 422)\n",
      "    Shape after merge: (12939, 1589)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[7/13] Merging: Personality wave 16_24p_EN_1.0p\n",
      "    Shape before merge: (12939, 1589)\n",
      "    Shape of new dataframe: (5477, 159)\n",
      "    Shape after merge: (12939, 1747)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[8/13] Merging: income wave 16_23p_EN_1.0p\n",
      "    Shape before merge: (12939, 1747)\n",
      "    Shape of new dataframe: (4905, 211)\n",
      "    Shape after merge: (12939, 1957)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[9/13] Merging: income wave 17_24q_EN_1.0p\n",
      "    Shape before merge: (12939, 1957)\n",
      "    Shape of new dataframe: (5342, 210)\n",
      "    Shape after merge: (12939, 2166)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[10/13] Merging: Health wave 17_24q_EN_1.0p\n",
      "    Shape before merge: (12939, 2166)\n",
      "    Shape of new dataframe: (5521, 215)\n",
      "    Shape after merge: (12939, 2380)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[11/13] Merging: social interation and leisure wave 17_24q_EN_1.0p\n",
      "    Shape before merge: (12939, 2380)\n",
      "    Shape of new dataframe: (5581, 450)\n",
      "    Shape after merge: (12939, 2829)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[12/13] Merging: Work and Schooling wave 16_23p_EN_1.0p\n",
      "    Shape before merge: (12939, 2829)\n",
      "    Shape of new dataframe: (5139, 422)\n",
      "    Shape after merge: (12939, 3250)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "[13/13] Merging: Background Variables wave 17_202407_EN_1.0p\n",
      "    Shape before merge: (12939, 3250)\n",
      "    Shape of new dataframe: (11700, 34)\n",
      "    Shape after merge: (12939, 3283)\n",
      "    Unique respondents (nomem_encr): 12939\n",
      "\n",
      "============================================================\n",
      "Merge completed successfully!\n",
      "============================================================\n",
      "\n",
      "Final merged dataset:\n",
      "  Total rows: 12,939\n",
      "  Total columns: 3,283\n",
      "  Unique respondents (nomem_encr): 12,939\n",
      "\n",
      "Merge order: Family and Household wave 16_23p_EN_1.0p -> social interation and leisure wave 16_23p_EN_1.0p -> LISS_Trimmed_76_Features -> Health wave 16_23p_EN_1.0p -> Background Variables wave 16_202307_EN_1.0p -> Work and Schooling wave 17_24q_EN_1.0p -> Personality wave 16_24p_EN_1.0p -> income wave 16_23p_EN_1.0p -> income wave 17_24q_EN_1.0p -> Health wave 17_24q_EN_1.0p -> social interation and leisure wave 17_24q_EN_1.0p -> Work and Schooling wave 16_23p_EN_1.0p -> Background Variables wave 17_202407_EN_1.0p\n",
      "\n",
      "Column overview (first 20 columns):\n",
      "   1. nomem_encr\n",
      "   2. cf23p_m\n",
      "   3. cf23p001\n",
      "   4. cf23p002\n",
      "   5. cf23p536\n",
      "   6. cf23p004\n",
      "   7. cf23p526\n",
      "   8. cf23p522\n",
      "   9. cf23p523\n",
      "  10. cf23p005\n",
      "  11. cf23p449\n",
      "  12. cf23p007\n",
      "  13. cf23p008\n",
      "  14. cf23p524\n",
      "  15. cf23p525\n",
      "  16. cf23p009\n",
      "  17. cf23p450\n",
      "  18. cf23p011\n",
      "  19. cf23p012\n",
      "  20. cf23p401\n",
      "  ... and 3263 more columns\n",
      "\n",
      "============================================================\n",
      "Saving merged dataset to: merged_liss_data.csv\n",
      "============================================================\n",
      "[OK] Successfully saved to 'merged_liss_data.csv'\n",
      "  File size: 63.88 MB\n",
      "\n",
      "============================================================\n",
      "Merged dataset is available as 'merged_df' variable\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2:  Merge All DataFrames\n",
    "# Merge all dataframes using nomem_encr as join key\n",
    "print(\"=\"*60)\n",
    "print(\"Merging all dataframes using 'nomem_encr' as join key\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check which dataframes have nomem_encr column\n",
    "dataframes_with_key = {}\n",
    "dataframes_without_key = {}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    if 'nomem_encr' in df.columns:\n",
    "        dataframes_with_key[key] = df\n",
    "        print(f\"[OK] {key}: has 'nomem_encr' column ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
    "    else:\n",
    "        dataframes_without_key[key] = df\n",
    "        print(f\"[WARNING] {key}: missing 'nomem_encr' column - will be skipped\")\n",
    "\n",
    "if len(dataframes_without_key) > 0:\n",
    "    print(f\"\\nWarning: {len(dataframes_without_key)} dataframes will be skipped due to missing 'nomem_encr' column\")\n",
    "\n",
    "if len(dataframes_with_key) == 0:\n",
    "    print(\"\\n[ERROR] Error: No dataframes found with 'nomem_encr' column!\")\n",
    "    print(\"Cannot proceed with merge operation.\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting merge of {len(dataframes_with_key)} dataframes...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Start with the first dataframe\n",
    "    merged_df = None\n",
    "    merge_order = []\n",
    "    \n",
    "    for i, (key, df) in enumerate(dataframes_with_key.items()):\n",
    "        if merged_df is None:\n",
    "            # Initialize with first dataframe\n",
    "            merged_df = df.copy()\n",
    "            merge_order.append(key)\n",
    "            print(f\"\\n[1/{len(dataframes_with_key)}] Starting with: {key}\")\n",
    "            print(f\"    Initial shape: {merged_df.shape}\")\n",
    "        else:\n",
    "            # Merge with next dataframe\n",
    "            print(f\"\\n[{i+1}/{len(dataframes_with_key)}] Merging: {key}\")\n",
    "            print(f\"    Shape before merge: {merged_df.shape}\")\n",
    "            print(f\"    Shape of new dataframe: {df.shape}\")\n",
    "            \n",
    "            # Check for overlapping columns (excluding nomem_encr)\n",
    "            overlapping_cols = set(merged_df.columns) & set(df.columns) - {'nomem_encr'}\n",
    "            \n",
    "            if overlapping_cols:\n",
    "                # Use suffixes for overlapping columns\n",
    "                merged_df = pd.merge(\n",
    "                    merged_df, \n",
    "                    df, \n",
    "                    on='nomem_encr', \n",
    "                    how='outer',\n",
    "                    suffixes=('', f'_{key}')\n",
    "                )\n",
    "            else:\n",
    "                # No overlapping columns, simple merge\n",
    "                merged_df = pd.merge(\n",
    "                    merged_df, \n",
    "                    df, \n",
    "                    on='nomem_encr', \n",
    "                    how='outer'\n",
    "                )\n",
    "            \n",
    "            merge_order.append(key)\n",
    "            print(f\"    Shape after merge: {merged_df.shape}\")\n",
    "            print(f\"    Unique respondents (nomem_encr): {merged_df['nomem_encr'].nunique()}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Merge completed successfully!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nFinal merged dataset:\")\n",
    "    print(f\"  Total rows: {len(merged_df):,}\")\n",
    "    print(f\"  Total columns: {len(merged_df.columns):,}\")\n",
    "    print(f\"  Unique respondents (nomem_encr): {merged_df['nomem_encr'].nunique():,}\")\n",
    "    print(f\"\\nMerge order: {' -> '.join(merge_order)}\")\n",
    "    \n",
    "    # Display column information\n",
    "    print(f\"\\nColumn overview (first 20 columns):\")\n",
    "    for i, col in enumerate(merged_df.columns[:20], 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    if len(merged_df.columns) > 20:\n",
    "        print(f\"  ... and {len(merged_df.columns) - 20} more columns\")\n",
    "    \n",
    "    # Save merged dataset\n",
    "    output_file = 'merged_liss_data.csv'\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Saving merged dataset to: {output_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        merged_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"[OK] Successfully saved to '{output_file}'\")\n",
    "        print(f\"  File size: {file_size:.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error saving file: {e}\")\n",
    "        # Try with different encoding\n",
    "        try:\n",
    "            merged_df.to_csv(output_file, index=False, encoding='latin-1')\n",
    "            file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
    "            print(f\"[OK] Successfully saved with latin-1 encoding\")\n",
    "            print(f\"  File size: {file_size:.2f} MB\")\n",
    "        except Exception as e2:\n",
    "            print(f\"[ERROR] Error with latin-1 encoding: {e2}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Merged dataset is available as 'merged_df' variable\")\n",
    "    print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Preliminary Feature Reduction (Pre-MICE Trimming)\n",
      "============================================================\n",
      "\n",
      "Original dataset shape: (12939, 3283)\n",
      "  Rows: 12,939\n",
      "  Columns: 3,283\n",
      "\n",
      "============================================================\n",
      "Task 1: Defining Core Variables to Keep\n",
      "============================================================\n",
      "\n",
      "1. Big Five original items: 50 variables\n",
      "   Range: cp24p020 to cp24p069\n",
      "\n",
      "2. Core Delta variables' original wave data: 10 variables\n",
      "   Variables: cw23p133, cw24q133, cw23p429, cw24q429, cw23p428, cw24q428, cw23p430, cw24q430, cw23p610, cw24q610\n",
      "\n",
      "3. WLB/Health Delta variables' original wave data: 4 variables\n",
      "   Variables: cw23p391, cw24q391, ch23p022, ch24q022\n",
      "\n",
      "4. Baseline control variables: 7 variables\n",
      "   Variables: nomem_encr, cw23p003, cf23p455, ch23p004, ch23p001, ci23p337, cw23p005\n",
      "\n",
      "5. W16/W17 income and occupation variables (for MICE): 6 variables\n",
      "   Variables: ci23p337, ci24q337, ci23p383, cw23p525, ci24q383, cw24q525\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total unique variables to keep: 76\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Task 2: Executing Feature Filtering\n",
      "============================================================\n",
      "\n",
      "Column availability check:\n",
      "  Available columns: 76/76\n",
      "\n",
      "[OK] Created trimmed dataset with 76 columns\n",
      "\n",
      "============================================================\n",
      "Task 3: Output Validation\n",
      "============================================================\n",
      "\n",
      "Trimmed dataset shape:\n",
      "  Rows: 12,939\n",
      "  Columns: 76\n",
      "\n",
      "Feature reduction:\n",
      "  Original columns: 3,283\n",
      "  Trimmed columns: 76\n",
      "  Reduction: 97.7%\n",
      "\n",
      "============================================================\n",
      "All Column Names in Trimmed Dataset\n",
      "============================================================\n",
      "\n",
      "Total: 76 columns\n",
      "\n",
      "Column names by category:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Big Five Items (50 columns):\n",
      "    1. cp24p020\n",
      "    2. cp24p021\n",
      "    3. cp24p022\n",
      "    4. cp24p023\n",
      "    5. cp24p024\n",
      "    6. cp24p025\n",
      "    7. cp24p026\n",
      "    8. cp24p027\n",
      "    9. cp24p028\n",
      "   10. cp24p029\n",
      "   11. cp24p030\n",
      "   12. cp24p031\n",
      "   13. cp24p032\n",
      "   14. cp24p033\n",
      "   15. cp24p034\n",
      "   16. cp24p035\n",
      "   17. cp24p036\n",
      "   18. cp24p037\n",
      "   19. cp24p038\n",
      "   20. cp24p039\n",
      "   21. cp24p040\n",
      "   22. cp24p041\n",
      "   23. cp24p042\n",
      "   24. cp24p043\n",
      "   25. cp24p044\n",
      "   26. cp24p045\n",
      "   27. cp24p046\n",
      "   28. cp24p047\n",
      "   29. cp24p048\n",
      "   30. cp24p049\n",
      "   31. cp24p050\n",
      "   32. cp24p051\n",
      "   33. cp24p052\n",
      "   34. cp24p053\n",
      "   35. cp24p054\n",
      "   36. cp24p055\n",
      "   37. cp24p056\n",
      "   38. cp24p057\n",
      "   39. cp24p058\n",
      "   40. cp24p059\n",
      "   41. cp24p060\n",
      "   42. cp24p061\n",
      "   43. cp24p062\n",
      "   44. cp24p063\n",
      "   45. cp24p064\n",
      "   46. cp24p065\n",
      "   47. cp24p066\n",
      "   48. cp24p067\n",
      "   49. cp24p068\n",
      "   50. cp24p069\n",
      "\n",
      "2. Core Delta Variables (10 columns):\n",
      "    1. cw23p133\n",
      "    2. cw24q133\n",
      "    3. cw23p429\n",
      "    4. cw24q429\n",
      "    5. cw23p428\n",
      "    6. cw24q428\n",
      "    7. cw23p430\n",
      "    8. cw24q430\n",
      "    9. cw23p610\n",
      "   10. cw24q610\n",
      "\n",
      "3. WLB/Health Delta Variables (4 columns):\n",
      "    1. cw23p391\n",
      "    2. cw24q391\n",
      "    3. ch23p022\n",
      "    4. ch24q022\n",
      "\n",
      "4. Baseline Control Variables (7 columns):\n",
      "    1. nomem_encr\n",
      "    2. cw23p003\n",
      "    3. cf23p455\n",
      "    4. ch23p004\n",
      "    5. ch23p001\n",
      "    6. ci23p337\n",
      "    7. cw23p005\n",
      "\n",
      "5. Income and Occupation Variables (6 columns):\n",
      "    1. ci23p337\n",
      "    2. ci24q337\n",
      "    3. ci23p383\n",
      "    4. cw23p525\n",
      "    5. ci24q383\n",
      "    6. cw24q525\n",
      "\n",
      "============================================================\n",
      "[OK] Pre-MICE trimming completed!\n",
      "Trimmed dataset is available as 'merged_df' variable\n",
      "Expected column count: 70-80 columns\n",
      "Actual column count: 76 columns\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Saving Trimmed Dataset to Disk\n",
      "============================================================\n",
      "\n",
      "Saving to: LISS_Trimmed_76_Features.csv\n",
      "  Rows: 12,939\n",
      "  Columns: 76\n",
      "\n",
      "[OK] Successfully saved to 'LISS_Trimmed_76_Features.csv'\n",
      "  File size: 1.59 MB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 3:  Preliminary Feature Reduction (Pre-MICE Trimming)\n",
    "print(\"=\"*60)\n",
    "print(\"Preliminary Feature Reduction (Pre-MICE Trimming)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'merged_df' in globals() and merged_df is not None:\n",
    "    print(f\"\\nOriginal dataset shape: {merged_df.shape}\")\n",
    "    print(f\"  Rows: {merged_df.shape[0]:,}\")\n",
    "    print(f\"  Columns: {merged_df.shape[1]:,}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 1: Define Core Variables to Keep (Keep List)\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 1: Defining Core Variables to Keep\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    columns_to_keep = []\n",
    "    \n",
    "    # 1. All Big Five original items: cp24p020 to cp24p069 (50 items)\n",
    "    big_five_items = [f'cp24p{i:03d}' for i in range(20, 70)]\n",
    "    columns_to_keep.extend(big_five_items)\n",
    "    print(f\"\\n1. Big Five original items: {len(big_five_items)} variables\")\n",
    "    print(f\"   Range: cp24p020 to cp24p069\")\n",
    "    \n",
    "    # 2. All core Delta variables' original wave data\n",
    "    core_delta_vars = [\n",
    "        'cw23p133', 'cw24q133',  # Job Satisfaction\n",
    "        'cw23p429', 'cw24q429',  # Autonomy\n",
    "        'cw23p428', 'cw24q428',  # Workload\n",
    "        'cw23p430', 'cw24q430',  # Skills\n",
    "        'cw23p610', 'cw24q610'   # Remote Hours\n",
    "    ]\n",
    "    columns_to_keep.extend(core_delta_vars)\n",
    "    print(f\"\\n2. Core Delta variables' original wave data: {len(core_delta_vars)} variables\")\n",
    "    print(f\"   Variables: {', '.join(core_delta_vars)}\")\n",
    "    \n",
    "    # 3. All WLB/Health Delta variables' original wave data\n",
    "    wlb_health_delta_vars = [\n",
    "        'cw23p391', 'cw24q391',  # WFC Proxy\n",
    "        'ch23p022', 'ch24q022'   # Health Hindrance\n",
    "    ]\n",
    "    columns_to_keep.extend(wlb_health_delta_vars)\n",
    "    print(f\"\\n3. WLB/Health Delta variables' original wave data: {len(wlb_health_delta_vars)} variables\")\n",
    "    print(f\"   Variables: {', '.join(wlb_health_delta_vars)}\")\n",
    "    \n",
    "    # 4. All baseline control variables\n",
    "    baseline_control_vars = [\n",
    "        'nomem_encr',      # ID\n",
    "        'cw23p003',        # Age\n",
    "        'cf23p455',        # Children Count\n",
    "        'ch23p004',        # Self-rated Health\n",
    "        'ch23p001',        # Gender\n",
    "        'ci23p337',        # Income (W16)\n",
    "        'cw23p005'         # Education\n",
    "    ]\n",
    "    columns_to_keep.extend(baseline_control_vars)\n",
    "    print(f\"\\n4. Baseline control variables: {len(baseline_control_vars)} variables\")\n",
    "    print(f\"   Variables: {', '.join(baseline_control_vars)}\")\n",
    "    \n",
    "    # 5. All W16/W17 income and occupation variables (for MICE)\n",
    "    income_occupation_vars = [\n",
    "        'ci23p337',        # W16 Income\n",
    "        'ci24q337',        # W17 Income\n",
    "        'ci23p383',        # W16 Occupation\n",
    "        'cw23p525',        # W16 Main Occupation\n",
    "        'ci24q383',        # W17 Occupation\n",
    "        'cw24q525'         # W17 Main Occupation\n",
    "    ]\n",
    "    columns_to_keep.extend(income_occupation_vars)\n",
    "    print(f\"\\n5. W16/W17 income and occupation variables (for MICE): {len(income_occupation_vars)} variables\")\n",
    "    print(f\"   Variables: {', '.join(income_occupation_vars)}\")\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    columns_to_keep = list(dict.fromkeys(columns_to_keep))\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*60)\n",
    "    print(f\"Total unique variables to keep: {len(columns_to_keep)}\")\n",
    "    print(f\"-\"*60)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 2: Execute Filtering\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Executing Feature Filtering\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check which columns exist in the dataset\n",
    "    available_columns = [col for col in columns_to_keep if col in merged_df.columns]\n",
    "    missing_columns = [col for col in columns_to_keep if col not in merged_df.columns]\n",
    "    \n",
    "    print(f\"\\nColumn availability check:\")\n",
    "    print(f\"  Available columns: {len(available_columns)}/{len(columns_to_keep)}\")\n",
    "    if missing_columns:\n",
    "        print(f\"  Missing columns: {len(missing_columns)}\")\n",
    "        print(f\"    {', '.join(missing_columns[:10])}{'...' if len(missing_columns) > 10 else ''}\")\n",
    "    \n",
    "    # Create new DataFrame with only selected columns\n",
    "    merged_df_trimmed = merged_df[available_columns].copy()\n",
    "    \n",
    "    print(f\"\\n[OK] Created trimmed dataset with {len(available_columns)} columns\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: Output Validation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Output Validation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nTrimmed dataset shape:\")\n",
    "    print(f\"  Rows: {merged_df_trimmed.shape[0]:,}\")\n",
    "    print(f\"  Columns: {merged_df_trimmed.shape[1]:,}\")\n",
    "    \n",
    "    reduction_ratio = (1 - merged_df_trimmed.shape[1] / merged_df.shape[1]) * 100\n",
    "    print(f\"\\nFeature reduction:\")\n",
    "    print(f\"  Original columns: {merged_df.shape[1]:,}\")\n",
    "    print(f\"  Trimmed columns: {merged_df_trimmed.shape[1]:,}\")\n",
    "    print(f\"  Reduction: {reduction_ratio:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"All Column Names in Trimmed Dataset\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTotal: {len(merged_df_trimmed.columns)} columns\\n\")\n",
    "    \n",
    "    # Group columns by category for better readability\n",
    "    print(\"Column names by category:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Big Five items\n",
    "    big_five_in_dataset = [col for col in big_five_items if col in merged_df_trimmed.columns]\n",
    "    if big_five_in_dataset:\n",
    "        print(f\"\\n1. Big Five Items ({len(big_five_in_dataset)} columns):\")\n",
    "        for i, col in enumerate(big_five_in_dataset, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # Core Delta variables\n",
    "    core_delta_in_dataset = [col for col in core_delta_vars if col in merged_df_trimmed.columns]\n",
    "    if core_delta_in_dataset:\n",
    "        print(f\"\\n2. Core Delta Variables ({len(core_delta_in_dataset)} columns):\")\n",
    "        for i, col in enumerate(core_delta_in_dataset, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # WLB/Health Delta variables\n",
    "    wlb_health_in_dataset = [col for col in wlb_health_delta_vars if col in merged_df_trimmed.columns]\n",
    "    if wlb_health_in_dataset:\n",
    "        print(f\"\\n3. WLB/Health Delta Variables ({len(wlb_health_in_dataset)} columns):\")\n",
    "        for i, col in enumerate(wlb_health_in_dataset, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # Baseline control variables\n",
    "    baseline_in_dataset = [col for col in baseline_control_vars if col in merged_df_trimmed.columns]\n",
    "    if baseline_in_dataset:\n",
    "        print(f\"\\n4. Baseline Control Variables ({len(baseline_in_dataset)} columns):\")\n",
    "        for i, col in enumerate(baseline_in_dataset, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # Income and occupation variables\n",
    "    income_occ_in_dataset = [col for col in income_occupation_vars if col in merged_df_trimmed.columns]\n",
    "    if income_occ_in_dataset:\n",
    "        print(f\"\\n5. Income and Occupation Variables ({len(income_occ_in_dataset)} columns):\")\n",
    "        for i, col in enumerate(income_occ_in_dataset, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # Update global variable\n",
    "    merged_df = merged_df_trimmed\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"[OK] Pre-MICE trimming completed!\")\n",
    "    print(f\"Trimmed dataset is available as 'merged_df' variable\")\n",
    "    print(f\"Expected column count: 70-80 columns\")\n",
    "    print(f\"Actual column count: {merged_df_trimmed.shape[1]} columns\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Save Trimmed Dataset to Disk\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Saving Trimmed Dataset to Disk\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define filename\n",
    "    output_filename = 'LISS_Trimmed_76_Features.csv'\n",
    "    \n",
    "    print(f\"\\nSaving to: {output_filename}\")\n",
    "    print(f\"  Rows: {merged_df_trimmed.shape[0]:,}\")\n",
    "    print(f\"  Columns: {merged_df_trimmed.shape[1]:,}\")\n",
    "    \n",
    "    # Save to CSV with index\n",
    "    merged_df_trimmed.to_csv(output_filename, index=True, encoding='utf-8')\n",
    "    \n",
    "    # Check file size\n",
    "    import os\n",
    "    if os.path.exists(output_filename):\n",
    "        file_size = os.path.getsize(output_filename) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"\\n[OK] Successfully saved to '{output_filename}'\")\n",
    "        print(f\"  File size: {file_size:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"\\n[ERROR] Error: File was not created successfully\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[ERROR] Error: 'merged_df' not found. Please run the merge cell (Cell 2) first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISS Missing Value Standardization\n",
      "============================================================\n",
      "\n",
      "LISS Missing Value Codes Mapping:\n",
      "     -9: I don't know (通用)\n",
      "     -8: I prefer not to say (通用)\n",
      "    -13: I don't know (收入特定)\n",
      "    -14: Prefer not to say (收入特定)\n",
      "    999: I don't know (评分特定, 0-10量表)\n",
      "     99: I don't know (分类特定)\n",
      "   9999: I don't know (大范围整数)\n",
      "\n",
      "============================================================\n",
      "Step 1: Standardizing LISS Missing Value Codes\n",
      "============================================================\n",
      "\n",
      "Processing 75 columns...\n",
      "\n",
      "Replacing missing value codes: [-9, -8, -13, -14, 999, 99, 9999]\n",
      "\n",
      "============================================================\n",
      "Step 2: Handling Income Zero Values\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Handling Income Zero Values\n",
      "============================================================\n",
      "\n",
      "ci23p337:\n",
      "  Zeros before: 0\n",
      "  Zeros with paid work (replaced): 0\n",
      "  Zeros after: 0\n",
      "  Occupation variable: ci23p383\n",
      "\n",
      "ci24q337:\n",
      "  Zeros before: 0\n",
      "  Zeros with paid work (replaced): 0\n",
      "  Zeros after: 0\n",
      "  Occupation variable: cw24q525\n",
      "\n",
      "============================================================\n",
      "Missing Value Processing Summary\n",
      "============================================================\n",
      "\n",
      "Overall Statistics:\n",
      "  Total columns processed: 75\n",
      "  Columns with missing values: 75\n",
      "  Total missing values before: 560,385\n",
      "  Total missing values after: 560,385\n",
      "  Values replaced by standardization: 0\n",
      "\n",
      "============================================================\n",
      "Top 20 Variables with Most Missing Values (After Processing)\n",
      "============================================================\n",
      "\n",
      "Variable                                 Missing Count   Missing % \n",
      "-----------------------------------------------------------------\n",
      "ci23p383                                 8,034            62.09%\n",
      "ci23p337                                 8,034            62.09%\n",
      "cw23p610                                 7,800            60.28%\n",
      "cw23p391                                 7,800            60.28%\n",
      "cw23p133                                 7,800            60.28%\n",
      "cw23p429                                 7,800            60.28%\n",
      "cw23p428                                 7,800            60.28%\n",
      "cw23p430                                 7,800            60.28%\n",
      "cw23p003                                 7,800            60.28%\n",
      "cw23p005                                 7,800            60.28%\n",
      "cw23p525                                 7,800            60.28%\n",
      "cf23p455                                 7,733            59.77%\n",
      "ci24q337                                 7,597            58.71%\n",
      "ci24q383                                 7,597            58.71%\n",
      "cp24p021                                 7,462            57.67%\n",
      "cp24p060                                 7,462            57.67%\n",
      "cp24p059                                 7,462            57.67%\n",
      "cp24p058                                 7,462            57.67%\n",
      "cp24p020                                 7,462            57.67%\n",
      "cp24p056                                 7,462            57.67%\n",
      "\n",
      "... and 55 more variables with missing values\n",
      "\n",
      "============================================================\n",
      "[OK] Missing value processing completed!\n",
      "Cleaned dataset is available as 'merged_df' variable\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4:  LISS Missing Value Standardization\n",
    "# LISS Missing Value Standardization\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISS Missing Value Standardization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Create mapping table for LISS missing value codes\n",
    "LISS_MISSING_CODES = {\n",
    "    -9: \"I don't know (通用)\",\n",
    "    -8: \"I prefer not to say (通用)\",\n",
    "    -13: \"I don't know (收入特定)\",\n",
    "    -14: \"Prefer not to say (收入特定)\",\n",
    "    999: \"I don't know (评分特定, 0-10量表)\",\n",
    "    99: \"I don't know (分类特定)\",\n",
    "    9999: \"I don't know (大范围整数)\"\n",
    "}\n",
    "\n",
    "print(\"\\nLISS Missing Value Codes Mapping:\")\n",
    "for code, description in LISS_MISSING_CODES.items():\n",
    "    print(f\"  {code:5d}: {description}\")\n",
    "\n",
    "def standardize_liss_missing_values(df, missing_codes=None):\n",
    "    \"\"\"\n",
    "    Standardize LISS missing value codes to NaN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    missing_codes : dict, optional\n",
    "        Dictionary mapping codes to descriptions. If None, uses default LISS codes.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with missing values standardized\n",
    "    dict\n",
    "        Statistics about missing values before and after processing\n",
    "    \"\"\"\n",
    "    if missing_codes is None:\n",
    "        missing_codes = LISS_MISSING_CODES\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Store statistics\n",
    "    stats = {\n",
    "        'before': {},\n",
    "        'after': {},\n",
    "        'replaced': {}\n",
    "    }\n",
    "    \n",
    "    # Get all columns except nomem_encr (identifier)\n",
    "    columns_to_process = [col for col in df_processed.columns if col != 'nomem_encr']\n",
    "    \n",
    "    print(f\"\\nProcessing {len(columns_to_process)} columns...\")\n",
    "    \n",
    "    # Calculate missing values before processing\n",
    "    for col in columns_to_process:\n",
    "        stats['before'][col] = df_processed[col].isna().sum()\n",
    "    \n",
    "    # Replace missing value codes with NaN\n",
    "    codes_to_replace = list(missing_codes.keys())\n",
    "    print(f\"\\nReplacing missing value codes: {codes_to_replace}\")\n",
    "    \n",
    "    for col in columns_to_process:\n",
    "        # Count how many values will be replaced\n",
    "        replaced_count = 0\n",
    "        for code in codes_to_replace:\n",
    "            mask = df_processed[col] == code\n",
    "            replaced_count += mask.sum()\n",
    "            df_processed.loc[mask, col] = np.nan\n",
    "        \n",
    "        if replaced_count > 0:\n",
    "            stats['replaced'][col] = replaced_count\n",
    "    \n",
    "    # Calculate missing values after processing\n",
    "    for col in columns_to_process:\n",
    "        stats['after'][col] = df_processed[col].isna().sum()\n",
    "    \n",
    "    return df_processed, stats\n",
    "\n",
    "def handle_income_zero_values(df):\n",
    "    \"\"\"\n",
    "    Handle special case of income = 0 values.\n",
    "    \n",
    "    Logic:\n",
    "    - If income = 0 AND main occupation shows paid work (codes 1-3), \n",
    "      then treat 0 as missing value\n",
    "    - Otherwise, keep 0 (genuine zero income)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with income zeros handled\n",
    "    dict\n",
    "        Statistics about income zero handling\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    income_stats = {}\n",
    "    \n",
    "    # Income variables to check (Wave 16 and Wave 17)\n",
    "    income_vars = {\n",
    "        'ci23p337': 'ci23p383',  # W16 income -> W16 main occupation\n",
    "        'ci24q337': 'cw24q525',  # W17 income -> W17 main occupation (may need adjustment)\n",
    "    }\n",
    "    \n",
    "    # Also check for alternative occupation variable names\n",
    "    alt_occupation_vars = {\n",
    "        'ci23p337': ['ci23p383', 'cw23p525'],  # W16 alternatives\n",
    "        'ci24q337': ['cw24q525', 'ci24q383'],  # W17 alternatives\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Handling Income Zero Values\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for income_var, occupation_var in income_vars.items():\n",
    "        if income_var not in df_processed.columns:\n",
    "            continue\n",
    "        \n",
    "        # Find alternative occupation variable if primary doesn't exist\n",
    "        if occupation_var not in df_processed.columns:\n",
    "            if income_var in alt_occupation_vars:\n",
    "                for alt_var in alt_occupation_vars[income_var]:\n",
    "                    if alt_var in df_processed.columns:\n",
    "                        occupation_var = alt_var\n",
    "                        break\n",
    "        \n",
    "        if occupation_var not in df_processed.columns:\n",
    "            print(f\"\\n[WARNING] {income_var}: Occupation variable not found, skipping zero handling\")\n",
    "            continue\n",
    "        \n",
    "        # Count zeros before processing\n",
    "        zeros_before = (df_processed[income_var] == 0).sum()\n",
    "        \n",
    "        # Find rows where income = 0 AND occupation indicates paid work (1, 2, or 3)\n",
    "        # Note: Adjust codes based on actual LISS codebook\n",
    "        paid_work_mask = df_processed[occupation_var].isin([1, 2, 3])\n",
    "        income_zero_mask = (df_processed[income_var] == 0)\n",
    "        zero_with_paid_work = income_zero_mask & paid_work_mask\n",
    "        \n",
    "        # Replace zeros with NaN where individual has paid work\n",
    "        n_replaced = zero_with_paid_work.sum()\n",
    "        df_processed.loc[zero_with_paid_work, income_var] = np.nan\n",
    "        \n",
    "        zeros_after = (df_processed[income_var] == 0).sum()\n",
    "        \n",
    "        income_stats[income_var] = {\n",
    "            'zeros_before': zeros_before,\n",
    "            'zeros_after': zeros_after,\n",
    "            'replaced_to_na': n_replaced,\n",
    "            'occupation_var_used': occupation_var\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{income_var}:\")\n",
    "        print(f\"  Zeros before: {zeros_before}\")\n",
    "        print(f\"  Zeros with paid work (replaced): {n_replaced}\")\n",
    "        print(f\"  Zeros after: {zeros_after}\")\n",
    "        print(f\"  Occupation variable: {occupation_var}\")\n",
    "    \n",
    "    return df_processed, income_stats\n",
    "\n",
    "# 2. Apply missing value standardization\n",
    "if 'merged_df' in globals() and merged_df is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 1: Standardizing LISS Missing Value Codes\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Standardize missing values\n",
    "    merged_df_cleaned, missing_stats = standardize_liss_missing_values(merged_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 2: Handling Income Zero Values\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Handle income zero values\n",
    "    merged_df_cleaned, income_stats = handle_income_zero_values(merged_df_cleaned)\n",
    "    \n",
    "    # 3. Generate summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Missing Value Processing Summary\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    total_cols = len([c for c in merged_df_cleaned.columns if c != 'nomem_encr'])\n",
    "    cols_with_missing = sum(1 for col in merged_df_cleaned.columns \n",
    "                           if col != 'nomem_encr' and merged_df_cleaned[col].isna().any())\n",
    "    \n",
    "    total_missing_before = sum(missing_stats['before'].values())\n",
    "    total_missing_after = merged_df_cleaned.isna().sum().sum()\n",
    "    total_replaced = sum(missing_stats['replaced'].values())\n",
    "    \n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Total columns processed: {total_cols}\")\n",
    "    print(f\"  Columns with missing values: {cols_with_missing}\")\n",
    "    print(f\"  Total missing values before: {total_missing_before:,}\")\n",
    "    print(f\"  Total missing values after: {total_missing_after:,}\")\n",
    "    print(f\"  Values replaced by standardization: {total_replaced:,}\")\n",
    "    \n",
    "    # Show top 20 variables with most missing values\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Top 20 Variables with Most Missing Values (After Processing)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    missing_counts = merged_df_cleaned.isna().sum()\n",
    "    missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'Variable':<40} {'Missing Count':<15} {'Missing %':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for var, count in missing_counts.head(20).items():\n",
    "        pct = (count / len(merged_df_cleaned)) * 100\n",
    "        print(f\"{var:<40} {count:<15,} {pct:>6.2f}%\")\n",
    "    \n",
    "    if len(missing_counts) > 20:\n",
    "        print(f\"\\n... and {len(missing_counts) - 20} more variables with missing values\")\n",
    "    \n",
    "    # Show variables where values were replaced\n",
    "    if missing_stats['replaced']:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Variables with Replaced Missing Value Codes\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n{'Variable':<40} {'Values Replaced':<15}\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        sorted_replaced = sorted(missing_stats['replaced'].items(), \n",
    "                                key=lambda x: x[1], reverse=True)\n",
    "        for var, count in sorted_replaced[:20]:\n",
    "            print(f\"{var:<40} {count:<15,}\")\n",
    "        \n",
    "        if len(sorted_replaced) > 20:\n",
    "            print(f\"\\n... and {len(sorted_replaced) - 20} more variables\")\n",
    "    \n",
    "    # Update the merged_df variable\n",
    "    merged_df = merged_df_cleaned\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"[OK] Missing value processing completed!\")\n",
    "    print(\"Cleaned dataset is available as 'merged_df' variable\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[ERROR] Error: 'merged_df' not found. Please run the merge cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step 4.5 - Logical Range Validation and Cleaning\n",
      "============================================================\n",
      "\n",
      "Original merged_df shape: (12939, 76)\n",
      "\n",
      "============================================================\n",
      "Task 1: Defining Logical Range Rules\n",
      "============================================================\n",
      "Job Satisfaction variables: ['cw23p133', 'cw24q133'] -> [0, 10]\n",
      "Big Five items: cp24p020–cp24p069 (50 items) -> [1, 5]\n",
      "Work characteristics (1–5 Likert): ['cw23p429', 'cw24q429', 'cw23p428', 'cw24q428', 'cw23p430', 'cw24q430'] -> [1, 5]\n",
      "Health variables (Likert): ['ch23p022', 'ch24q022', 'ch23p004', 'ch24p004'] -> [1, 5]\n",
      "WFC binary indicators: ['cw23p391', 'cw24q391'] -> [1, 2]\n",
      "Remote work hours: ['cw23p610', 'cw24q610'] -> [0, 168]\n",
      "Control variables: cw23p003 -> [16, 100], cf23p455 -> [0, 20]\n",
      "\n",
      "============================================================\n",
      "Task 2: Applying Logical Range Cleaning\n",
      "============================================================\n",
      "[CLEAN] cw23p133: 48 values outside [0, 10] -> set to NaN\n",
      "[CLEAN] cw24q133: 58 values outside [0, 10] -> set to NaN\n",
      "[CLEAN] cw23p391: 1224 values outside [1, 2] -> set to NaN\n",
      "[CLEAN] cw24q391: 1350 values outside [1, 2] -> set to NaN\n",
      "\n",
      "[OK] Logical cleaning applied to 4 columns\n",
      "Total invalid values set to NaN: 2680\n",
      "\n",
      "============================================================\n",
      "Task 3: Validation for Job Satisfaction Variables\n",
      "============================================================\n",
      "\n",
      "cw23p133 (after cleaning):\n",
      "  Min (ignoring NaN): 0.0\n",
      "  Max (ignoring NaN): 10.0\n",
      "  Out-of-range values remaining outside [0, 10]: 0\n",
      "\n",
      "cw24q133 (after cleaning):\n",
      "  Min (ignoring NaN): 0.0\n",
      "  Max (ignoring NaN): 10.0\n",
      "  Out-of-range values remaining outside [0, 10]: 0\n",
      "\n",
      "[CHECK] Post-cleaning logical range check for Job Satisfaction:\n",
      "  cw23p133: min=0.0, max=10.0\n",
      "  cw24q133: min=0.0, max=10.0\n",
      "\n",
      "============================================================\n",
      "Task 4: Updating merged_df with Cleaned Data\n",
      "============================================================\n",
      "[OK] merged_df updated. New shape: (12939, 76)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5:  Step 4.5 - Logical Range Validation and Cleaning\n",
    "print(\"=\"*60)\n",
    "print(\"Step 4.5 - Logical Range Validation and Cleaning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if 'merged_df' in globals() and merged_df is not None:\n",
    "    df = merged_df.copy()\n",
    "    print(f\"\\nOriginal merged_df shape: {df.shape}\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # Task 1: Define Logical Range Rules\n",
    "    # =====================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 1: Defining Logical Range Rules\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    range_rules = {}\n",
    "    \n",
    "    # 1. Job Satisfaction: cw23p133, cw24q133 (0–10)\n",
    "    job_sat_vars = ['cw23p133', 'cw24q133']\n",
    "    for col in job_sat_vars:\n",
    "        range_rules[col] = (0, 10)\n",
    "    print(f\"Job Satisfaction variables: {job_sat_vars} -> [0, 10]\")\n",
    "    \n",
    "    # 2. Big Five items: cp24p020–cp24p069 (1–5)\n",
    "    big_five_items = [f'cp24p{i:03d}' for i in range(20, 70)]\n",
    "    for col in big_five_items:\n",
    "        range_rules[col] = (1, 5)\n",
    "    print(f\"Big Five items: cp24p020–cp24p069 ({len(big_five_items)} items) -> [1, 5]\")\n",
    "    \n",
    "    # 3. Work characteristics (1–5 Likert)\n",
    "    work_char_vars = [\n",
    "        'cw23p429', 'cw24q429',  # Autonomy\n",
    "        'cw23p428', 'cw24q428',  # Workload\n",
    "        'cw23p430', 'cw24q430'   # Skills\n",
    "    ]\n",
    "    for col in work_char_vars:\n",
    "        range_rules[col] = (1, 5)\n",
    "    print(f\"Work characteristics (1–5 Likert): {work_char_vars} -> [1, 5]\")\n",
    "    \n",
    "    # 4. Health variables (Likert 1–5)\n",
    "    health_vars = ['ch23p022', 'ch24q022', 'ch23p004', 'ch24p004']\n",
    "    for col in health_vars:\n",
    "        range_rules[col] = (1, 5)\n",
    "    print(f\"Health variables (Likert): {health_vars} -> [1, 5]\")\n",
    "    \n",
    "    # 5. WFC binary indicator (1–2)\n",
    "    wfc_vars = ['cw23p391', 'cw24q391']\n",
    "    for col in wfc_vars:\n",
    "        range_rules[col] = (1, 2)\n",
    "    print(f\"WFC binary indicators: {wfc_vars} -> [1, 2]\")\n",
    "    \n",
    "    # 6. Remote work hours (0–168 per week)\n",
    "    remote_hours_vars = ['cw23p610', 'cw24q610']\n",
    "    for col in remote_hours_vars:\n",
    "        range_rules[col] = (0, 168)\n",
    "    print(f\"Remote work hours: {remote_hours_vars} -> [0, 168]\")\n",
    "    \n",
    "    # 7. Control variables\n",
    "    control_ranges = {\n",
    "        'cw23p003': (16, 100),  # Age\n",
    "        'cf23p455': (0, 20)     # Children count\n",
    "    }\n",
    "    for col, (lo, hi) in control_ranges.items():\n",
    "        range_rules[col] = (lo, hi)\n",
    "    print(f\"Control variables: cw23p003 -> [16, 100], cf23p455 -> [0, 20]\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # Task 2: Apply Logical Range Cleaning\n",
    "    # =====================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Applying Logical Range Cleaning\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cleaned_columns = []\n",
    "    total_invalid_before = 0\n",
    "    \n",
    "    for col, (min_val, max_val) in range_rules.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Ensure numeric dtype\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Count invalid values before cleaning\n",
    "        invalid_mask = (df[col] < min_val) | (df[col] > max_val)\n",
    "        n_invalid = invalid_mask.sum()\n",
    "        total_invalid_before += int(n_invalid)\n",
    "        \n",
    "        if n_invalid > 0:\n",
    "            print(f\"[CLEAN] {col}: {n_invalid} values outside [{min_val}, {max_val}] -> set to NaN\")\n",
    "            df.loc[invalid_mask, col] = np.nan\n",
    "            cleaned_columns.append(col)\n",
    "    \n",
    "    if cleaned_columns:\n",
    "        print(f\"\\n[OK] Logical cleaning applied to {len(cleaned_columns)} columns\")\n",
    "        print(f\"Total invalid values set to NaN: {total_invalid_before}\")\n",
    "    else:\n",
    "        print(\"\\n[INFO] No columns required logical range cleaning (no out-of-range values found)\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # Task 3: Validation for Job Satisfaction Variables\n",
    "    # =====================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Validation for Job Satisfaction Variables\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for col in job_sat_vars:\n",
    "        if col in df.columns:\n",
    "            # Before/after comparison requires original values; here we can only\n",
    "            # check post-cleaning range, but we can still report current min/max.\n",
    "            col_series = df[col]\n",
    "            print(f\"\\n{col} (after cleaning):\")\n",
    "            print(f\"  Min (ignoring NaN): {col_series.min()}\")\n",
    "            print(f\"  Max (ignoring NaN): {col_series.max()}\")\n",
    "            out_of_range = ((col_series < 0) | (col_series > 10)).sum()\n",
    "            print(f\"  Out-of-range values remaining outside [0, 10]: {out_of_range}\")\n",
    "    \n",
    "    # Ensure that max does not exceed 10 and min does not go below 0\n",
    "    print(\"\\n[CHECK] Post-cleaning logical range check for Job Satisfaction:\")\n",
    "    for col in job_sat_vars:\n",
    "        if col in df.columns:\n",
    "            col_series = df[col]\n",
    "            valid_mask = col_series.notna()\n",
    "            if valid_mask.any():\n",
    "                min_val = col_series[valid_mask].min()\n",
    "                max_val = col_series[valid_mask].max()\n",
    "                print(f\"  {col}: min={min_val}, max={max_val}\")\n",
    "            else:\n",
    "                print(f\"  {col}: all values are NaN after cleaning\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # Task 4: Update Global merged_df\n",
    "    # =====================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 4: Updating merged_df with Cleaned Data\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    merged_df = df\n",
    "    print(f\"[OK] merged_df updated. New shape: {merged_df.shape}\")\n",
    "else:\n",
    "    print(\"\\n[ERROR] 'merged_df' not found. Please run the merge and trimming steps first (Cells 2 and 3, and 4).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Big Five Personality Scales & Core Delta Variables\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task 1: Creating Big Five Personality Scales (W16)\n",
      "============================================================\n",
      "\n",
      "Processing N (Neuroticism)...\n",
      "  [OK] Created N_Score\n",
      "    Items used: 10 (9 forward, 1 reverse)\n",
      "    Cronbach's Alpha: 0.8205\n",
      "\n",
      "Processing E (Extraversion)...\n",
      "  [OK] Created E_Score\n",
      "    Items used: 10 (5 forward, 5 reverse)\n",
      "    Cronbach's Alpha: 0.8825\n",
      "\n",
      "Processing C (Conscientiousness)...\n",
      "  [OK] Created C_Score\n",
      "    Items used: 10 (6 forward, 4 reverse)\n",
      "    Cronbach's Alpha: 0.7903\n",
      "\n",
      "Processing A (Agreeableness)...\n",
      "  [OK] Created A_Score\n",
      "    Items used: 10 (4 forward, 6 reverse)\n",
      "    Cronbach's Alpha: 0.5629\n",
      "\n",
      "Processing O (Openness)...\n",
      "  [OK] Created O_Score\n",
      "    Items used: 10 (5 forward, 5 reverse)\n",
      "    Cronbach's Alpha: 0.4873\n",
      "\n",
      "============================================================\n",
      "Task 2: Creating Core Delta Change Variables\n",
      "============================================================\n",
      "[OK] Created Delta_JobSatisfaction = cw24q133 - cw23p133\n",
      "[OK] Created Delta_Autonomy = (5 - cw24q429) - (5 - cw23p429)\n",
      "[OK] Created Delta_Workload = cw24q428 - cw23p428\n",
      "[OK] Created Delta_Skills = cw24q430 - cw23p430\n",
      "[OK] Created Delta_RemoteHours = cw24q610 - cw23p610\n",
      "\n",
      "============================================================\n",
      "Task 3: Validation Output\n",
      "============================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "Cronbach's Alpha Coefficients for Big Five Scales\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dimension       Scale Name                Alpha      Interpretation\n",
      "----------------------------------------------------------------------\n",
      "N               Neuroticism                 0.8205  Good\n",
      "E               Extraversion                0.8825  Good\n",
      "C               Conscientiousness           0.7903  Acceptable\n",
      "A               Agreeableness               0.5629  Poor\n",
      "O               Openness                    0.4873  Poor\n",
      "\n",
      "------------------------------------------------------------\n",
      "Big Five Personality Scores - Summary Statistics\n",
      "------------------------------------------------------------\n",
      "\n",
      "           N_Score      E_Score      C_Score      A_Score      O_Score\n",
      "count  5466.000000  5466.000000  5466.000000  5466.000000  5466.000000\n",
      "mean      2.685835     3.169573     3.722866     3.501296     3.412322\n",
      "std       0.625865     0.675079     0.535189     0.388626     0.379198\n",
      "min       1.200000     1.000000     1.600000     1.500000     2.000000\n",
      "25%       2.200000     2.700000     3.400000     3.300000     3.100000\n",
      "50%       2.600000     3.200000     3.750000     3.500000     3.400000\n",
      "75%       3.100000     3.600000     4.100000     3.800000     3.700000\n",
      "max       4.800000     5.000000     5.000000     5.000000     4.800000\n",
      "\n",
      "------------------------------------------------------------\n",
      "Additional Statistics\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dimension       Mean       Std        Min        Max        Missing   \n",
      "-----------------------------------------------------------------\n",
      "N_Score             2.686     0.626     1.200     4.800     7,473\n",
      "E_Score             3.170     0.675     1.000     5.000     7,473\n",
      "C_Score             3.723     0.535     1.600     5.000     7,473\n",
      "A_Score             3.501     0.389     1.500     5.000     7,473\n",
      "O_Score             3.412     0.379     2.000     4.800     7,473\n",
      "\n",
      "------------------------------------------------------------\n",
      "Delta Change Variables - Summary Statistics\n",
      "------------------------------------------------------------\n",
      "\n",
      "       Delta_JobSatisfaction  Delta_Autonomy  Delta_Workload  Delta_Skills  Delta_RemoteHours\n",
      "count            1968.000000     2254.000000     2254.000000   2254.000000        2030.000000\n",
      "mean                0.019309       -0.014641       -0.034605     -0.012866           0.033990\n",
      "std                 1.356463        0.781078        0.730206      0.668427           6.494743\n",
      "min                -7.000000       -3.000000       -3.000000     -3.000000         -50.000000\n",
      "25%                -1.000000        0.000000        0.000000      0.000000           0.000000\n",
      "50%                 0.000000        0.000000        0.000000      0.000000           0.000000\n",
      "75%                 1.000000        0.000000        0.000000      0.000000           0.000000\n",
      "max                 8.000000        3.000000        3.000000      3.000000          40.000000\n",
      "\n",
      "------------------------------------------------------------\n",
      "Additional Statistics\n",
      "------------------------------------------------------------\n",
      "\n",
      "Variable                  Mean         Std          Min          Max          Missing   \n",
      "-------------------------------------------------------------------------------------\n",
      "Delta_JobSatisfaction           0.019       1.356      -7.000       8.000    10,971\n",
      "Delta_Autonomy                 -0.015       0.781      -3.000       3.000    10,685\n",
      "Delta_Workload                 -0.035       0.730      -3.000       3.000    10,685\n",
      "Delta_Skills                   -0.013       0.668      -3.000       3.000    10,685\n",
      "Delta_RemoteHours               0.034       6.495     -50.000      40.000    10,909\n",
      "\n",
      "------------------------------------------------------------\n",
      "Correlation Matrix of Delta Variables\n",
      "------------------------------------------------------------\n",
      "\n",
      "                       Delta_JobSatisfaction  Delta_Autonomy  Delta_Workload  Delta_Skills  Delta_RemoteHours\n",
      "Delta_JobSatisfaction               1.000000        0.160339       -0.113891      0.190407           0.042293\n",
      "Delta_Autonomy                      0.160339        1.000000       -0.242913      0.059149           0.032548\n",
      "Delta_Workload                     -0.113891       -0.242913        1.000000     -0.000913          -0.005683\n",
      "Delta_Skills                        0.190407        0.059149       -0.000913      1.000000           0.047926\n",
      "Delta_RemoteHours                   0.042293        0.032548       -0.005683      0.047926           1.000000\n",
      "\n",
      "============================================================\n",
      "[OK] Big Five and Delta variables creation completed!\n",
      "Updated dataset is available as 'merged_df' variable\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6:  Big Five Personality Scales and Core Delta Variables Creation\n",
    "# Big Five Personality Scales and Core Delta Variables Creation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Big Five Personality Scales & Core Delta Variables\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'merged_df' in globals() and merged_df is not None:\n",
    "    df = merged_df.copy()\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 1: Create Big Five Personality Scales (W16 Baseline)\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 1: Creating Big Five Personality Scales (W16)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define Big Five items and coding direction\n",
    "    # Items are 1-5 scale, reverse coding uses: 6 - Value\n",
    "    \n",
    "    big_five_items = {\n",
    "        'N': {  # Neuroticism (神经质)\n",
    "            'forward': ['cp24p023', 'cp24p028', 'cp24p033', 'cp24p043', 'cp24p048', \n",
    "                       'cp24p053', 'cp24p058', 'cp24p063', 'cp24p068'],\n",
    "            'reverse': ['cp24p038']\n",
    "        },\n",
    "        'E': {  # Extraversion (外向性)\n",
    "            'forward': ['cp24p020', 'cp24p030', 'cp24p040', 'cp24p050', 'cp24p060'],\n",
    "            'reverse': ['cp24p025', 'cp24p035', 'cp24p045', 'cp24p055', 'cp24p065']\n",
    "        },\n",
    "        'C': {  # Conscientiousness (尽责性)\n",
    "            'forward': ['cp24p022', 'cp24p032', 'cp24p042', 'cp24p052', 'cp24p062', 'cp24p067'],\n",
    "            'reverse': ['cp24p027', 'cp24p037', 'cp24p047', 'cp24p057']\n",
    "        },\n",
    "        'A': {  # Agreeableness (宜人性)\n",
    "            'forward': ['cp24p026', 'cp24p036', 'cp24p046', 'cp24p061'],\n",
    "            'reverse': ['cp24p021', 'cp24p031', 'cp24p041', 'cp24p051', 'cp24p056', 'cp24p066']\n",
    "        },\n",
    "        'O': {  # Openness (开放性)\n",
    "            'forward': ['cp24p034', 'cp24p044', 'cp24p054', 'cp24p064', 'cp24p069'],\n",
    "            'reverse': ['cp24p024', 'cp24p029', 'cp24p039', 'cp24p049', 'cp24p059']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Function to calculate Cronbach's Alpha\n",
    "    def cronbach_alpha(df_items):\n",
    "        \"\"\"\n",
    "        Calculate Cronbach's Alpha for a set of items.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df_items : pandas.DataFrame\n",
    "            DataFrame with items as columns\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Cronbach's Alpha coefficient\n",
    "        \"\"\"\n",
    "        # Remove rows with any missing values\n",
    "        df_clean = df_items.dropna()\n",
    "        \n",
    "        if len(df_clean) < 2 or df_clean.shape[1] < 2:\n",
    "            return np.nan\n",
    "        \n",
    "        # Calculate item variances and total variance\n",
    "        item_variances = df_clean.var(axis=0, ddof=1)\n",
    "        total_variance = df_clean.sum(axis=1).var(ddof=1)\n",
    "        \n",
    "        # Cronbach's Alpha formula\n",
    "        n_items = df_clean.shape[1]\n",
    "        if total_variance == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        alpha = (n_items / (n_items - 1)) * (1 - item_variances.sum() / total_variance)\n",
    "        return alpha\n",
    "    \n",
    "    # Process each Big Five dimension\n",
    "    big_five_scores = {}\n",
    "    cronbach_alphas = {}\n",
    "    \n",
    "    for dimension, items in big_five_items.items():\n",
    "        print(f\"\\nProcessing {dimension} ({'Neuroticism' if dimension == 'N' else 'Extraversion' if dimension == 'E' else 'Conscientiousness' if dimension == 'C' else 'Agreeableness' if dimension == 'A' else 'Openness'})...\")\n",
    "        \n",
    "        # Collect all items for this dimension\n",
    "        all_items = []\n",
    "        processed_items = []\n",
    "        \n",
    "        # Process forward items\n",
    "        for item in items['forward']:\n",
    "            if item in df.columns:\n",
    "                # Convert to numeric, coercing errors to NaN\n",
    "                df[item] = pd.to_numeric(df[item], errors='coerce')\n",
    "                all_items.append(item)\n",
    "                processed_items.append(item)\n",
    "            else:\n",
    "                print(f\"  [WARNING] Warning: {item} not found in dataset\")\n",
    "        \n",
    "        # Process reverse items\n",
    "        for item in items['reverse']:\n",
    "            if item in df.columns:\n",
    "                # Convert to numeric, coercing errors to NaN\n",
    "                numeric_values = pd.to_numeric(df[item], errors='coerce')\n",
    "                # Reverse code: 6 - Value (only for valid numeric values)\n",
    "                reverse_col = f\"{item}_rev\"\n",
    "                df[reverse_col] = 6 - numeric_values\n",
    "                all_items.append(item)\n",
    "                processed_items.append(reverse_col)\n",
    "            else:\n",
    "                print(f\"  [WARNING] Warning: {item} not found in dataset\")\n",
    "        \n",
    "        if len(processed_items) == 0:\n",
    "            print(f\"  [ERROR] Error: No items found for {dimension}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate mean score\n",
    "        score_col = f\"{dimension}_Score\"\n",
    "        df[score_col] = df[processed_items].mean(axis=1)\n",
    "        big_five_scores[dimension] = score_col\n",
    "        \n",
    "        # Calculate Cronbach's Alpha\n",
    "        df_for_alpha = df[processed_items].copy()\n",
    "        alpha = cronbach_alpha(df_for_alpha)\n",
    "        cronbach_alphas[dimension] = alpha\n",
    "        \n",
    "        print(f\"  [OK] Created {score_col}\")\n",
    "        print(f\"    Items used: {len(processed_items)} ({len(items['forward'])} forward, {len(items['reverse'])} reverse)\")\n",
    "        print(f\"    Cronbach's Alpha: {alpha:.4f}\" if not np.isnan(alpha) else \"    Cronbach's Alpha: N/A (insufficient data)\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 2: Calculate Core 5 Delta Change Variables\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Creating Core Delta Change Variables\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    delta_variables = {}\n",
    "    \n",
    "    # Delta_JobSatisfaction = cw24q133 - cw23p133\n",
    "    if 'cw24q133' in df.columns and 'cw23p133' in df.columns:\n",
    "        w24 = pd.to_numeric(df['cw24q133'], errors='coerce')\n",
    "        w23 = pd.to_numeric(df['cw23p133'], errors='coerce')\n",
    "        df['Delta_JobSatisfaction'] = w24 - w23\n",
    "        delta_variables['Delta_JobSatisfaction'] = ['cw24q133', 'cw23p133']\n",
    "        print(\"[OK] Created Delta_JobSatisfaction = cw24q133 - cw23p133\")\n",
    "    else:\n",
    "        missing = [v for v in ['cw24q133', 'cw23p133'] if v not in df.columns]\n",
    "        print(f\"[WARNING] Warning: Missing variables for Delta_JobSatisfaction: {missing}\")\n",
    "    \n",
    "    # Delta_Autonomy = (5 - cw24q429) - (5 - cw23p429)\n",
    "    if 'cw24q429' in df.columns and 'cw23p429' in df.columns:\n",
    "        w24 = pd.to_numeric(df['cw24q429'], errors='coerce')\n",
    "        w23 = pd.to_numeric(df['cw23p429'], errors='coerce')\n",
    "        df['Delta_Autonomy'] = (5 - w24) - (5 - w23)\n",
    "        delta_variables['Delta_Autonomy'] = ['cw24q429', 'cw23p429']\n",
    "        print(\"[OK] Created Delta_Autonomy = (5 - cw24q429) - (5 - cw23p429)\")\n",
    "    else:\n",
    "        missing = [v for v in ['cw24q429', 'cw23p429'] if v not in df.columns]\n",
    "        print(f\"[WARNING] Warning: Missing variables for Delta_Autonomy: {missing}\")\n",
    "    \n",
    "    # Delta_Workload = cw24q428 - cw23p428\n",
    "    if 'cw24q428' in df.columns and 'cw23p428' in df.columns:\n",
    "        w24 = pd.to_numeric(df['cw24q428'], errors='coerce')\n",
    "        w23 = pd.to_numeric(df['cw23p428'], errors='coerce')\n",
    "        df['Delta_Workload'] = w24 - w23\n",
    "        delta_variables['Delta_Workload'] = ['cw24q428', 'cw23p428']\n",
    "        print(\"[OK] Created Delta_Workload = cw24q428 - cw23p428\")\n",
    "    else:\n",
    "        missing = [v for v in ['cw24q428', 'cw23p428'] if v not in df.columns]\n",
    "        print(f\"[WARNING] Warning: Missing variables for Delta_Workload: {missing}\")\n",
    "    \n",
    "    # Delta_Skills = cw24q430 - cw23p430\n",
    "    if 'cw24q430' in df.columns and 'cw23p430' in df.columns:\n",
    "        w24 = pd.to_numeric(df['cw24q430'], errors='coerce')\n",
    "        w23 = pd.to_numeric(df['cw23p430'], errors='coerce')\n",
    "        df['Delta_Skills'] = w24 - w23\n",
    "        delta_variables['Delta_Skills'] = ['cw24q430', 'cw23p430']\n",
    "        print(\"[OK] Created Delta_Skills = cw24q430 - cw23p430\")\n",
    "    else:\n",
    "        missing = [v for v in ['cw24q430', 'cw23p430'] if v not in df.columns]\n",
    "        print(f\"[WARNING] Warning: Missing variables for Delta_Skills: {missing}\")\n",
    "    \n",
    "    # Delta_RemoteHours = cw24q610 - cw23p610\n",
    "    if 'cw24q610' in df.columns and 'cw23p610' in df.columns:\n",
    "        w24 = pd.to_numeric(df['cw24q610'], errors='coerce')\n",
    "        w23 = pd.to_numeric(df['cw23p610'], errors='coerce')\n",
    "        df['Delta_RemoteHours'] = w24 - w23\n",
    "        delta_variables['Delta_RemoteHours'] = ['cw24q610', 'cw23p610']\n",
    "        print(\"[OK] Created Delta_RemoteHours = cw24q610 - cw23p610\")\n",
    "    else:\n",
    "        missing = [v for v in ['cw24q610', 'cw23p610'] if v not in df.columns]\n",
    "        print(f\"[WARNING] Warning: Missing variables for Delta_RemoteHours: {missing}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: Output Validation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Validation Output\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Print Cronbach's Alpha values\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Cronbach's Alpha Coefficients for Big Five Scales\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"\\n{'Dimension':<15} {'Scale Name':<25} {'Alpha':<10} {'Interpretation'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    alpha_interpretation = {\n",
    "        'Excellent': (0.9, 1.0),\n",
    "        'Good': (0.8, 0.9),\n",
    "        'Acceptable': (0.7, 0.8),\n",
    "        'Questionable': (0.6, 0.7),\n",
    "        'Poor': (0.0, 0.6)\n",
    "    }\n",
    "    \n",
    "    dimension_names = {\n",
    "        'N': 'Neuroticism',\n",
    "        'E': 'Extraversion',\n",
    "        'C': 'Conscientiousness',\n",
    "        'A': 'Agreeableness',\n",
    "        'O': 'Openness'\n",
    "    }\n",
    "    \n",
    "    for dim in ['N', 'E', 'C', 'A', 'O']:\n",
    "        if dim in cronbach_alphas:\n",
    "            alpha = cronbach_alphas[dim]\n",
    "            if not np.isnan(alpha):\n",
    "                for label, (low, high) in alpha_interpretation.items():\n",
    "                    if low <= alpha < high:\n",
    "                        interpretation = label\n",
    "                        break\n",
    "                else:\n",
    "                    interpretation = \"N/A\"\n",
    "            else:\n",
    "                interpretation = \"N/A\"\n",
    "            \n",
    "            print(f\"{dim:<15} {dimension_names[dim]:<25} {alpha:>8.4f}  {interpretation}\")\n",
    "        else:\n",
    "            print(f\"{dim:<15} {dimension_names[dim]:<25} {'N/A':>8}  Not calculated\")\n",
    "    \n",
    "    # Print Big Five scores summary statistics\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Big Five Personality Scores - Summary Statistics\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    big_five_cols = [f\"{dim}_Score\" for dim in ['N', 'E', 'C', 'A', 'O'] if f\"{dim}_Score\" in df.columns]\n",
    "    \n",
    "    if big_five_cols:\n",
    "        print(\"\\n\" + df[big_five_cols].describe().to_string())\n",
    "        \n",
    "        # Additional statistics\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"Additional Statistics\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"\\n{'Dimension':<15} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10} {'Missing':<10}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        for col in big_five_cols:\n",
    "            mean_val = df[col].mean()\n",
    "            std_val = df[col].std()\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            missing = df[col].isna().sum()\n",
    "            print(f\"{col:<15} {mean_val:>9.3f} {std_val:>9.3f} {min_val:>9.3f} {max_val:>9.3f} {missing:>9,}\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] No Big Five scores found in dataset\")\n",
    "    \n",
    "    # Print Delta variables summary statistics\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Delta Change Variables - Summary Statistics\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    delta_cols = [col for col in ['Delta_JobSatisfaction', 'Delta_Autonomy', 'Delta_Workload', \n",
    "                                   'Delta_Skills', 'Delta_RemoteHours'] if col in df.columns]\n",
    "    \n",
    "    if delta_cols:\n",
    "        print(\"\\n\" + df[delta_cols].describe().to_string())\n",
    "        \n",
    "        # Additional statistics\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"Additional Statistics\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"\\n{'Variable':<25} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12} {'Missing':<10}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        for col in delta_cols:\n",
    "            mean_val = df[col].mean()\n",
    "            std_val = df[col].std()\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            missing = df[col].isna().sum()\n",
    "            print(f\"{col:<25} {mean_val:>11.3f} {std_val:>11.3f} {min_val:>11.3f} {max_val:>11.3f} {missing:>9,}\")\n",
    "        \n",
    "        # Show correlation between delta variables\n",
    "        if len(delta_cols) > 1:\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"Correlation Matrix of Delta Variables\")\n",
    "            print(\"-\"*60)\n",
    "            corr_matrix = df[delta_cols].corr()\n",
    "            print(\"\\n\" + corr_matrix.to_string())\n",
    "    else:\n",
    "        print(\"\\n[WARNING] No Delta variables found in dataset\")\n",
    "    \n",
    "    # Update merged_df\n",
    "    merged_df = df\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[OK] Big Five and Delta variables creation completed!\")\n",
    "    print(\"Updated dataset is available as 'merged_df' variable\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\" Error: 'merged_df' not found. Please run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Module 2/4: Remaining Delta Variables & Baseline Controls\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task 1: Calculating Last 2 Delta Variables\n",
      "============================================================\n",
      "[OK] Created Delta_WFC_Proxy = cw24q391 - cw23p391\n",
      "[OK] Created Delta_Health_Hindrance = ch24q022 - ch23p022\n",
      "\n",
      "============================================================\n",
      "Task 2: Baseline Continuous Variables Selection\n",
      "============================================================\n",
      "[OK] Found Age: cw23p003\n",
      "[OK] Found Children_Count: cf23p455\n",
      "[OK] Found SelfRated_Health: ch23p004\n",
      "\n",
      "============================================================\n",
      "Task 3: Education Level Recoding\n",
      "============================================================\n",
      "[OK] Created Education_Recode from cw23p005\n",
      "\n",
      "  Original education codes distribution:\n",
      "    Range: 1 - 28\n",
      "    Unique values: 28\n",
      "\n",
      "  Recoded education levels:\n",
      "    1 (低教育): 625 cases\n",
      "    2 (中教育): 1,265 cases\n",
      "    3 (高教育): 2,349 cases\n",
      "    4 (最高教育): 900 cases\n",
      "    Missing: 7,800 cases\n",
      "\n",
      "============================================================\n",
      "Task 4: Income Quartile Binning\n",
      "============================================================\n",
      "  Income quartile boundaries:\n",
      "    Q1 (25th percentile): -9,999,999,999.00\n",
      "    Q2 (50th percentile/median): -9,999,999,998.00\n",
      "    Q3 (75th percentile): 32,142.50\n",
      "[OK] Created Income_Quartile from ci23p337\n",
      "\n",
      "  Income quartile distribution:\n",
      "    Quartile 1: 1,998 cases (41.4%)\n",
      "    Quartile 2: 682 cases (14.1%)\n",
      "    Quartile 3: 941 cases (19.5%)\n",
      "    Quartile 4: 1,207 cases (25.0%)\n",
      "    Missing: 8,111 cases\n",
      "\n",
      "============================================================\n",
      "Task 5: Validation Output\n",
      "============================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "Delta Variables - Summary Statistics\n",
      "------------------------------------------------------------\n",
      "\n",
      "       Delta_WFC_Proxy  Delta_Health_Hindrance\n",
      "count            163.0             4885.000000\n",
      "mean               0.0                0.011464\n",
      "std                0.0                1.013965\n",
      "min                0.0               -4.000000\n",
      "25%                0.0                0.000000\n",
      "50%                0.0                0.000000\n",
      "75%                0.0                0.000000\n",
      "max                0.0                4.000000\n",
      "\n",
      "------------------------------------------------------------\n",
      "Additional Statistics\n",
      "------------------------------------------------------------\n",
      "\n",
      "Variable                  Mean         Std          Min          Max          Missing   \n",
      "-------------------------------------------------------------------------------------\n",
      "Delta_WFC_Proxy                 0.000       0.000       0.000       0.000    12,776\n",
      "Delta_Health_Hindrance          0.011       1.014      -4.000       4.000     8,054\n",
      "\n",
      "------------------------------------------------------------\n",
      "Education_Recode - Value Counts\n",
      "------------------------------------------------------------\n",
      "\n",
      "Level      Label           Count           Percentage\n",
      "--------------------------------------------------\n",
      "1          低教育 (Low)       625                12.16%\n",
      "2          中教育 (Medium)    1,265              24.62%\n",
      "3          高教育 (High)      2,349              45.71%\n",
      "4          最高教育 (Highest)  900                17.51%\n",
      "Missing                    7,800              60.28%\n",
      "\n",
      "Total cases: 12,939\n",
      "\n",
      "------------------------------------------------------------\n",
      "Income_Quartile - Value Counts\n",
      "------------------------------------------------------------\n",
      "\n",
      "Quartile   Label                Count           Percentage\n",
      "-------------------------------------------------------\n",
      "1          Q1 (Lowest 25%)      1,998              41.38%\n",
      "2          Q2 (25-50%)          682                14.13%\n",
      "3          Q3 (50-75%)          941                19.49%\n",
      "4          Q4 (Highest 25%)     1,207              25.00%\n",
      "Missing                         8,111              62.69%\n",
      "\n",
      "Total cases: 12,939\n",
      "\n",
      "------------------------------------------------------------\n",
      "Baseline Continuous Variables - Summary Statistics\n",
      "------------------------------------------------------------\n",
      "\n",
      "          cw23p003     cf23p455     ch23p004\n",
      "count  5139.000000  3529.000000  6058.000000\n",
      "mean     54.651294     2.249929     3.121987\n",
      "std      18.397365     0.983071     0.804976\n",
      "min      16.000000     0.000000     1.000000\n",
      "25%      40.000000     2.000000     3.000000\n",
      "50%      58.000000     2.000000     3.000000\n",
      "75%      70.000000     3.000000     4.000000\n",
      "max      96.000000     9.000000     5.000000\n",
      "\n",
      "============================================================\n",
      "[OK] Module 2/4 completed!\n",
      "Updated dataset is available as 'merged_df' variable\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 7:  Remaining Delta Variables & Baseline Control Variables Preparation\n",
    "#  Remaining Delta Variables & Baseline Control Variables Preparation\n",
    "print(\"=\"*60)\n",
    "print(\"Module 2/4: Remaining Delta Variables & Baseline Controls\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'merged_df' in globals() and merged_df is not None:\n",
    "    df = merged_df.copy()\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 1: Calculate Last 2 Delta Variables (WLB/Health Stress)\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 1: Calculating Last 2 Delta Variables\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Delta_WFC_Proxy = cw24q391 - cw23p391\n",
    "    if 'cw24q391' in df.columns and 'cw23p391' in df.columns:\n",
    "        w24 = pd.to_numeric(df['cw24q391'], errors='coerce')\n",
    "        w23 = pd.to_numeric(df['cw23p391'], errors='coerce')\n",
    "        df['Delta_WFC_Proxy'] = w24 - w23\n",
    "        print(\"[OK] Created Delta_WFC_Proxy = cw24q391 - cw23p391\")\n",
    "    else:\n",
    "        missing = [v for v in ['cw24q391', 'cw23p391'] if v not in df.columns]\n",
    "        print(f\"[WARNING] Warning: Missing variables for Delta_WFC_Proxy: {missing}\")\n",
    "    \n",
    "    # Delta_Health_Hindrance = ch24q022 - ch23p022\n",
    "    if 'ch24q022' in df.columns and 'ch23p022' in df.columns:\n",
    "        w24 = pd.to_numeric(df['ch24q022'], errors='coerce')\n",
    "        w23 = pd.to_numeric(df['ch23p022'], errors='coerce')\n",
    "        df['Delta_Health_Hindrance'] = w24 - w23\n",
    "        print(\"[OK] Created Delta_Health_Hindrance = ch24q022 - ch23p022\")\n",
    "    else:\n",
    "        missing = [v for v in ['ch24q022', 'ch23p022'] if v not in df.columns]\n",
    "        print(f\"[WARNING] Warning: Missing variables for Delta_Health_Hindrance: {missing}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 2: Baseline Continuous Variables Selection\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Baseline Continuous Variables Selection\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    baseline_continuous_vars = {\n",
    "        'Age': 'cw23p003',\n",
    "        'Children_Count': 'cf23p455',\n",
    "        'SelfRated_Health': 'ch23p004'\n",
    "    }\n",
    "    \n",
    "    baseline_vars_found = {}\n",
    "    baseline_vars_missing = {}\n",
    "    \n",
    "    for var_name, var_code in baseline_continuous_vars.items():\n",
    "        if var_code in df.columns:\n",
    "            # Convert to numeric\n",
    "            df[var_code] = pd.to_numeric(df[var_code], errors='coerce')\n",
    "            baseline_vars_found[var_name] = var_code\n",
    "            print(f\"[OK] Found {var_name}: {var_code}\")\n",
    "        else:\n",
    "            baseline_vars_missing[var_name] = var_code\n",
    "            print(f\"[WARNING] Warning: {var_name} ({var_code}) not found in dataset\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: Education Level Recoding\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Education Level Recoding\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'cw23p005' in df.columns:\n",
    "        # Convert to numeric\n",
    "        df['cw23p005'] = pd.to_numeric(df['cw23p005'], errors='coerce')\n",
    "        \n",
    "        # Recode education levels\n",
    "        # 1 (低教育): codes 1-7\n",
    "        # 2 (中教育): codes 8-15\n",
    "        # 3 (高教育): codes 16-22\n",
    "        # 4 (最高教育): codes 23-28\n",
    "        \n",
    "        def recode_education(value):\n",
    "            if pd.isna(value):\n",
    "                return np.nan\n",
    "            value = int(value)\n",
    "            if 1 <= value <= 7:\n",
    "                return 1  # 低教育\n",
    "            elif 8 <= value <= 15:\n",
    "                return 2  # 中教育\n",
    "            elif 16 <= value <= 22:\n",
    "                return 3  # 高教育\n",
    "            elif 23 <= value <= 28:\n",
    "                return 4  # 最高教育\n",
    "            else:\n",
    "                return np.nan  # Out of range\n",
    "        \n",
    "        df['Education_Recode'] = df['cw23p005'].apply(recode_education)\n",
    "        \n",
    "        # Count original values\n",
    "        original_counts = df['cw23p005'].value_counts().sort_index()\n",
    "        recoded_counts = df['Education_Recode'].value_counts().sort_index()\n",
    "        \n",
    "        print(\"[OK] Created Education_Recode from cw23p005\")\n",
    "        print(f\"\\n  Original education codes distribution:\")\n",
    "        print(f\"    Range: {df['cw23p005'].min():.0f} - {df['cw23p005'].max():.0f}\")\n",
    "        print(f\"    Unique values: {df['cw23p005'].nunique()}\")\n",
    "        print(f\"\\n  Recoded education levels:\")\n",
    "        print(f\"    1 (低教育): {recoded_counts.get(1, 0):,} cases\")\n",
    "        print(f\"    2 (中教育): {recoded_counts.get(2, 0):,} cases\")\n",
    "        print(f\"    3 (高教育): {recoded_counts.get(3, 0):,} cases\")\n",
    "        print(f\"    4 (最高教育): {recoded_counts.get(4, 0):,} cases\")\n",
    "        print(f\"    Missing: {df['Education_Recode'].isna().sum():,} cases\")\n",
    "    else:\n",
    "        print(\"[WARNING] Warning: cw23p005 (W16 highest education) not found in dataset\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 4: Income Quartile Binning\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 4: Income Quartile Binning\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'ci23p337' in df.columns:\n",
    "        # Convert to numeric\n",
    "        df['ci23p337'] = pd.to_numeric(df['ci23p337'], errors='coerce')\n",
    "        \n",
    "        # Calculate quartiles (excluding NaN values)\n",
    "        income_valid = df['ci23p337'].dropna()\n",
    "        \n",
    "        if len(income_valid) > 0:\n",
    "            # Calculate quartile boundaries\n",
    "            q1 = income_valid.quantile(0.25)\n",
    "            q2 = income_valid.quantile(0.50)  # median\n",
    "            q3 = income_valid.quantile(0.75)\n",
    "            \n",
    "            print(f\"  Income quartile boundaries:\")\n",
    "            print(f\"    Q1 (25th percentile): {q1:,.2f}\")\n",
    "            print(f\"    Q2 (50th percentile/median): {q2:,.2f}\")\n",
    "            print(f\"    Q3 (75th percentile): {q3:,.2f}\")\n",
    "            \n",
    "            # Create quartile bins\n",
    "            def assign_quartile(value):\n",
    "                if pd.isna(value):\n",
    "                    return np.nan\n",
    "                if value <= q1:\n",
    "                    return 1  # Q1 (lowest)\n",
    "                elif value <= q2:\n",
    "                    return 2  # Q2\n",
    "                elif value <= q3:\n",
    "                    return 3  # Q3\n",
    "                else:\n",
    "                    return 4  # Q4 (highest)\n",
    "            \n",
    "            df['Income_Quartile'] = df['ci23p337'].apply(assign_quartile)\n",
    "            \n",
    "            quartile_counts = df['Income_Quartile'].value_counts().sort_index()\n",
    "            \n",
    "            print(\"[OK] Created Income_Quartile from ci23p337\")\n",
    "            print(f\"\\n  Income quartile distribution:\")\n",
    "            for q in [1, 2, 3, 4]:\n",
    "                count = quartile_counts.get(q, 0)\n",
    "                pct = (count / len(income_valid)) * 100 if len(income_valid) > 0 else 0\n",
    "                print(f\"    Quartile {q}: {count:,} cases ({pct:.1f}%)\")\n",
    "            print(f\"    Missing: {df['Income_Quartile'].isna().sum():,} cases\")\n",
    "        else:\n",
    "            print(\"[WARNING] Warning: No valid income values found in ci23p337\")\n",
    "    else:\n",
    "        print(\"[WARNING] Warning: ci23p337 (W16 total taxable income) not found in dataset\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 5: Output Validation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 5: Validation Output\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Print Delta variables summary statistics\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Delta Variables - Summary Statistics\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    delta_cols = [col for col in ['Delta_WFC_Proxy', 'Delta_Health_Hindrance'] \n",
    "                  if col in df.columns]\n",
    "    \n",
    "    if delta_cols:\n",
    "        print(\"\\n\" + df[delta_cols].describe().to_string())\n",
    "        \n",
    "        # Additional statistics\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"Additional Statistics\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"\\n{'Variable':<25} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12} {'Missing':<10}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        for col in delta_cols:\n",
    "            mean_val = df[col].mean()\n",
    "            std_val = df[col].std()\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            missing = df[col].isna().sum()\n",
    "            print(f\"{col:<25} {mean_val:>11.3f} {std_val:>11.3f} {min_val:>11.3f} {max_val:>11.3f} {missing:>9,}\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] No Delta variables found in dataset\")\n",
    "    \n",
    "    # Print Education_Recode value counts\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Education_Recode - Value Counts\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    if 'Education_Recode' in df.columns:\n",
    "        edu_counts = df['Education_Recode'].value_counts().sort_index()\n",
    "        edu_pct = df['Education_Recode'].value_counts(normalize=True).sort_index() * 100\n",
    "        \n",
    "        print(f\"\\n{'Level':<10} {'Label':<15} {'Count':<15} {'Percentage':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        edu_labels = {\n",
    "            1: '低教育 (Low)',\n",
    "            2: '中教育 (Medium)',\n",
    "            3: '高教育 (High)',\n",
    "            4: '最高教育 (Highest)'\n",
    "        }\n",
    "        \n",
    "        for level in [1, 2, 3, 4]:\n",
    "            count = edu_counts.get(level, 0)\n",
    "            pct = edu_pct.get(level, 0)\n",
    "            label = edu_labels.get(level, 'Unknown')\n",
    "            print(f\"{level:<10} {label:<15} {count:<15,} {pct:>8.2f}%\")\n",
    "        \n",
    "        missing_edu = df['Education_Recode'].isna().sum()\n",
    "        if missing_edu > 0:\n",
    "            missing_pct = (missing_edu / len(df)) * 100\n",
    "            print(f\"{'Missing':<10} {'':<15} {missing_edu:<15,} {missing_pct:>8.2f}%\")\n",
    "        \n",
    "        print(f\"\\nTotal cases: {len(df):,}\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] Education_Recode not found in dataset\")\n",
    "    \n",
    "    # Print Income_Quartile value counts\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Income_Quartile - Value Counts\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    if 'Income_Quartile' in df.columns:\n",
    "        income_counts = df['Income_Quartile'].value_counts().sort_index()\n",
    "        income_pct = df['Income_Quartile'].value_counts(normalize=True).sort_index() * 100\n",
    "        \n",
    "        print(f\"\\n{'Quartile':<10} {'Label':<20} {'Count':<15} {'Percentage':<10}\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        quartile_labels = {\n",
    "            1: 'Q1 (Lowest 25%)',\n",
    "            2: 'Q2 (25-50%)',\n",
    "            3: 'Q3 (50-75%)',\n",
    "            4: 'Q4 (Highest 25%)'\n",
    "        }\n",
    "        \n",
    "        for q in [1, 2, 3, 4]:\n",
    "            count = income_counts.get(q, 0)\n",
    "            pct = income_pct.get(q, 0)\n",
    "            label = quartile_labels.get(q, 'Unknown')\n",
    "            print(f\"{q:<10} {label:<20} {count:<15,} {pct:>8.2f}%\")\n",
    "        \n",
    "        missing_income = df['Income_Quartile'].isna().sum()\n",
    "        if missing_income > 0:\n",
    "            missing_pct = (missing_income / len(df)) * 100\n",
    "            print(f\"{'Missing':<10} {'':<20} {missing_income:<15,} {missing_pct:>8.2f}%\")\n",
    "        \n",
    "        print(f\"\\nTotal cases: {len(df):,}\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] Income_Quartile not found in dataset\")\n",
    "    \n",
    "    # Print baseline continuous variables summary\n",
    "    if baseline_vars_found:\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"Baseline Continuous Variables - Summary Statistics\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        baseline_cols = list(baseline_vars_found.values())\n",
    "        print(\"\\n\" + df[baseline_cols].describe().to_string())\n",
    "    \n",
    "    # Update merged_df\n",
    "    merged_df = df\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[OK] Module 2/4 completed!\")\n",
    "    print(\"Updated dataset is available as 'merged_df' variable\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[ERROR] Error: 'merged_df' not found. Please run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MICE Imputation using IterativeImputer\n",
      "============================================================\n",
      "\n",
      "Input dataset shape: (12939, 111)\n",
      "  Rows: 12,939\n",
      "  Columns: 111\n",
      "\n",
      "============================================================\n",
      "Task 1: Defining Imputation Feature Set\n",
      "============================================================\n",
      "\n",
      "1. Big Five Scores: 5 features\n",
      "   - N_Score\n",
      "   - E_Score\n",
      "   - C_Score\n",
      "   - A_Score\n",
      "   - O_Score\n",
      "\n",
      "2. Delta Change Variables: 7 features\n",
      "   - Delta_JobSatisfaction\n",
      "   - Delta_Autonomy\n",
      "   - Delta_Workload\n",
      "   - Delta_Skills\n",
      "   - Delta_RemoteHours\n",
      "   - Delta_WFC_Proxy\n",
      "   - Delta_Health_Hindrance\n",
      "\n",
      "3. Continuous Control Variables: 3 features\n",
      "   - cw23p003\n",
      "   - cf23p455\n",
      "   - ch23p004\n",
      "\n",
      "4. Income and Occupation Variables (auxiliary): 6 features\n",
      "   - ci23p337\n",
      "   - ci24q337\n",
      "   - ci23p383\n",
      "   - cw23p525\n",
      "   - ci24q383\n",
      "   - cw24q525\n",
      "\n",
      "------------------------------------------------------------\n",
      "Imputation Feature Set Summary:\n",
      "------------------------------------------------------------\n",
      "  Total features for imputation: 21\n",
      "  Excluded variables: 1 (ID + OHE dummies)\n",
      "\n",
      "  Available features for imputation: 21\n",
      "\n",
      "  Missing values in imputation features: 182,507\n",
      "  Features with missing values:\n",
      "    - Delta_WFC_Proxy: 12,776 (98.7%)\n",
      "    - Delta_JobSatisfaction: 10,971 (84.8%)\n",
      "    - Delta_RemoteHours: 10,909 (84.3%)\n",
      "    - Delta_Autonomy: 10,685 (82.6%)\n",
      "    - Delta_Workload: 10,685 (82.6%)\n",
      "    - Delta_Skills: 10,685 (82.6%)\n",
      "    - cf23p455: 9,410 (72.7%)\n",
      "    - ci23p337: 8,111 (62.7%)\n",
      "    - Delta_Health_Hindrance: 8,054 (62.2%)\n",
      "    - ci23p383: 8,034 (62.1%)\n",
      "    ... and 11 more features with missing values\n",
      "\n",
      "============================================================\n",
      "Task 2: Executing MICE Imputation (M=5)\n",
      "============================================================\n",
      "\n",
      "Converting features to numeric type...\n",
      "\n",
      "Imputation setup:\n",
      "  Features to impute: 21\n",
      "  Observations: 12,939\n",
      "  Total missing values: 182,621\n",
      "\n",
      "  Final validation:\n",
      "    Data type: <class 'numpy.ndarray'>\n",
      "    Shape: (12939, 21)\n",
      "    Dtype: float64\n",
      "    Contains NaN: True\n",
      "\n",
      "IterativeImputer configuration:\n",
      "  Estimator: BayesianRidge\n",
      "  Max iterations: 10\n",
      "  Random state: 42\n",
      "\n",
      "Performing 5 imputations...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Imputation 1/5... Complete (missing values: 0)\n",
      "\n",
      "Imputation 2/5... Complete (missing values: 0)\n",
      "\n",
      "Imputation 3/5... Complete (missing values: 0)\n",
      "\n",
      "Imputation 4/5... Complete (missing values: 0)\n",
      "\n",
      "Imputation 5/5... Complete (missing values: 0)\n",
      "\n",
      "------------------------------------------------------------\n",
      "[OK] Successfully created 5 imputed datasets\n",
      "\n",
      "============================================================\n",
      "Task 3: Final Output and Validation\n",
      "============================================================\n",
      "\n",
      "[OK] Successfully created 5 imputed datasets\n",
      "\n",
      "Validation of first imputed dataset (imputed_dataframes[0]):\n",
      "------------------------------------------------------------\n",
      "  Delta_JobSatisfaction missing values: 0 (expected: 0)\n",
      "    [OK] Delta_JobSatisfaction is complete\n",
      "\n",
      "  Age (cw23p003) statistics:\n",
      "    Mean: 53.17\n",
      "    Std: 12.36\n",
      "    Missing values: 0 (expected: 0)\n",
      "    [OK] Age is complete\n",
      "\n",
      "  Overall missing values in imputation features: 0 (expected: 0)\n",
      "    [OK] All imputation features are complete\n",
      "\n",
      "============================================================\n",
      "[OK] MICE imputation completed!\n",
      "  Created 5 imputed datasets\n",
      "  Available as 'imputed_dataframes' variable (list of 5 DataFrames)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 8:  MICE Imputation using IterativeImputer\n",
    "print(\"=\"*60)\n",
    "print(\"MICE Imputation using IterativeImputer\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'merged_df' in globals() and merged_df is not None:\n",
    "    df = merged_df.copy()\n",
    "    \n",
    "    print(f\"\\nInput dataset shape: {df.shape}\")\n",
    "    print(f\"  Rows: {df.shape[0]:,}\")\n",
    "    print(f\"  Columns: {df.shape[1]:,}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 1: Define Imputation Feature Set\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 1: Defining Imputation Feature Set\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    imputation_features = []\n",
    "    \n",
    "    # 1. Big Five Scores (5 features)\n",
    "    big_five_scores = ['N_Score', 'E_Score', 'C_Score', 'A_Score', 'O_Score']\n",
    "    for score in big_five_scores:\n",
    "        if score in df.columns:\n",
    "            imputation_features.append(score)\n",
    "    print(f\"\\n1. Big Five Scores: {len([s for s in big_five_scores if s in df.columns])} features\")\n",
    "    for score in big_five_scores:\n",
    "        if score in df.columns:\n",
    "            print(f\"   - {score}\")\n",
    "    \n",
    "    # 2. Delta change variables (7 features)\n",
    "    delta_variables = [\n",
    "        'Delta_JobSatisfaction',\n",
    "        'Delta_Autonomy',\n",
    "        'Delta_Workload',\n",
    "        'Delta_Skills',\n",
    "        'Delta_RemoteHours',\n",
    "        'Delta_WFC_Proxy',\n",
    "        'Delta_Health_Hindrance'\n",
    "    ]\n",
    "    for delta_var in delta_variables:\n",
    "        if delta_var in df.columns:\n",
    "            imputation_features.append(delta_var)\n",
    "    print(f\"\\n2. Delta Change Variables: {len([v for v in delta_variables if v in df.columns])} features\")\n",
    "    for delta_var in delta_variables:\n",
    "        if delta_var in df.columns:\n",
    "            print(f\"   - {delta_var}\")\n",
    "    \n",
    "    # 3. Continuous control variables (3 features)\n",
    "    continuous_controls = [\n",
    "        'cw23p003',      # Age\n",
    "        'cf23p455',      # Children Count\n",
    "        'ch23p004'       # Self-rated Health\n",
    "    ]\n",
    "    for control_var in continuous_controls:\n",
    "        if control_var in df.columns:\n",
    "            imputation_features.append(control_var)\n",
    "    print(f\"\\n3. Continuous Control Variables: {len([v for v in continuous_controls if v in df.columns])} features\")\n",
    "    for control_var in continuous_controls:\n",
    "        if control_var in df.columns:\n",
    "            print(f\"   - {control_var}\")\n",
    "    \n",
    "    # 4. Income and occupation variables (for auxiliary imputation)\n",
    "    income_occupation_vars = [\n",
    "        'ci23p337',      # W16 Income\n",
    "        'ci24q337',      # W17 Income\n",
    "        'ci23p383',      # W16 Occupation\n",
    "        'cw23p525',      # W16 Main Occupation\n",
    "        'ci24q383',      # W17 Occupation\n",
    "        'cw24q525'       # W17 Main Occupation\n",
    "    ]\n",
    "    for var in income_occupation_vars:\n",
    "        if var in df.columns:\n",
    "            imputation_features.append(var)\n",
    "    print(f\"\\n4. Income and Occupation Variables (auxiliary): {len([v for v in income_occupation_vars if v in df.columns])} features\")\n",
    "    for var in income_occupation_vars:\n",
    "        if var in df.columns:\n",
    "            print(f\"   - {var}\")\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    imputation_features = list(dict.fromkeys(imputation_features))\n",
    "    \n",
    "    # Exclude OHE dummy variables and ID\n",
    "    # Identify dummy variables (typically contain underscores and numbers, or are binary 0/1)\n",
    "    excluded_vars = ['nomem_encr']  # Always exclude ID\n",
    "    \n",
    "    # Exclude OHE dummy variables (they typically have patterns like 'var_name_2.0', 'var_name_3.0', etc.)\n",
    "    dummy_patterns = []\n",
    "    for col in df.columns:\n",
    "        # Check if column looks like a dummy variable (contains underscore and ends with number/float pattern)\n",
    "        if '_' in col and col not in imputation_features:\n",
    "            # Check if it's likely a dummy variable (binary or categorical encoded)\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                unique_vals = df[col].dropna().unique()\n",
    "                if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):\n",
    "                    dummy_patterns.append(col)\n",
    "    \n",
    "    excluded_vars.extend(dummy_patterns)\n",
    "    \n",
    "    # Remove excluded variables from imputation features\n",
    "    imputation_features = [f for f in imputation_features if f not in excluded_vars]\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*60)\n",
    "    print(f\"Imputation Feature Set Summary:\")\n",
    "    print(f\"-\"*60)\n",
    "    print(f\"  Total features for imputation: {len(imputation_features)}\")\n",
    "    print(f\"  Excluded variables: {len(excluded_vars)} (ID + OHE dummies)\")\n",
    "    \n",
    "    # Check which features exist in dataset\n",
    "    available_imputation_features = [f for f in imputation_features if f in df.columns]\n",
    "    missing_imputation_features = [f for f in imputation_features if f not in df.columns]\n",
    "    \n",
    "    if missing_imputation_features:\n",
    "        print(f\"\\n  [WARNING] Missing features: {len(missing_imputation_features)}\")\n",
    "        print(f\"    {', '.join(missing_imputation_features[:10])}{'...' if len(missing_imputation_features) > 10 else ''}\")\n",
    "    \n",
    "    print(f\"\\n  Available features for imputation: {len(available_imputation_features)}\")\n",
    "    \n",
    "    # Check missing values in imputation features\n",
    "    imputation_data = df[available_imputation_features].copy()\n",
    "    missing_counts = imputation_data.isna().sum()\n",
    "    total_missing = missing_counts.sum()\n",
    "    \n",
    "    print(f\"\\n  Missing values in imputation features: {total_missing:,}\")\n",
    "    if total_missing > 0:\n",
    "        print(f\"  Features with missing values:\")\n",
    "        missing_features = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "        for feat, count in missing_features.head(10).items():\n",
    "            pct = (count / len(imputation_data)) * 100\n",
    "            print(f\"    - {feat}: {count:,} ({pct:.1f}%)\")\n",
    "        if len(missing_features) > 10:\n",
    "            print(f\"    ... and {len(missing_features) - 10} more features with missing values\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 2: Execute MICE Imputation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Executing MICE Imputation (M=5)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    import numpy as np\n",
    "    \n",
    "    # Prepare data for imputation\n",
    "    X_impute = imputation_data.copy()\n",
    "    \n",
    "    # Convert all columns to numeric, coercing errors to NaN\n",
    "    print(f\"\\nConverting features to numeric type...\")\n",
    "    numeric_columns = []\n",
    "    non_numeric_columns = []\n",
    "    \n",
    "    for col in available_imputation_features:\n",
    "        if col in X_impute.columns:\n",
    "            # Try to convert to numeric\n",
    "            original_dtype = X_impute[col].dtype\n",
    "            X_impute[col] = pd.to_numeric(X_impute[col], errors='coerce')\n",
    "            \n",
    "            # Check if conversion was successful\n",
    "            if X_impute[col].dtype in ['int64', 'float64']:\n",
    "                numeric_columns.append(col)\n",
    "            else:\n",
    "                non_numeric_columns.append(col)\n",
    "                print(f\"  [WARNING] {col}: Could not convert to numeric (original dtype: {original_dtype})\")\n",
    "    \n",
    "    # Remove non-numeric columns from imputation\n",
    "    if non_numeric_columns:\n",
    "        print(f\"\\n  Excluding {len(non_numeric_columns)} non-numeric columns from imputation:\")\n",
    "        for col in non_numeric_columns:\n",
    "            print(f\"    - {col}\")\n",
    "        available_imputation_features = [f for f in available_imputation_features if f in numeric_columns]\n",
    "        X_impute = X_impute[numeric_columns].copy()\n",
    "    \n",
    "    # Ensure all remaining columns are float64 for IterativeImputer\n",
    "    for col in X_impute.columns:\n",
    "        if X_impute[col].dtype == 'int64':\n",
    "            X_impute[col] = X_impute[col].astype('float64')\n",
    "    \n",
    "    print(f\"\\nImputation setup:\")\n",
    "    print(f\"  Features to impute: {len(available_imputation_features)}\")\n",
    "    print(f\"  Observations: {len(X_impute):,}\")\n",
    "    print(f\"  Total missing values: {X_impute.isna().sum().sum():,}\")\n",
    "    \n",
    "    # Final check: ensure no string values remain\n",
    "    for col in X_impute.columns:\n",
    "        if X_impute[col].dtype == 'object':\n",
    "            print(f\"  [ERROR] {col} is still object type - will be excluded\")\n",
    "            available_imputation_features = [f for f in available_imputation_features if f != col]\n",
    "    \n",
    "    if len(available_imputation_features) == 0:\n",
    "        raise ValueError(\"No numeric features available for imputation!\")\n",
    "    \n",
    "    X_impute = X_impute[available_imputation_features].copy()\n",
    "    \n",
    "    # Final validation: ensure X_impute is purely numeric\n",
    "    # Replace any remaining non-numeric values with NaN\n",
    "    for col in X_impute.columns:\n",
    "        # Convert to numeric again to catch any remaining issues\n",
    "        X_impute[col] = pd.to_numeric(X_impute[col], errors='coerce')\n",
    "        # Ensure float64\n",
    "        if X_impute[col].dtype != 'float64':\n",
    "            X_impute[col] = X_impute[col].astype('float64')\n",
    "    \n",
    "    # Verify no object columns remain\n",
    "    object_cols = X_impute.select_dtypes(include=['object']).columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"\\n  [WARNING] Removing {len(object_cols)} object columns: {list(object_cols)}\")\n",
    "        X_impute = X_impute.select_dtypes(exclude=['object'])\n",
    "        available_imputation_features = [f for f in available_imputation_features if f in X_impute.columns]\n",
    "    \n",
    "    # Convert to numpy array to ensure compatibility\n",
    "    X_impute_values = X_impute.values.astype('float64')\n",
    "    \n",
    "    print(f\"\\n  Final validation:\")\n",
    "    print(f\"    Data type: {type(X_impute_values)}\")\n",
    "    print(f\"    Shape: {X_impute_values.shape}\")\n",
    "    print(f\"    Dtype: {X_impute_values.dtype}\")\n",
    "    print(f\"    Contains NaN: {np.isnan(X_impute_values).any()}\")\n",
    "    \n",
    "    # Initialize IterativeImputer with BayesianRidge as estimator\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=BayesianRidge(),\n",
    "        max_iter=10,\n",
    "        random_state=42,\n",
    "        imputation_order='ascending',\n",
    "        n_nearest_features=None,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nIterativeImputer configuration:\")\n",
    "    print(f\"  Estimator: BayesianRidge\")\n",
    "    print(f\"  Max iterations: 10\")\n",
    "    print(f\"  Random state: 42\")\n",
    "    \n",
    "    # Perform 5 imputations (M=5)\n",
    "    M = 5\n",
    "    imputed_dataframes = []\n",
    "    \n",
    "    print(f\"\\nPerforming {M} imputations...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for m in range(M):\n",
    "        print(f\"\\nImputation {m+1}/{M}...\", end=' ')\n",
    "        \n",
    "        # Fit and transform (each call uses different random initialization)\n",
    "        # Set random_state differently for each imputation to get variation\n",
    "        imputer_m = IterativeImputer(\n",
    "            estimator=BayesianRidge(),\n",
    "            max_iter=10,\n",
    "            random_state=42 + m,  # Different seed for each imputation\n",
    "            imputation_order='ascending',\n",
    "            n_nearest_features=None,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Perform imputation (use numpy array to avoid pandas dtype issues)\n",
    "        X_imputed_array = imputer_m.fit_transform(X_impute_values)\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        X_imputed_df = pd.DataFrame(\n",
    "            X_imputed_array,\n",
    "            columns=available_imputation_features,\n",
    "            index=X_impute.index\n",
    "        )\n",
    "        \n",
    "        # Create complete dataset by replacing imputed columns in original dataframe\n",
    "        df_imputed = df.copy()\n",
    "        for col in available_imputation_features:\n",
    "            df_imputed[col] = X_imputed_df[col]\n",
    "        \n",
    "        imputed_dataframes.append(df_imputed)\n",
    "        \n",
    "        # Check missing values after imputation\n",
    "        missing_after = df_imputed[available_imputation_features].isna().sum().sum()\n",
    "        print(f\"Complete (missing values: {missing_after})\")\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*60)\n",
    "    print(f\"[OK] Successfully created {M} imputed datasets\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: Final Output and Validation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Final Output and Validation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n[OK] Successfully created {len(imputed_dataframes)} imputed datasets\")\n",
    "    \n",
    "    # Validate first imputed dataset\n",
    "    df_first = imputed_dataframes[0]\n",
    "    \n",
    "    print(f\"\\nValidation of first imputed dataset (imputed_dataframes[0]):\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Check Delta_JobSatisfaction missing values\n",
    "    if 'Delta_JobSatisfaction' in df_first.columns:\n",
    "        missing_delta = df_first['Delta_JobSatisfaction'].isna().sum()\n",
    "        print(f\"  Delta_JobSatisfaction missing values: {missing_delta} (expected: 0)\")\n",
    "        if missing_delta == 0:\n",
    "            print(f\"    [OK] Delta_JobSatisfaction is complete\")\n",
    "        else:\n",
    "            print(f\"    [WARNING] Delta_JobSatisfaction still has {missing_delta} missing values\")\n",
    "    else:\n",
    "        print(f\"  [WARNING] Delta_JobSatisfaction not found in dataset\")\n",
    "    \n",
    "    # Check Age (cw23p003) statistics\n",
    "    if 'cw23p003' in df_first.columns:\n",
    "        age_data = df_first['cw23p003'].dropna()\n",
    "        if len(age_data) > 0:\n",
    "            age_mean = age_data.mean()\n",
    "            age_std = age_data.std()\n",
    "            age_missing = df_first['cw23p003'].isna().sum()\n",
    "            print(f\"\\n  Age (cw23p003) statistics:\")\n",
    "            print(f\"    Mean: {age_mean:.2f}\")\n",
    "            print(f\"    Std: {age_std:.2f}\")\n",
    "            print(f\"    Missing values: {age_missing} (expected: 0)\")\n",
    "            if age_missing == 0:\n",
    "                print(f\"    [OK] Age is complete\")\n",
    "        else:\n",
    "            print(f\"  [WARNING] Age (cw23p003) has no valid values\")\n",
    "    else:\n",
    "        print(f\"  [WARNING] Age (cw23p003) not found in dataset\")\n",
    "    \n",
    "    # Overall missing values check\n",
    "    total_missing_first = df_first[available_imputation_features].isna().sum().sum()\n",
    "    print(f\"\\n  Overall missing values in imputation features: {total_missing_first} (expected: 0)\")\n",
    "    if total_missing_first == 0:\n",
    "        print(f\"    [OK] All imputation features are complete\")\n",
    "    else:\n",
    "        print(f\"    [WARNING] Still have {total_missing_first} missing values\")\n",
    "    \n",
    "    # Store imputed datasets in global variable\n",
    "    globals()['imputed_dataframes'] = imputed_dataframes\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"[OK] MICE imputation completed!\")\n",
    "    print(f\"  Created {M} imputed datasets\")\n",
    "    print(f\"  Available as 'imputed_dataframes' variable (list of {M} DataFrames)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[ERROR] Error: 'merged_df' not found. Please run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Module 3/4 & 4/4: MICE Results Final Processing\n",
      "============================================================\n",
      "\n",
      "Found 5 MICE imputed datasets\n",
      "\n",
      "============================================================\n",
      "Processing MICE Dataset 1/5\n",
      "============================================================\n",
      "  Dataset shape: (12939, 111)\n",
      "\n",
      "  Task 2: Categorical Encoding and Binning\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Education_Recode already exists\n",
      "  [OK] Income_Quartile already exists\n",
      "\n",
      "  Performing One-Hot Encoding...\n",
      "    [OK] Gender: Created 2 dummy variables, dropped original\n",
      "    [OK] Income_Quartile: Created 3 dummy variables, dropped original\n",
      "    [OK] Education_Recode: Created 3 dummy variables, dropped original\n",
      "\n",
      "  Task 3: Building and Standardizing X and Y Matrices\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Target variable (Y): Delta_JobSatisfaction\n",
      "    Shape: (12939, 1)\n",
      "  [OK] Big Five Scores: 5 features\n",
      "  [OK] Delta Variables: 6 features\n",
      "  [OK] Baseline Controls: 3 features\n",
      "  [OK] Dummy variables from OHE: 8 features\n",
      "\n",
      "  X matrix shape: (12939, 22)\n",
      "\n",
      "  Standardizing 14 continuous features...\n",
      "    [OK] Standardized 14 continuous features\n",
      "\n",
      "  Task 4: Saving to Disk\n",
      "  ------------------------------------------------------------\n",
      "    [OK] Saved X matrix: LISS_Final_X_M1.csv (4.00 MB)\n",
      "    [OK] Saved Y matrix: LISS_Final_Y_M1.csv (0.28 MB)\n",
      "\n",
      "============================================================\n",
      "Processing MICE Dataset 2/5\n",
      "============================================================\n",
      "  Dataset shape: (12939, 111)\n",
      "\n",
      "  Task 2: Categorical Encoding and Binning\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Education_Recode already exists\n",
      "  [OK] Income_Quartile already exists\n",
      "\n",
      "  Performing One-Hot Encoding...\n",
      "    [OK] Gender: Created 2 dummy variables, dropped original\n",
      "    [OK] Income_Quartile: Created 3 dummy variables, dropped original\n",
      "    [OK] Education_Recode: Created 3 dummy variables, dropped original\n",
      "\n",
      "  Task 3: Building and Standardizing X and Y Matrices\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Target variable (Y): Delta_JobSatisfaction\n",
      "    Shape: (12939, 1)\n",
      "  [OK] Big Five Scores: 5 features\n",
      "  [OK] Delta Variables: 6 features\n",
      "  [OK] Baseline Controls: 3 features\n",
      "  [OK] Dummy variables from OHE: 8 features\n",
      "\n",
      "  X matrix shape: (12939, 22)\n",
      "\n",
      "  Standardizing 14 continuous features...\n",
      "    [OK] Standardized 14 continuous features\n",
      "\n",
      "  Task 4: Saving to Disk\n",
      "  ------------------------------------------------------------\n",
      "    [OK] Saved X matrix: LISS_Final_X_M2.csv (4.00 MB)\n",
      "    [OK] Saved Y matrix: LISS_Final_Y_M2.csv (0.28 MB)\n",
      "\n",
      "============================================================\n",
      "Processing MICE Dataset 3/5\n",
      "============================================================\n",
      "  Dataset shape: (12939, 111)\n",
      "\n",
      "  Task 2: Categorical Encoding and Binning\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Education_Recode already exists\n",
      "  [OK] Income_Quartile already exists\n",
      "\n",
      "  Performing One-Hot Encoding...\n",
      "    [OK] Gender: Created 2 dummy variables, dropped original\n",
      "    [OK] Income_Quartile: Created 3 dummy variables, dropped original\n",
      "    [OK] Education_Recode: Created 3 dummy variables, dropped original\n",
      "\n",
      "  Task 3: Building and Standardizing X and Y Matrices\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Target variable (Y): Delta_JobSatisfaction\n",
      "    Shape: (12939, 1)\n",
      "  [OK] Big Five Scores: 5 features\n",
      "  [OK] Delta Variables: 6 features\n",
      "  [OK] Baseline Controls: 3 features\n",
      "  [OK] Dummy variables from OHE: 8 features\n",
      "\n",
      "  X matrix shape: (12939, 22)\n",
      "\n",
      "  Standardizing 14 continuous features...\n",
      "    [OK] Standardized 14 continuous features\n",
      "\n",
      "  Task 4: Saving to Disk\n",
      "  ------------------------------------------------------------\n",
      "    [OK] Saved X matrix: LISS_Final_X_M3.csv (4.00 MB)\n",
      "    [OK] Saved Y matrix: LISS_Final_Y_M3.csv (0.28 MB)\n",
      "\n",
      "============================================================\n",
      "Processing MICE Dataset 4/5\n",
      "============================================================\n",
      "  Dataset shape: (12939, 111)\n",
      "\n",
      "  Task 2: Categorical Encoding and Binning\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Education_Recode already exists\n",
      "  [OK] Income_Quartile already exists\n",
      "\n",
      "  Performing One-Hot Encoding...\n",
      "    [OK] Gender: Created 2 dummy variables, dropped original\n",
      "    [OK] Income_Quartile: Created 3 dummy variables, dropped original\n",
      "    [OK] Education_Recode: Created 3 dummy variables, dropped original\n",
      "\n",
      "  Task 3: Building and Standardizing X and Y Matrices\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Target variable (Y): Delta_JobSatisfaction\n",
      "    Shape: (12939, 1)\n",
      "  [OK] Big Five Scores: 5 features\n",
      "  [OK] Delta Variables: 6 features\n",
      "  [OK] Baseline Controls: 3 features\n",
      "  [OK] Dummy variables from OHE: 8 features\n",
      "\n",
      "  X matrix shape: (12939, 22)\n",
      "\n",
      "  Standardizing 14 continuous features...\n",
      "    [OK] Standardized 14 continuous features\n",
      "\n",
      "  Task 4: Saving to Disk\n",
      "  ------------------------------------------------------------\n",
      "    [OK] Saved X matrix: LISS_Final_X_M4.csv (4.00 MB)\n",
      "    [OK] Saved Y matrix: LISS_Final_Y_M4.csv (0.28 MB)\n",
      "\n",
      "============================================================\n",
      "Processing MICE Dataset 5/5\n",
      "============================================================\n",
      "  Dataset shape: (12939, 111)\n",
      "\n",
      "  Task 2: Categorical Encoding and Binning\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Education_Recode already exists\n",
      "  [OK] Income_Quartile already exists\n",
      "\n",
      "  Performing One-Hot Encoding...\n",
      "    [OK] Gender: Created 2 dummy variables, dropped original\n",
      "    [OK] Income_Quartile: Created 3 dummy variables, dropped original\n",
      "    [OK] Education_Recode: Created 3 dummy variables, dropped original\n",
      "\n",
      "  Task 3: Building and Standardizing X and Y Matrices\n",
      "  ------------------------------------------------------------\n",
      "  [OK] Target variable (Y): Delta_JobSatisfaction\n",
      "    Shape: (12939, 1)\n",
      "  [OK] Big Five Scores: 5 features\n",
      "  [OK] Delta Variables: 6 features\n",
      "  [OK] Baseline Controls: 3 features\n",
      "  [OK] Dummy variables from OHE: 8 features\n",
      "\n",
      "  X matrix shape: (12939, 22)\n",
      "\n",
      "  Standardizing 14 continuous features...\n",
      "    [OK] Standardized 14 continuous features\n",
      "\n",
      "  Task 4: Saving to Disk\n",
      "  ------------------------------------------------------------\n",
      "    [OK] Saved X matrix: LISS_Final_X_M5.csv (4.00 MB)\n",
      "    [OK] Saved Y matrix: LISS_Final_Y_M5.csv (0.28 MB)\n",
      "\n",
      "============================================================\n",
      "Task 5: Final Output Validation\n",
      "============================================================\n",
      "\n",
      "[OK] Successfully processed 5 MICE datasets\n",
      "\n",
      "Validation of first dataset (M1):\n",
      "  X matrix shape: (12939, 22)\n",
      "  Y matrix shape: (12939, 1)\n",
      "\n",
      "  X matrix column names (22 features):\n",
      "  ------------------------------------------------------------\n",
      "     1. N_Score\n",
      "     2. E_Score\n",
      "     3. C_Score\n",
      "     4. A_Score\n",
      "     5. O_Score\n",
      "     6. Delta_Autonomy\n",
      "     7. Delta_Workload\n",
      "     8. Delta_Skills\n",
      "     9. Delta_RemoteHours\n",
      "    10. Delta_WFC_Proxy\n",
      "    11. Delta_Health_Hindrance\n",
      "    12. cw23p003\n",
      "    13. cf23p455\n",
      "    14. ch23p004\n",
      "    15. ch23p001_2.0\n",
      "    16. ch23p001_3.0\n",
      "    17. Income_Quartile_2.0\n",
      "    18. Income_Quartile_3.0\n",
      "    19. Income_Quartile_4.0\n",
      "    20. Education_Recode_2.0\n",
      "    21. Education_Recode_3.0\n",
      "    22. Education_Recode_4.0\n",
      "\n",
      "  Y matrix:\n",
      "    Target variable: Delta_JobSatisfaction\n",
      "    Missing values: 0\n",
      "\n",
      "============================================================\n",
      "[OK] Module 3/4 & 4/4 completed!\n",
      "  Processed 5 MICE datasets\n",
      "  Final X and Y matrices available as 'final_X_matrices' and 'final_Y_matrices'\n",
      "  Saved files: LISS_Final_X_M1.csv to LISS_Final_X_M5.csv\n",
      "               LISS_Final_Y_M1.csv to LISS_Final_Y_M5.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 9:  Module 3/4 & 4/4 - MICE Results Final Processing\n",
    "# Module 3/4 & 4/4: MICE Results Final Processing\n",
    "print(\"=\"*60)\n",
    "print(\"Module 3/4 & 4/4: MICE Results Final Processing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'imputed_dataframes' in globals() and imputed_dataframes is not None and len(imputed_dataframes) > 0:\n",
    "    print(f\"\\nFound {len(imputed_dataframes)} MICE imputed datasets\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Loop through MICE datasets\n",
    "    # ============================================================================\n",
    "    M = len(imputed_dataframes)\n",
    "    final_X_matrices = []\n",
    "    final_Y_matrices = []\n",
    "    \n",
    "    for m_idx, df_imputed in enumerate(imputed_dataframes, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing MICE Dataset {m_idx}/{M}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        df = df_imputed.copy()\n",
    "        print(f\"  Dataset shape: {df.shape}\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Task 2: Categorical Encoding (OHE) and Binning\n",
    "        # ============================================================================\n",
    "        print(f\"\\n  Task 2: Categorical Encoding and Binning\")\n",
    "        print(f\"  {'-'*60}\")\n",
    "        \n",
    "        # 2.1 Education Recode (if not already done)\n",
    "        if 'Education_Recode' not in df.columns and 'cw23p005' in df.columns:\n",
    "            print(f\"\\n  Creating Education_Recode from cw23p005...\")\n",
    "            \n",
    "            def recode_education(value):\n",
    "                if pd.isna(value):\n",
    "                    return np.nan\n",
    "                value = float(value)\n",
    "                if 1 <= value <= 7:\n",
    "                    return 1  # Low education\n",
    "                elif 8 <= value <= 15:\n",
    "                    return 2  # Medium education\n",
    "                elif 16 <= value <= 22:\n",
    "                    return 3  # High education\n",
    "                elif 23 <= value <= 28:\n",
    "                    return 4  # Highest education\n",
    "                else:\n",
    "                    return np.nan\n",
    "            \n",
    "            df['cw23p005'] = pd.to_numeric(df['cw23p005'], errors='coerce')\n",
    "            df['Education_Recode'] = df['cw23p005'].apply(recode_education)\n",
    "            print(f\"    [OK] Created Education_Recode\")\n",
    "        elif 'Education_Recode' in df.columns:\n",
    "            print(f\"  [OK] Education_Recode already exists\")\n",
    "        else:\n",
    "            print(f\"  [WARNING] cw23p005 not found, skipping Education_Recode\")\n",
    "        \n",
    "        # 2.2 Income Quartile (if not already done)\n",
    "        if 'Income_Quartile' not in df.columns and 'ci23p337' in df.columns:\n",
    "            print(f\"\\n  Creating Income_Quartile from ci23p337...\")\n",
    "            \n",
    "            income_data = pd.to_numeric(df['ci23p337'], errors='coerce')\n",
    "            valid_income = income_data.dropna()\n",
    "            \n",
    "            if len(valid_income) > 0:\n",
    "                quartiles = valid_income.quantile([0.25, 0.5, 0.75])\n",
    "                \n",
    "                def assign_quartile(value):\n",
    "                    if pd.isna(value):\n",
    "                        return np.nan\n",
    "                    value = float(value)\n",
    "                    if value <= quartiles[0.25]:\n",
    "                        return 1\n",
    "                    elif value <= quartiles[0.5]:\n",
    "                        return 2\n",
    "                    elif value <= quartiles[0.75]:\n",
    "                        return 3\n",
    "                    else:\n",
    "                        return 4\n",
    "                \n",
    "                df['Income_Quartile'] = income_data.apply(assign_quartile)\n",
    "                print(f\"    [OK] Created Income_Quartile\")\n",
    "            else:\n",
    "                print(f\"    [WARNING] No valid income values found\")\n",
    "        elif 'Income_Quartile' in df.columns:\n",
    "            print(f\"  [OK] Income_Quartile already exists\")\n",
    "        else:\n",
    "            print(f\"  [WARNING] ci23p337 not found, skipping Income_Quartile\")\n",
    "        \n",
    "        # 2.3 One-Hot Encoding (OHE)\n",
    "        print(f\"\\n  Performing One-Hot Encoding...\")\n",
    "        \n",
    "        ohe_variables = {\n",
    "            'ch23p001': 'Gender',\n",
    "            'Income_Quartile': 'Income_Quartile',\n",
    "            'Education_Recode': 'Education_Recode'\n",
    "        }\n",
    "        \n",
    "        ohe_dummy_vars = {}\n",
    "        \n",
    "        for var_name, var_label in ohe_variables.items():\n",
    "            if var_name in df.columns:\n",
    "                # Convert to numeric if needed\n",
    "                df[var_name] = pd.to_numeric(df[var_name], errors='coerce')\n",
    "                \n",
    "                # Perform OHE\n",
    "                dummies = pd.get_dummies(df[var_name], prefix=var_name, drop_first=True)\n",
    "                \n",
    "                # Add dummy variables to dataframe\n",
    "                for dummy_col in dummies.columns:\n",
    "                    df[dummy_col] = dummies[dummy_col]\n",
    "                    if var_name not in ohe_dummy_vars:\n",
    "                        ohe_dummy_vars[var_name] = []\n",
    "                    ohe_dummy_vars[var_name].append(dummy_col)\n",
    "                \n",
    "                # Drop original variable\n",
    "                df = df.drop(columns=[var_name])\n",
    "                \n",
    "                print(f\"    [OK] {var_label}: Created {len(dummies.columns)} dummy variables, dropped original\")\n",
    "            else:\n",
    "                print(f\"    [WARNING] {var_label} ({var_name}) not found\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Task 3: Build and Standardize Matrices\n",
    "        # ============================================================================\n",
    "        print(f\"\\n  Task 3: Building and Standardizing X and Y Matrices\")\n",
    "        print(f\"  {'-'*60}\")\n",
    "        \n",
    "        # Define Y (target variable)\n",
    "        target_var = 'Delta_JobSatisfaction'\n",
    "        if target_var in df.columns:\n",
    "            Y = df[[target_var]].copy()\n",
    "            print(f\"  [OK] Target variable (Y): {target_var}\")\n",
    "            print(f\"    Shape: {Y.shape}\")\n",
    "        else:\n",
    "            print(f\"  [ERROR] Target variable {target_var} not found!\")\n",
    "            continue\n",
    "        \n",
    "        # Define X features\n",
    "        X_features = []\n",
    "        \n",
    "        # Big Five Scores (5 features)\n",
    "        big_five_scores = ['N_Score', 'E_Score', 'C_Score', 'A_Score', 'O_Score']\n",
    "        X_big_five = [f for f in big_five_scores if f in df.columns]\n",
    "        X_features.extend(X_big_five)\n",
    "        print(f\"  [OK] Big Five Scores: {len(X_big_five)} features\")\n",
    "        \n",
    "        # Delta variables (7 features)\n",
    "        delta_vars = [\n",
    "            'Delta_JobSatisfaction',  # Exclude from X (it's Y)\n",
    "            'Delta_Autonomy',\n",
    "            'Delta_Workload',\n",
    "            'Delta_Skills',\n",
    "            'Delta_RemoteHours',\n",
    "            'Delta_WFC_Proxy',\n",
    "            'Delta_Health_Hindrance'\n",
    "        ]\n",
    "        X_delta = [f for f in delta_vars if f in df.columns and f != target_var]\n",
    "        X_features.extend(X_delta)\n",
    "        print(f\"  [OK] Delta Variables: {len(X_delta)} features\")\n",
    "        \n",
    "        # Continuous control variables (3 features)\n",
    "        continuous_controls = ['cw23p003', 'cf23p455', 'ch23p004']\n",
    "        X_controls = [f for f in continuous_controls if f in df.columns]\n",
    "        X_features.extend(X_controls)\n",
    "        print(f\"  [OK] Baseline Controls: {len(X_controls)} features\")\n",
    "        \n",
    "        # OHE dummy variables\n",
    "        X_dummy = []\n",
    "        for var_name, dummy_cols in ohe_dummy_vars.items():\n",
    "            for dummy_col in dummy_cols:\n",
    "                if dummy_col in df.columns:\n",
    "                    X_dummy.append(dummy_col)\n",
    "        X_features.extend(X_dummy)\n",
    "        print(f\"  [OK] Dummy variables from OHE: {len(X_dummy)} features\")\n",
    "        \n",
    "        # Create X matrix\n",
    "        X = df[X_features].copy()\n",
    "        print(f\"\\n  X matrix shape: {X.shape}\")\n",
    "        \n",
    "        # Identify continuous features for standardization\n",
    "        continuous_features = X_big_five + X_delta + X_controls\n",
    "        \n",
    "        # Standardize continuous features (Z-Score)\n",
    "        print(f\"\\n  Standardizing {len(continuous_features)} continuous features...\")\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_continuous_data = X[continuous_features].values\n",
    "        X_continuous_scaled = scaler.fit_transform(X_continuous_data)\n",
    "        \n",
    "        # Replace standardized values in X\n",
    "        for idx, feat in enumerate(continuous_features):\n",
    "            X[feat] = X_continuous_scaled[:, idx]\n",
    "        \n",
    "        print(f\"    [OK] Standardized {len(continuous_features)} continuous features\")\n",
    "        \n",
    "        # Store matrices\n",
    "        final_X_matrices.append(X)\n",
    "        final_Y_matrices.append(Y)\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Task 4: Save to Disk\n",
    "        # ============================================================================\n",
    "        print(f\"\\n  Task 4: Saving to Disk\")\n",
    "        print(f\"  {'-'*60}\")\n",
    "        \n",
    "        X_filename = f'LISS_Final_X_M{m_idx}.csv'\n",
    "        Y_filename = f'LISS_Final_Y_M{m_idx}.csv'\n",
    "        \n",
    "        X.to_csv(X_filename, index=True, encoding='utf-8')\n",
    "        Y.to_csv(Y_filename, index=True, encoding='utf-8')\n",
    "        \n",
    "        import os\n",
    "        X_size = os.path.getsize(X_filename) / (1024 * 1024)\n",
    "        Y_size = os.path.getsize(Y_filename) / (1024 * 1024)\n",
    "        \n",
    "        print(f\"    [OK] Saved X matrix: {X_filename} ({X_size:.2f} MB)\")\n",
    "        print(f\"    [OK] Saved Y matrix: {Y_filename} ({Y_size:.2f} MB)\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 5: Output Validation\n",
    "    # ============================================================================\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Task 5: Final Output Validation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n[OK] Successfully processed {M} MICE datasets\")\n",
    "    \n",
    "    # Validate first dataset\n",
    "    if len(final_X_matrices) > 0:\n",
    "        X_first = final_X_matrices[0]\n",
    "        Y_first = final_Y_matrices[0]\n",
    "        \n",
    "        print(f\"\\nValidation of first dataset (M1):\")\n",
    "        print(f\"  X matrix shape: {X_first.shape}\")\n",
    "        print(f\"  Y matrix shape: {Y_first.shape}\")\n",
    "        \n",
    "        print(f\"\\n  X matrix column names ({len(X_first.columns)} features):\")\n",
    "        print(f\"  {'-'*60}\")\n",
    "        for i, col in enumerate(X_first.columns, 1):\n",
    "            print(f\"    {i:2d}. {col}\")\n",
    "        \n",
    "        print(f\"\\n  Y matrix:\")\n",
    "        print(f\"    Target variable: {Y_first.columns[0]}\")\n",
    "        print(f\"    Missing values: {Y_first.isna().sum().sum()}\")\n",
    "    \n",
    "    # Store in global variables\n",
    "    globals()['final_X_matrices'] = final_X_matrices\n",
    "    globals()['final_Y_matrices'] = final_Y_matrices\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[OK] Module 3/4 & 4/4 completed!\")\n",
    "    print(f\"  Processed {M} MICE datasets\")\n",
    "    print(f\"  Final X and Y matrices available as 'final_X_matrices' and 'final_Y_matrices'\")\n",
    "    print(f\"  Saved files: LISS_Final_X_M1.csv to LISS_Final_X_M{M}.csv\")\n",
    "    print(f\"               LISS_Final_Y_M1.csv to LISS_Final_Y_M{M}.csv\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[ERROR] Error: 'imputed_dataframes' not found. Please run Cell 7 (MICE imputation) first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Found OpenMP library: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libomp.dylib\n",
      "[OK] Fixed XGBoost library dependency\n",
      "[OK] Fixed LightGBM library dependency\n",
      "[OK] XGBoost imported successfully\n",
      "[OK] LightGBM imported successfully\n",
      "============================================================\n",
      "Module 5/5: Model Training & Randomized Search Benchmarking\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task 1: Loading MICE Datasets\n",
      "============================================================\n",
      "[OK] M1: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "[OK] M2: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "[OK] M3: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "[OK] M4: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "[OK] M5: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "\n",
      "[OK] Successfully loaded 5 MICE datasets\n",
      "  Total datasets: 5\n",
      "  All datasets ready for modeling\n",
      "\n",
      "============================================================\n",
      "Task 2: Define Models and Randomized Search Parameter Distributions\n",
      "============================================================\n",
      "\n",
      "Defined 5 models with parameter distributions:\n",
      "  - Elastic-Net: 2 hyperparameters (using distributions)\n",
      "  - SVR: 3 hyperparameters (using distributions)\n",
      "  - Random Forest: 3 hyperparameters (using distributions)\n",
      "  - XGBoost: 4 hyperparameters (using distributions)\n",
      "  - LightGBM: 4 hyperparameters (using distributions)\n",
      "\n",
      "============================================================\n",
      "Task 3: MICE Pooling with Nested Cross-Validation\n",
      "============================================================\n",
      "\n",
      "Cross-Validation Setup:\n",
      "  MICE Datasets: 5\n",
      "  Outer CV: 5-fold (for final evaluation)\n",
      "  Inner CV: 3-fold RandomizedSearchCV (n_iter=50 for hyperparameter tuning)\n",
      "  Total test folds: 5 datasets × 5 folds = 25 test results\n",
      "\n",
      "============================================================\n",
      "Training and Evaluating Models Across MICE Datasets...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MICE Dataset 1/5\n",
      "============================================================\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "\n",
      "  Elastic-Net:\n",
      "    M1 Fold 1/5: RMSE=0.4428\n",
      "\n",
      "  SVR:\n",
      "    M1 Fold 1/5: RMSE=0.4410\n",
      "\n",
      "  Random Forest:\n",
      "    M1 Fold 1/5: RMSE=0.4347\n",
      "\n",
      "  XGBoost:\n",
      "    M1 Fold 1/5: RMSE=0.4431\n",
      "\n",
      "  LightGBM:\n",
      "    M1 Fold 1/5: RMSE=0.4270\n",
      "    M1 Fold 2/5: RMSE=0.4975\n",
      "    M1 Fold 2/5: RMSE=0.4956\n",
      "    M1 Fold 2/5: RMSE=0.5090\n",
      "    M1 Fold 2/5: RMSE=0.4925\n",
      "    M1 Fold 2/5: RMSE=0.4877\n",
      "    M1 Fold 3/5: RMSE=0.5275\n",
      "    M1 Fold 3/5: RMSE=0.5290\n",
      "    M1 Fold 3/5: RMSE=0.5174\n",
      "    M1 Fold 3/5: RMSE=0.5196\n",
      "    M1 Fold 3/5: RMSE=0.5086\n",
      "    M1 Fold 5/5: RMSE=0.5454\n",
      "    M1 Fold 5/5: RMSE=0.5468\n",
      "    M1 Fold 5/5: RMSE=0.5341\n",
      "    M1 Fold 5/5: RMSE=0.5277\n",
      "    M1 Fold 5/5: RMSE=0.5201\n",
      "\n",
      "============================================================\n",
      "MICE Dataset 2/5\n",
      "============================================================\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "    M2 Fold 1/5: RMSE=0.4428\n",
      "    M2 Fold 1/5: RMSE=0.4410\n",
      "    M2 Fold 1/5: RMSE=0.4347\n",
      "    M2 Fold 1/5: RMSE=0.4431\n",
      "    M2 Fold 1/5: RMSE=0.4270\n",
      "    M2 Fold 2/5: RMSE=0.4975\n",
      "    M2 Fold 2/5: RMSE=0.4956\n",
      "    M2 Fold 2/5: RMSE=0.5090\n",
      "    M2 Fold 2/5: RMSE=0.4925\n",
      "    M2 Fold 2/5: RMSE=0.4877\n",
      "    M2 Fold 3/5: RMSE=0.5275\n",
      "    M2 Fold 3/5: RMSE=0.5290\n",
      "    M2 Fold 3/5: RMSE=0.5174\n",
      "    M2 Fold 3/5: RMSE=0.5196\n",
      "    M2 Fold 3/5: RMSE=0.5086\n",
      "    M2 Fold 5/5: RMSE=0.5454\n",
      "    M2 Fold 5/5: RMSE=0.5468\n",
      "    M2 Fold 5/5: RMSE=0.5341\n",
      "    M2 Fold 5/5: RMSE=0.5277\n",
      "    M2 Fold 5/5: RMSE=0.5201\n",
      "\n",
      "============================================================\n",
      "MICE Dataset 3/5\n",
      "============================================================\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "    M3 Fold 1/5: RMSE=0.4428\n",
      "    M3 Fold 1/5: RMSE=0.4410\n",
      "    M3 Fold 1/5: RMSE=0.4347\n",
      "    M3 Fold 1/5: RMSE=0.4431\n",
      "    M3 Fold 1/5: RMSE=0.4270\n",
      "    M3 Fold 2/5: RMSE=0.4975\n",
      "    M3 Fold 2/5: RMSE=0.4956\n",
      "    M3 Fold 2/5: RMSE=0.5090\n",
      "    M3 Fold 2/5: RMSE=0.4925\n",
      "    M3 Fold 2/5: RMSE=0.4877\n",
      "    M3 Fold 3/5: RMSE=0.5275\n",
      "    M3 Fold 3/5: RMSE=0.5290\n",
      "    M3 Fold 3/5: RMSE=0.5174\n",
      "    M3 Fold 3/5: RMSE=0.5196\n",
      "    M3 Fold 3/5: RMSE=0.5086\n",
      "    M3 Fold 5/5: RMSE=0.5454\n",
      "    M3 Fold 5/5: RMSE=0.5468\n",
      "    M3 Fold 5/5: RMSE=0.5341\n",
      "    M3 Fold 5/5: RMSE=0.5277\n",
      "    M3 Fold 5/5: RMSE=0.5201\n",
      "\n",
      "============================================================\n",
      "MICE Dataset 4/5\n",
      "============================================================\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "    M4 Fold 1/5: RMSE=0.4428\n",
      "    M4 Fold 1/5: RMSE=0.4410\n",
      "    M4 Fold 1/5: RMSE=0.4347\n",
      "    M4 Fold 1/5: RMSE=0.4431\n",
      "    M4 Fold 1/5: RMSE=0.4270\n",
      "    M4 Fold 2/5: RMSE=0.4975\n",
      "    M4 Fold 2/5: RMSE=0.4956\n",
      "    M4 Fold 2/5: RMSE=0.5090\n",
      "    M4 Fold 2/5: RMSE=0.4925\n",
      "    M4 Fold 2/5: RMSE=0.4877\n",
      "    M4 Fold 3/5: RMSE=0.5275\n",
      "    M4 Fold 3/5: RMSE=0.5290\n",
      "    M4 Fold 3/5: RMSE=0.5174\n",
      "    M4 Fold 3/5: RMSE=0.5196\n",
      "    M4 Fold 3/5: RMSE=0.5086\n",
      "    M4 Fold 5/5: RMSE=0.5454\n",
      "    M4 Fold 5/5: RMSE=0.5468\n",
      "    M4 Fold 5/5: RMSE=0.5341\n",
      "    M4 Fold 5/5: RMSE=0.5277\n",
      "    M4 Fold 5/5: RMSE=0.5201\n",
      "\n",
      "============================================================\n",
      "MICE Dataset 5/5\n",
      "============================================================\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "    M5 Fold 1/5: RMSE=0.4428\n",
      "    M5 Fold 1/5: RMSE=0.4410\n",
      "    M5 Fold 1/5: RMSE=0.4347\n",
      "    M5 Fold 1/5: RMSE=0.4431\n",
      "    M5 Fold 1/5: RMSE=0.4270\n",
      "    M5 Fold 2/5: RMSE=0.4975\n",
      "    M5 Fold 2/5: RMSE=0.4956\n",
      "    M5 Fold 2/5: RMSE=0.5090\n",
      "    M5 Fold 2/5: RMSE=0.4925\n",
      "    M5 Fold 2/5: RMSE=0.4877\n",
      "    M5 Fold 3/5: RMSE=0.5275\n",
      "    M5 Fold 3/5: RMSE=0.5290\n",
      "    M5 Fold 3/5: RMSE=0.5174\n",
      "    M5 Fold 3/5: RMSE=0.5196\n",
      "    M5 Fold 3/5: RMSE=0.5086\n",
      "    M5 Fold 5/5: RMSE=0.5454\n",
      "    M5 Fold 5/5: RMSE=0.5468\n",
      "    M5 Fold 5/5: RMSE=0.5341\n",
      "    M5 Fold 5/5: RMSE=0.5277\n",
      "    M5 Fold 5/5: RMSE=0.5201\n",
      "\n",
      "============================================================\n",
      "Task 4: MICE Pooling (Result Integration)\n",
      "============================================================\n",
      "\n",
      "  Elastic-Net:\n",
      "    Total test folds: 25\n",
      "    Pooled Mean RMSE: 0.5085 (±0.0363)\n",
      "\n",
      "  SVR:\n",
      "    Total test folds: 25\n",
      "    Pooled Mean RMSE: 0.5082 (±0.0374)\n",
      "\n",
      "  Random Forest:\n",
      "    Total test folds: 25\n",
      "    Pooled Mean RMSE: 0.5030 (±0.0351)\n",
      "\n",
      "  XGBoost:\n",
      "    Total test folds: 25\n",
      "    Pooled Mean RMSE: 0.5017 (±0.0319)\n",
      "\n",
      "  LightGBM:\n",
      "    Total test folds: 25\n",
      "    Pooled Mean RMSE: 0.4905 (±0.0334)\n",
      "\n",
      "============================================================\n",
      "Task 5: Final Results Summary (After MICE Pooling)\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "Pooled Model Performance: Nested Cross-Validation Across 5 MICE Datasets\n",
      "================================================================================\n",
      "Total test folds: 5 datasets × 5 folds = 25 results per model\n",
      "\n",
      "Model                Mean RMSE       RMSE Std       \n",
      "--------------------------------------------------\n",
      "LightGBM                    0.4905         0.0334\n",
      "XGBoost                     0.5017         0.0319\n",
      "Random Forest               0.5030         0.0351\n",
      "SVR                         0.5082         0.0374\n",
      "Elastic-Net                 0.5085         0.0363\n",
      "\n",
      "================================================================================\n",
      "Best Model Selection\n",
      "================================================================================\n",
      "\n",
      "Best Model by RMSE (lowest):\n",
      "  Model: LightGBM\n",
      "  Mean RMSE: 0.4905 (±0.0334)\n",
      "\n",
      "================================================================================\n",
      "[BEST] BEST OVERALL MODEL (by RMSE): LightGBM\n",
      "================================================================================\n",
      "  Mean RMSE: 0.4905 (±0.0334)\n",
      "\n",
      "================================================================================\n",
      "Model Rankings (by RMSE)\n",
      "================================================================================\n",
      "\n",
      "[OK] Results saved: Model_Benchmarking_RMSE.csv\n",
      "\n",
      "============================================================\n",
      "[OK] Module 5/5 completed!\n",
      "Pooled results are stored in 'results' dictionary\n",
      "  Processed 5 MICE datasets\n",
      "  Total test folds per model: 25\n",
      "  Results saved to: Model_Benchmarking_RMSE.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 10:  Module 5/5 - Model Training & Randomized Search Benchmarking (Pooling)\n",
    "# Module 5/5: Model Training & Randomized Search Benchmarking with MICE Pooling\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# Fix OpenMP library dependencies for XGBoost and LightGBM (macOS)\n",
    "# Find libomp.dylib in common locations\n",
    "omp_paths = [\n",
    "    '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib',\n",
    "    '/opt/homebrew/opt/libomp/lib',\n",
    "    '/usr/local/opt/libomp/lib',\n",
    "]\n",
    "\n",
    "omp_lib_path = None\n",
    "for path in omp_paths:\n",
    "    omp_lib = os.path.join(path, 'libomp.dylib')\n",
    "    if os.path.exists(omp_lib):\n",
    "        omp_lib_path = omp_lib\n",
    "        break\n",
    "\n",
    "# Also try glob pattern for R framework\n",
    "if omp_lib_path is None:\n",
    "    r_paths = glob.glob('/Library/Frameworks/R.framework/Versions/*/Resources/lib/libomp.dylib')\n",
    "    if r_paths:\n",
    "        omp_lib_path = r_paths[0]\n",
    "\n",
    "if omp_lib_path and os.path.exists(omp_lib_path):\n",
    "    print(f\"[OK] Found OpenMP library: {omp_lib_path}\")\n",
    "    \n",
    "    # Find XGBoost and LightGBM installation paths using sys.path\n",
    "    import site\n",
    "    site_packages = []\n",
    "    \n",
    "    # Get site-packages from sys.path\n",
    "    for path in sys.path:\n",
    "        if 'site-packages' in path:\n",
    "            site_packages.append(path)\n",
    "    \n",
    "    # Also try getsitepackages\n",
    "    try:\n",
    "        site_packages.extend(site.getsitepackages())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Remove duplicates and check each path\n",
    "    site_packages = list(set(site_packages))\n",
    "    \n",
    "    # Fix XGBoost library dependency\n",
    "    xgb_fixed = False\n",
    "    for sp in site_packages:\n",
    "        xgb_lib_dir = os.path.join(sp, 'xgboost', 'lib')\n",
    "        xgb_dylib = os.path.join(xgb_lib_dir, 'libxgboost.dylib')\n",
    "        omp_in_xgb = os.path.join(xgb_lib_dir, 'libomp.dylib')\n",
    "        \n",
    "        if os.path.exists(xgb_dylib):\n",
    "            # Copy libomp to XGBoost lib directory if not exists\n",
    "            if not os.path.exists(omp_in_xgb):\n",
    "                shutil.copy2(omp_lib_path, omp_in_xgb)\n",
    "                print(f\"[OK] Copied libomp.dylib to XGBoost lib directory\")\n",
    "            \n",
    "            # Update library dependency to use @loader_path\n",
    "            try:\n",
    "                result = subprocess.run(['install_name_tool', '-change', \n",
    "                                       '@rpath/libomp.dylib', \n",
    "                                       '@loader_path/libomp.dylib', \n",
    "                                       xgb_dylib], \n",
    "                                      capture_output=True, text=True, check=False)\n",
    "                if result.returncode == 0:\n",
    "                    print(\"[OK] Fixed XGBoost library dependency\")\n",
    "                    xgb_fixed = True\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            break\n",
    "    \n",
    "    # Fix LightGBM library dependency\n",
    "    lgb_fixed = False\n",
    "    for sp in site_packages:\n",
    "        lgb_lib_dir = os.path.join(sp, 'lightgbm', 'lib')\n",
    "        lgb_dylib = os.path.join(lgb_lib_dir, 'lib_lightgbm.dylib')\n",
    "        omp_in_lgb = os.path.join(lgb_lib_dir, 'libomp.dylib')\n",
    "        \n",
    "        if os.path.exists(lgb_dylib):\n",
    "            # Copy libomp to LightGBM lib directory if not exists\n",
    "            if not os.path.exists(omp_in_lgb):\n",
    "                shutil.copy2(omp_lib_path, omp_in_lgb)\n",
    "                print(f\"[OK] Copied libomp.dylib to LightGBM lib directory\")\n",
    "            \n",
    "            # Update library dependency\n",
    "            try:\n",
    "                result = subprocess.run(['install_name_tool', '-change', \n",
    "                                       '@rpath/libomp.dylib', \n",
    "                                       '@loader_path/libomp.dylib', \n",
    "                                       lgb_dylib], \n",
    "                                      capture_output=True, text=True, check=False)\n",
    "                if result.returncode == 0:\n",
    "                    print(\"[OK] Fixed LightGBM library dependency\")\n",
    "                    lgb_fixed = True\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            break\n",
    "else:\n",
    "    print(\"[WARNING] Warning: Could not find libomp.dylib. XGBoost and LightGBM may not work.\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, loguniform, randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"[OK] XGBoost imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Warning: XGBoost not available: {e}\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    xgb = None\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "    print(\"[OK] LightGBM imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Warning: LightGBM not available: {e}\")\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    lgb = None\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Module 5/5: Model Training & Randomized Search Benchmarking\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# Task 1: Load MICE Datasets\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Task 1: Loading MICE Datasets\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "M = 5\n",
    "mice_X_datasets = []\n",
    "mice_Y_datasets = []\n",
    "\n",
    "for m in range(1, M + 1):\n",
    "    X_file = f'LISS_Final_X_M{m}.csv'\n",
    "    Y_file = f'LISS_Final_Y_M{m}.csv'\n",
    "    \n",
    "    try:\n",
    "        X_m = pd.read_csv(X_file, encoding='utf-8', index_col=0)\n",
    "        Y_m = pd.read_csv(Y_file, encoding='utf-8', index_col=0)\n",
    "        \n",
    "        # Align indices\n",
    "        common_idx = X_m.index.intersection(Y_m.index)\n",
    "        X_m = X_m.loc[common_idx]\n",
    "        Y_m = Y_m.loc[common_idx]\n",
    "        \n",
    "        # Check for missing values\n",
    "        X_missing = X_m.isna().sum().sum()\n",
    "        Y_missing = Y_m.isna().sum().sum()\n",
    "        \n",
    "        if X_missing > 0 or Y_missing > 0:\n",
    "            valid_mask = ~(X_m.isna().any(axis=1) | Y_m.isna().any(axis=1))\n",
    "            X_m = X_m[valid_mask]\n",
    "            Y_m = Y_m[valid_mask]\n",
    "        \n",
    "        # Extract target variable\n",
    "        target_col = Y_m.columns[0]\n",
    "        y_m = Y_m[target_col].values\n",
    "        X_m_values = X_m.values\n",
    "        \n",
    "        mice_X_datasets.append(X_m_values)\n",
    "        mice_Y_datasets.append(y_m)\n",
    "        \n",
    "        print(f\"[OK] M{m}: X shape {X_m_values.shape}, y shape {y_m.shape}, target: {target_col}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"[ERROR] M{m}: Could not find {X_file} or {Y_file}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] M{m}: Error loading data: {e}\")\n",
    "        break\n",
    "\n",
    "if len(mice_X_datasets) == M:\n",
    "    print(f\"\\n[OK] Successfully loaded {M} MICE datasets\")\n",
    "    print(f\"  Total datasets: {len(mice_X_datasets)}\")\n",
    "    print(f\"  All datasets ready for modeling\")\n",
    "else:\n",
    "    print(f\"\\n[ERROR] Only loaded {len(mice_X_datasets)}/{M} datasets. Cannot proceed.\")\n",
    "    mice_X_datasets = []\n",
    "    mice_Y_datasets = []\n",
    "\n",
    "# ============================================================================\n",
    "# Task 2: Define Model List and Parameter Spaces\n",
    "# ============================================================================\n",
    "if len(mice_X_datasets) == M and len(mice_Y_datasets) == M:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Define Models and Randomized Search Parameter Distributions\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define models and their parameter distributions for RandomizedSearchCV\n",
    "    models_config = {\n",
    "        'Elastic-Net': {\n",
    "            'model': ElasticNet(random_state=42, max_iter=1000),\n",
    "            'param_distributions': {\n",
    "                'alpha': loguniform(1e-3, 1e1),  # 0.001 to 10\n",
    "                'l1_ratio': uniform(0.1, 0.8)  # 0.1 to 0.9\n",
    "            }\n",
    "        },\n",
    "        'SVR': {\n",
    "            'model': SVR(),\n",
    "            'param_distributions': {\n",
    "                'C': loguniform(1e-2, 1e2),  # 0.01 to 100\n",
    "                'gamma': ['scale', 'auto', 1e-4, 1e-3, 1e-2, 1e-1],  # scale, auto, or numeric values\n",
    "                'epsilon': uniform(0.01, 0.49)  # 0.01 to 0.5\n",
    "            }\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestRegressor(random_state=42, n_jobs=1),\n",
    "            'param_distributions': {\n",
    "                'n_estimators': randint(50, 201),  # 50 to 200\n",
    "                'max_depth': [5, 10, 15, 20, None],\n",
    "                'min_samples_split': randint(2, 11)  # 2 to 10\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add XGBoost\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        models_config['XGBoost'] = {\n",
    "            'model': xgb.XGBRegressor(random_state=42, n_jobs=1, verbosity=0),\n",
    "            'param_distributions': {\n",
    "                'n_estimators': randint(50, 201),  # 50 to 200\n",
    "                'max_depth': randint(3, 8),  # 3 to 7\n",
    "                'learning_rate': loguniform(1e-3, 1e0),  # 0.001 to 1.0\n",
    "                'subsample': uniform(0.6, 0.4)  # 0.6 to 1.0\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        raise ImportError(\"XGBoost is required but could not be imported. Please check OpenMP installation.\")\n",
    "    \n",
    "    # Add LightGBM\n",
    "    if LIGHTGBM_AVAILABLE:\n",
    "        models_config['LightGBM'] = {\n",
    "            'model': lgb.LGBMRegressor(random_state=42, n_jobs=1, verbosity=-1),\n",
    "            'param_distributions': {\n",
    "                'n_estimators': randint(50, 201),  # 50 to 200\n",
    "                'max_depth': randint(3, 8),  # 3 to 7\n",
    "                'learning_rate': loguniform(1e-3, 1e0),  # 0.001 to 1.0\n",
    "                'num_leaves': randint(31, 101)  # 31 to 100\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        raise ImportError(\"LightGBM is required but could not be imported. Please check OpenMP installation.\")\n",
    "    \n",
    "    print(f\"\\nDefined {len(models_config)} models with parameter distributions:\")\n",
    "    for model_name, config in models_config.items():\n",
    "        n_params = len(config['param_distributions'])\n",
    "        print(f\"  - {model_name}: {n_params} hyperparameters (using distributions)\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: MICE Pooling with Nested Cross-Validation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: MICE Pooling with Nested Cross-Validation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Outer CV: 5-fold for final performance evaluation\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Inner CV: 3-fold for hyperparameter tuning\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(f\"\\nCross-Validation Setup:\")\n",
    "    print(f\"  MICE Datasets: {M}\")\n",
    "    print(f\"  Outer CV: {outer_cv.n_splits}-fold (for final evaluation)\")\n",
    "    print(f\"  Inner CV: {inner_cv.n_splits}-fold RandomizedSearchCV (n_iter=50 for hyperparameter tuning)\")\n",
    "    print(f\"  Total test folds: {M} datasets × {outer_cv.n_splits} folds = {M * outer_cv.n_splits} test results\")\n",
    "    \n",
    "    # Store results for pooling (all RMSE and R² scores across all MICE datasets)\n",
    "    pooled_results = {}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training and Evaluating Models Across MICE Datasets...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Main loop: iterate over MICE datasets\n",
    "    for m_idx in range(M):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MICE Dataset {m_idx+1}/{M}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        X = mice_X_datasets[m_idx]\n",
    "        y = mice_Y_datasets[m_idx]\n",
    "        \n",
    "        print(f\"  Dataset shape: X {X.shape}, y {y.shape}\")\n",
    "        \n",
    "                # Outer CV loop\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(X), 1):\n",
    "            X_train_outer, X_test_outer = X[train_idx], X[test_idx]\n",
    "            y_train_outer, y_test_outer = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Process each model\n",
    "            for model_name, config in models_config.items():\n",
    "                # Initialize storage for this model if not already present\n",
    "                if model_name not in pooled_results:\n",
    "                    pooled_results[model_name] = {\n",
    "                        'rmse_scores': []\n",
    "                    }\n",
    "\n",
    "                model = config['model']\n",
    "                param_distributions = config['param_distributions']\n",
    "\n",
    "                # Inner Randomized Search for hyperparameter tuning\n",
    "                random_search = RandomizedSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_distributions=param_distributions,\n",
    "                    n_iter=50,  # Number of parameter settings sampled\n",
    "                    cv=inner_cv,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_jobs=1,  # Use single job to avoid memory issues\n",
    "                    random_state=42,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    # Fit on outer training set\n",
    "                    random_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "                    # Get best model\n",
    "                    best_model = random_search.best_estimator_\n",
    "\n",
    "                    # Predict on outer test set\n",
    "                    y_pred = best_model.predict(X_test_outer)\n",
    "\n",
    "                    # Calculate RMSE on test set\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test_outer, y_pred))\n",
    "\n",
    "                    # Collect results for pooling (across all MICE datasets)\n",
    "                    pooled_results[model_name]['rmse_scores'].append(rmse)\n",
    "\n",
    "                    # Verbose logging for first few folds\n",
    "                    if fold_idx == 1 and m_idx == 0:\n",
    "                        print(f\"\\n  {model_name}:\")\n",
    "                    if fold_idx == 1:\n",
    "                        print(f\"    M{m_idx+1} Fold {fold_idx}/{outer_cv.n_splits}: RMSE={rmse:.4f}\")\n",
    "                    elif fold_idx <= 3:\n",
    "                        print(f\"    M{m_idx+1} Fold {fold_idx}/{outer_cv.n_splits}: RMSE={rmse:.4f}\")\n",
    "                    elif fold_idx == outer_cv.n_splits:\n",
    "                        print(f\"    M{m_idx+1} Fold {fold_idx}/{outer_cv.n_splits}: RMSE={rmse:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Do not stop the whole process; log the error and continue\n",
    "                    print(f\"[ERROR] Error training {model_name} on M{m_idx+1} Fold {fold_idx}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 4: MICE Pooling (Result Integration)\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 4: MICE Pooling (Result Integration)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate pooled statistics for each model\n",
    "    results = {}\n",
    "    for model_name in pooled_results.keys():\n",
    "        rmse_scores = pooled_results[model_name]['rmse_scores']\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'mean_rmse': np.mean(rmse_scores),\n",
    "            'std_rmse': np.std(rmse_scores),\n",
    "            'rmse_scores': rmse_scores,\n",
    "            'n_folds': len(rmse_scores)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n  {model_name}:\")\n",
    "        print(f\"    Total test folds: {len(rmse_scores)}\")\n",
    "        print(f\"    Pooled Mean RMSE: {results[model_name]['mean_rmse']:.4f} (±{results[model_name]['std_rmse']:.4f})\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 5: Final Output\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 5: Final Results Summary (After MICE Pooling)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create results table (sorted by RMSE, ascending)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Pooled Model Performance: Nested Cross-Validation Across 5 MICE Datasets\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total test folds: {M} datasets × {outer_cv.n_splits} folds = {M * outer_cv.n_splits} results per model\")\n",
    "    \n",
    "    # Sort models by RMSE (ascending - lower is better)\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['mean_rmse'])\n",
    "    \n",
    "    print(f\"\\n{'Model':<20} {'Mean RMSE':<15} {'RMSE Std':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, r in sorted_results:\n",
    "        print(f\"{model_name:<20} {r['mean_rmse']:>13.4f} {r['std_rmse']:>14.4f}\")\n",
    "    \n",
    "    # Determine best model\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Best Model Selection\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find best model by lowest RMSE\n",
    "    best_by_rmse = min(results.items(), key=lambda x: x[1]['mean_rmse'])\n",
    "    \n",
    "    # Find best model by highest R²\n",
    "    \n",
    "    print(f\"\\nBest Model by RMSE (lowest):\")\n",
    "    print(f\"  Model: {best_by_rmse[0]}\")\n",
    "    print(f\"  Mean RMSE: {best_by_rmse[1]['mean_rmse']:.4f} (±{best_by_rmse[1]['std_rmse']:.4f})\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # Overall best model is the same as best by RMSE (since RMSE is the only metric)\n",
    "    best_overall = best_by_rmse[0]\n",
    "    best_result = best_by_rmse[1]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[BEST] BEST OVERALL MODEL (by RMSE): {best_overall}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Mean RMSE: {best_result['mean_rmse']:.4f} (±{best_result['std_rmse']:.4f})\")\n",
    "    \n",
    "    # Detailed ranking table (by RMSE only)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Model Rankings (by RMSE)\")\n",
    "    print(\"=\"*80)\n",
    "    # Create results DataFrame for saving (sorted by RMSE, ascending)\n",
    "    results_table_data = []\n",
    "    for rank, (model_name, r) in enumerate(sorted_results, start=1):\n",
    "        results_table_data.append({\n",
    "            'Model': model_name,\n",
    "            'Mean RMSE': r['mean_rmse'],\n",
    "            'RMSE Std': r['std_rmse'],\n",
    "            'Rank': rank\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results_table_data)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_filename = 'Model_Benchmarking_RMSE.csv'\n",
    "    results_df.to_csv(results_filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\n[OK] Results saved: {results_filename}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[OK] Module 5/5 completed!\")\n",
    "    print(\"Pooled results are stored in 'results' dictionary\")\n",
    "    print(f\"  Processed {M} MICE datasets\")\n",
    "    print(f\"  Total test folds per model: {M * outer_cv.n_splits}\")\n",
    "    print(f\"  Results saved to: {results_filename}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[ERROR] Cannot proceed: MICE datasets not loaded successfully.\")\n",
    "    print(f\"  Expected {M} datasets, but loaded {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Cell 11: RQ1 Incremental Contribution Analysis (LightGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task 1: Environment Verification & Data Loading\n",
      "============================================================\n",
      "[INFO] Case A: Using existing mice_X_datasets and mice_Y_datasets from environment\n",
      "  Found 5 X datasets and 5 Y datasets\n",
      "\n",
      "[OK] Successfully loaded 5 MICE datasets\n",
      "  Total datasets: 5\n",
      "  All datasets ready for visualization\n",
      "\n",
      "============================================================\n",
      "Task 2: Define Random Forest Model\n",
      "============================================================\n",
      "\n",
      "LightGBM Configuration:\n",
      "  n_estimators: 100\n",
      "  max_depth: 15\n",
      "  learning_rate: 0.1\n",
      "  num_leaves: 31\n",
      "  min_child_samples: 20\n",
      "  random_state: 42\n",
      "  n_jobs: 1\n",
      "  verbosity: -1\n",
      "\n",
      "Evaluation Metric:\n",
      "  RMSE (Root Mean Squared Error) - to measure prediction error\n",
      "\n",
      "============================================================\n",
      "Task 3: Define 4 Incremental Feature Blocks\n",
      "============================================================\n",
      "\n",
      "Feature Blocks Definition:\n",
      "================================================================================\n",
      "\n",
      "Block 1: Baseline Controls\n",
      "  Description: Age, Children Count, Self-rated Health, Gender OHE, Education OHE, Income OHE\n",
      "  Features: 11/11 available\n",
      "\n",
      "Block 2: + Work Characteristics\n",
      "  Description: Block 1 + Δ Autonomy, Δ Workload, Δ Skills\n",
      "  Features: 14/14 available\n",
      "\n",
      "Block 3: + WLB & Health Factors\n",
      "  Description: Block 2 + Δ RemoteHours, Δ WFC Proxy, Δ Health Hindrance\n",
      "  Features: 17/17 available\n",
      "\n",
      "Block 4: + Personality Traits\n",
      "  Description: Block 3 + N_Score, E_Score, C_Score, A_Score, O_Score (Full Model)\n",
      "  Features: 22/22 available\n",
      "\n",
      "============================================================\n",
      "Task 4: Execute Incremental Modeling\n",
      "============================================================\n",
      "\n",
      "Cross-Validation Setup:\n",
      "  CV: 5-fold KFold\n",
      "  MICE Datasets: 5\n",
      "  Feature Blocks: 4\n",
      "  Total evaluations: 5 datasets × 4 blocks × 5 folds\n",
      "\n",
      "============================================================\n",
      "Training Models Across Feature Blocks and MICE Datasets...\n",
      "============================================================\n",
      "\n",
      "Processing MICE Dataset 1/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "    Block 1: 11 features\n",
      "      Mean RMSE: 0.5206 (±0.0373)\n",
      "    Block 2: 14 features\n",
      "      Mean RMSE: 0.4929 (±0.0291)\n",
      "    Block 3: 17 features\n",
      "      Mean RMSE: 0.4976 (±0.0291)\n",
      "    Block 4: 22 features\n",
      "      Mean RMSE: 0.4893 (±0.0324)\n",
      "\n",
      "Processing MICE Dataset 2/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "    Block 1: 11 features\n",
      "      Mean RMSE: 0.5206 (±0.0373)\n",
      "    Block 2: 14 features\n",
      "      Mean RMSE: 0.4929 (±0.0291)\n",
      "    Block 3: 17 features\n",
      "      Mean RMSE: 0.4976 (±0.0291)\n",
      "    Block 4: 22 features\n",
      "      Mean RMSE: 0.4893 (±0.0324)\n",
      "\n",
      "Processing MICE Dataset 3/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "    Block 1: 11 features\n",
      "      Mean RMSE: 0.5206 (±0.0373)\n",
      "    Block 2: 14 features\n",
      "      Mean RMSE: 0.4929 (±0.0291)\n",
      "    Block 3: 17 features\n",
      "      Mean RMSE: 0.4976 (±0.0291)\n",
      "    Block 4: 22 features\n",
      "      Mean RMSE: 0.4893 (±0.0324)\n",
      "\n",
      "Processing MICE Dataset 4/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "    Block 1: 11 features\n",
      "      Mean RMSE: 0.5206 (±0.0373)\n",
      "    Block 2: 14 features\n",
      "      Mean RMSE: 0.4929 (±0.0291)\n",
      "    Block 3: 17 features\n",
      "      Mean RMSE: 0.4976 (±0.0291)\n",
      "    Block 4: 22 features\n",
      "      Mean RMSE: 0.4893 (±0.0324)\n",
      "\n",
      "Processing MICE Dataset 5/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "    Block 1: 11 features\n",
      "      Mean RMSE: 0.5206 (±0.0373)\n",
      "    Block 2: 14 features\n",
      "      Mean RMSE: 0.4929 (±0.0291)\n",
      "    Block 3: 17 features\n",
      "      Mean RMSE: 0.4976 (±0.0291)\n",
      "    Block 4: 22 features\n",
      "      Mean RMSE: 0.4893 (±0.0324)\n",
      "\n",
      "[OK] Incremental modeling completed for all 5 MICE datasets\n",
      "\n",
      "============================================================\n",
      "Task 5: Results Pooling and Output\n",
      "============================================================\n",
      "\n",
      "====================================================================================================\n",
      "RQ1: Incremental Contribution Analysis Results (RMSE)\n",
      "====================================================================================================\n",
      "\n",
      "Model Block  Included Features                   Cumulative RMSE (Pooled Mean)  Error Reduction (ΔRMSE)  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Block 1      Baseline Controls                                          0.5206                  +0.0000\n",
      "Block 2      + Work Characteristics                                     0.4929                  +0.0277\n",
      "Block 3      + WLB & Health Factors                                     0.4976                  -0.0046\n",
      "Block 4      + Personality Traits                                       0.4893                  +0.0082\n",
      "\n",
      "====================================================================================================\n",
      "Detailed Statistics:\n",
      "====================================================================================================\n",
      "\n",
      "Model Block  Included Features                   Cumulative RMSE      RMSE Std        Error Reduction (ΔRMSE)   N_Evaluations  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Block 1      Baseline Controls                                0.5206        ±0.0373                  +0.0000             25\n",
      "Block 2      + Work Characteristics                           0.4929        ±0.0291                  +0.0277             25\n",
      "Block 3      + WLB & Health Factors                           0.4976        ±0.0291                  -0.0046             25\n",
      "Block 4      + Personality Traits                             0.4893        ±0.0324                  +0.0082             25\n",
      "\n",
      "====================================================================================================\n",
      "Error Reduction Analysis:\n",
      "====================================================================================================\n",
      "\n",
      "Baseline RMSE (Block 1): 0.5206\n",
      "\n",
      "Error Reduction Breakdown:\n",
      "\n",
      "[OK] Results saved: RQ1_Incremental_RMSE_LightGBM.csv\n",
      "\n",
      "============================================================\n",
      "[OK] RQ1 Incremental Contribution Analysis completed!\n",
      "  Processed 5 MICE datasets\n",
      "  Analyzed 4 feature blocks\n",
      "  Generated incremental contribution analysis table\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 11:  RQ1 Incremental Contribution Analysis (LightGBM)\n",
    "# RQ1: Incremental Contribution Analysis using LightGBM\n",
    "print(\"=\"*60)\n",
    "print(\"Cell 11: RQ1 Incremental Contribution Analysis (LightGBM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt  # Temporarily kept for existing visualization code\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if LightGBM is available\n",
    "try:\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    raise ImportError(\"LightGBM is required but could not be imported.\")\n",
    "\n",
    "# Set matplotlib style (temporarily kept for existing visualization code)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# Task 1: Environment Verification & Data Loading\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Task 1: Environment Verification & Data Loading\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Case A: Check if mice_X_datasets and mice_Y_datasets exist in environment\n",
    "try:\n",
    "    if 'mice_X_datasets' in globals() and 'mice_Y_datasets' in globals() and \\\n",
    "       mice_X_datasets is not None and mice_Y_datasets is not None and \\\n",
    "       len(mice_X_datasets) > 0 and len(mice_Y_datasets) > 0:\n",
    "        print(\"[INFO] Case A: Using existing mice_X_datasets and mice_Y_datasets from environment\")\n",
    "        print(f\"  Found {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n",
    "        M = len(mice_X_datasets)\n",
    "        # Ensure mice_X_dataframes exists\n",
    "        if 'mice_X_dataframes' not in globals() or mice_X_dataframes is None or len(mice_X_dataframes) == 0:\n",
    "            print(\"[WARNING] mice_X_dataframes not found, will reload to preserve feature names\")\n",
    "            raise NameError(\"mice_X_dataframes missing\")\n",
    "except (NameError, AttributeError):\n",
    "    # Case B: Reload from CSV files\n",
    "    print(\"[INFO] Case B: Reloading MICE datasets from CSV files\")\n",
    "    data_loaded = False\n",
    "\n",
    "if not data_loaded:\n",
    "    M = 5\n",
    "    mice_X_datasets = []\n",
    "    mice_Y_datasets = []\n",
    "    mice_X_dataframes = []\n",
    "\n",
    "    for m in range(1, M + 1):\n",
    "        X_file = f'LISS_Final_X_M{m}.csv'\n",
    "        Y_file = f'LISS_Final_Y_M{m}.csv'\n",
    "        \n",
    "        try:\n",
    "            X_m = pd.read_csv(X_file, encoding='utf-8', index_col=0)\n",
    "            Y_m = pd.read_csv(Y_file, encoding='utf-8', index_col=0)\n",
    "            \n",
    "            # Align indices\n",
    "            common_idx = X_m.index.intersection(Y_m.index)\n",
    "            X_m = X_m.loc[common_idx]\n",
    "            Y_m = Y_m.loc[common_idx]\n",
    "            \n",
    "            # Check for missing values\n",
    "            X_missing = X_m.isna().sum().sum()\n",
    "            Y_missing = Y_m.isna().sum().sum()\n",
    "            \n",
    "            if X_missing > 0 or Y_missing > 0:\n",
    "                valid_mask = ~(X_m.isna().any(axis=1) | Y_m.isna().any(axis=1))\n",
    "                X_m = X_m[valid_mask]\n",
    "                Y_m = Y_m[valid_mask]\n",
    "            \n",
    "            # Extract target variable\n",
    "            target_col = Y_m.columns[0]\n",
    "            y_m = Y_m[target_col].values\n",
    "            X_m_values = X_m.values\n",
    "            \n",
    "            mice_X_datasets.append(X_m_values)\n",
    "            mice_Y_datasets.append(y_m)\n",
    "            mice_X_dataframes.append(X_m.copy())  # Keep DataFrame for feature names\n",
    "            \n",
    "            print(f\"[OK] M{m}: X shape {X_m_values.shape}, y shape {y_m.shape}, target: {target_col}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"[ERROR] M{m}: Could not find {X_file} or {Y_file}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] M{m}: Error loading data: {e}\")\n",
    "            break\n",
    "\n",
    "    if len(mice_X_datasets) == M:\n",
    "        print(f\"\\n[OK] Successfully loaded {M} MICE datasets from CSV files\")\n",
    "        print(f\"  Total datasets: {len(mice_X_datasets)}\")\n",
    "        print(f\"  Feature names preserved for block selection\")\n",
    "        mice_X_dataframes = []\n",
    "\n",
    "# Verify data is available\n",
    "if len(mice_X_datasets) == M and len(mice_Y_datasets) == M:\n",
    "    print(f\"\\n[OK] Successfully loaded {M} MICE datasets\")\n",
    "    print(f\"  Total datasets: {len(mice_X_datasets)}\")\n",
    "    print(f\"  All datasets ready for visualization\")\n",
    "\n",
    "# ============================================================================\n",
    "# Task 2: Define Model\n",
    "# ============================================================================\n",
    "if len(mice_X_datasets) == M and len(mice_Y_datasets) == M:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Define Random Forest Model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "        # Define LightGBM Regressor\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "    print(f\"\\nLightGBM Configuration:\")\n",
    "    print(f\"  n_estimators: 100\")\n",
    "    print(f\"  max_depth: 15\")\n",
    "    print(f\"  learning_rate: 0.1\")\n",
    "    print(f\"  num_leaves: 31\")\n",
    "    print(f\"  min_child_samples: 20\")\n",
    "    print(f\"  random_state: 42\")\n",
    "    print(f\"  n_jobs: 1\")\n",
    "    print(f\"  verbosity: -1\")\n",
    "    \n",
    "    print(f\"\\nEvaluation Metric:\")\n",
    "    print(f\"  RMSE (Root Mean Squared Error) - to measure prediction error\")\n",
    "\n",
    "# ============================================================================\n",
    "# Task 3: Define 4 Incremental Feature Blocks\n",
    "# ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Define 4 Incremental Feature Blocks\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get feature names from first dataset\n",
    "    feature_names = mice_X_dataframes[0].columns.tolist()\n",
    "    \n",
    "    # Define feature blocks\n",
    "    feature_blocks = {}\n",
    "    \n",
    "    # Block 1: Baseline Controls\n",
    "    # Age, Children Count, Self-rated Health, Gender OHE, Education OHE, Income OHE\n",
    "    block1_features = []\n",
    "    \n",
    "    # Continuous baseline controls\n",
    "    baseline_continuous = ['cw23p003', 'cf23p455', 'ch23p004']  # Age, Children Count, Self-rated Health\n",
    "    for feat in baseline_continuous:\n",
    "        if feat in feature_names:\n",
    "            block1_features.append(feat)\n",
    "    \n",
    "    # Gender OHE dummies\n",
    "    gender_dummies = [f for f in feature_names if f.startswith('ch23p001_')]\n",
    "    block1_features.extend(gender_dummies)\n",
    "    \n",
    "    # Education OHE dummies\n",
    "    education_dummies = [f for f in feature_names if f.startswith('Education_Recode_')]\n",
    "    block1_features.extend(education_dummies)\n",
    "    \n",
    "    # Income OHE dummies\n",
    "    income_dummies = [f for f in feature_names if f.startswith('Income_Quartile_')]\n",
    "    block1_features.extend(income_dummies)\n",
    "    \n",
    "    feature_blocks['Block 1'] = {\n",
    "        'name': 'Baseline Controls',\n",
    "        'features': block1_features,\n",
    "        'description': 'Age, Children Count, Self-rated Health, Gender OHE, Education OHE, Income OHE'\n",
    "    }\n",
    "    \n",
    "    # Block 2: Block 1 + Work Characteristics\n",
    "    block2_features = block1_features.copy()\n",
    "    work_char_features = ['Delta_Autonomy', 'Delta_Workload', 'Delta_Skills']\n",
    "    for feat in work_char_features:\n",
    "        if feat in feature_names:\n",
    "            block2_features.append(feat)\n",
    "    \n",
    "    feature_blocks['Block 2'] = {\n",
    "        'name': '+ Work Characteristics',\n",
    "        'features': block2_features,\n",
    "        'description': 'Block 1 + Δ Autonomy, Δ Workload, Δ Skills'\n",
    "    }\n",
    "    \n",
    "    # Block 3: Block 2 + WLB & Health Factors\n",
    "    block3_features = block2_features.copy()\n",
    "    wlb_health_features = ['Delta_RemoteHours', 'Delta_WFC_Proxy', 'Delta_Health_Hindrance']\n",
    "    for feat in wlb_health_features:\n",
    "        if feat in feature_names:\n",
    "            block3_features.append(feat)\n",
    "    \n",
    "    feature_blocks['Block 3'] = {\n",
    "        'name': '+ WLB & Health Factors',\n",
    "        'features': block3_features,\n",
    "        'description': 'Block 2 + Δ RemoteHours, Δ WFC Proxy, Δ Health Hindrance'\n",
    "    }\n",
    "    \n",
    "    # Block 4: Block 3 + Personality Traits (Full Model)\n",
    "    block4_features = block3_features.copy()\n",
    "    personality_features = ['N_Score', 'E_Score', 'C_Score', 'A_Score', 'O_Score']\n",
    "    for feat in personality_features:\n",
    "        if feat in feature_names:\n",
    "            block4_features.append(feat)\n",
    "    \n",
    "    feature_blocks['Block 4'] = {\n",
    "        'name': '+ Personality Traits',\n",
    "        'features': block4_features,\n",
    "        'description': 'Block 3 + N_Score, E_Score, C_Score, A_Score, O_Score (Full Model)'\n",
    "    }\n",
    "    \n",
    "    # Print feature block definitions\n",
    "    print(f\"\\nFeature Blocks Definition:\")\n",
    "    print(\"=\"*80)\n",
    "    for block_name, block_info in feature_blocks.items():\n",
    "        available_features = [f for f in block_info['features'] if f in feature_names]\n",
    "        print(f\"\\n{block_name}: {block_info['name']}\")\n",
    "        print(f\"  Description: {block_info['description']}\")\n",
    "        print(f\"  Features: {len(available_features)}/{len(block_info['features'])} available\")\n",
    "        if len(available_features) <= 10:\n",
    "            print(f\"    {', '.join(available_features)}\")\n",
    "# ============================================================================\n",
    "# Task 4: Execute Incremental Modeling\n",
    "# ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 4: Execute Incremental Modeling\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 5-Fold Cross-Validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Store RMSE scores for each block across all MICE datasets and CV folds\n",
    "    block_rmse = {block_name: [] for block_name in feature_blocks.keys()}\n",
    "    \n",
    "    print(f\"\\nCross-Validation Setup:\")\n",
    "    print(f\"  CV: {cv.n_splits}-fold KFold\")\n",
    "    print(f\"  MICE Datasets: {M}\")\n",
    "    print(f\"  Feature Blocks: {len(feature_blocks)}\")\n",
    "    print(f\"  Total evaluations: {M} datasets × {len(feature_blocks)} blocks × {cv.n_splits} folds\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Models Across Feature Blocks and MICE Datasets...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Main loop: iterate over MICE datasets\n",
    "    for m_idx in range(M):\n",
    "        print(f\"\\nProcessing MICE Dataset {m_idx+1}/{M}...\")\n",
    "        \n",
    "        X_df = mice_X_dataframes[m_idx]\n",
    "        y = mice_Y_datasets[m_idx]\n",
    "        \n",
    "        print(f\"  Dataset shape: X {X_df.shape}, y {y.shape}\")\n",
    "        \n",
    "        # Process each feature block\n",
    "        for block_name, block_info in feature_blocks.items():\n",
    "            # Select features for this block\n",
    "            available_block_features = [f for f in block_info['features'] if f in X_df.columns]\n",
    "            \n",
    "            if len(available_block_features) == 0:\n",
    "                print(f\"    [WARNING] {block_name}: No features available, skipping\")\n",
    "                continue\n",
    "            \n",
    "            X_block = X_df[available_block_features].values\n",
    "            \n",
    "            print(f\"    {block_name}: {len(available_block_features)} features\")\n",
    "            \n",
    "            # Cross-validation (using neg_mean_squared_error, then convert to RMSE)\n",
    "            neg_mse_scores = cross_val_score(\n",
    "                lgb_model,\n",
    "                X_block,\n",
    "                y,\n",
    "                cv=cv,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                n_jobs=1\n",
    "            )\n",
    "            \n",
    "            # Convert to RMSE (neg_mean_squared_error is negative, so we negate and take sqrt)\n",
    "            rmse_scores = np.sqrt(-neg_mse_scores)\n",
    "            \n",
    "            # Store RMSE scores\n",
    "            block_rmse[block_name].extend(rmse_scores)\n",
    "            \n",
    "            mean_rmse = np.mean(rmse_scores)\n",
    "            std_rmse = np.std(rmse_scores)\n",
    "            print(f\"      Mean RMSE: {mean_rmse:.4f} (±{std_rmse:.4f})\")\n",
    "    \n",
    "    print(f\"\\n[OK] Incremental modeling completed for all {M} MICE datasets\")\n",
    "\n",
    "# ============================================================================\n",
    "# Task 5: Results Pooling and Output\n",
    "# ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 5: Results Pooling and Output\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate pooled RMSE (average across all MICE datasets and CV folds)\n",
    "    pooled_rmse = {}\n",
    "    pooled_rmse_std = {}\n",
    "    pooled_rmse_count = {}\n",
    "    \n",
    "    for block_name in feature_blocks.keys():\n",
    "        if len(block_rmse[block_name]) > 0:\n",
    "            pooled_rmse[block_name] = np.mean(block_rmse[block_name])\n",
    "            pooled_rmse_std[block_name] = np.std(block_rmse[block_name])\n",
    "            pooled_rmse_count[block_name] = len(block_rmse[block_name])\n",
    "    \n",
    "    # Calculate cumulative RMSE and Error Reduction (ΔRMSE)\n",
    "    cumulative_rmse = {}\n",
    "    delta_rmse = {}\n",
    "    \n",
    "    # Block 1 is the baseline\n",
    "    cumulative_rmse['Block 1'] = pooled_rmse['Block 1']\n",
    "    delta_rmse['Block 1'] = 0.0  # First block has no previous block for comparison\n",
    "    \n",
    "    # Calculate for subsequent blocks\n",
    "    # Error Reduction (ΔRMSE) = Previous Block RMSE - Current Block RMSE\n",
    "    # Positive value means error reduction (improvement), negative means error increase (worsening)\n",
    "    for i in range(2, len(feature_blocks) + 1):\n",
    "        block_name = f'Block {i}'\n",
    "        prev_block_name = f'Block {i-1}'\n",
    "        \n",
    "        cumulative_rmse[block_name] = pooled_rmse[block_name]\n",
    "        if not np.isnan(pooled_rmse[block_name]) and not np.isnan(cumulative_rmse[prev_block_name]):\n",
    "            # Error Reduction = Previous RMSE - Current RMSE\n",
    "            delta_rmse[block_name] = cumulative_rmse[prev_block_name] - pooled_rmse[block_name]\n",
    "    results_data = []\n",
    "    for block_name in feature_blocks.keys():\n",
    "        results_data.append({\n",
    "            'Model Block': block_name,\n",
    "            'Included Features': feature_blocks[block_name]['name'],\n",
    "            'Cumulative RMSE (Pooled Mean)': cumulative_rmse[block_name] if block_name in cumulative_rmse else pooled_rmse[block_name],\n",
    "            'Error Reduction (ΔRMSE)': delta_rmse[block_name] if block_name in delta_rmse else np.nan,\n",
    "            'N_Evaluations': pooled_rmse_count[block_name] if block_name in pooled_rmse_count else 0\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Print results table\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"RQ1: Incremental Contribution Analysis Results (RMSE)\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Model Block':<12} {'Included Features':<35} {'Cumulative RMSE (Pooled Mean)':<30} {'Error Reduction (ΔRMSE)':<25}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        cum_rmse_str = f\"{row['Cumulative RMSE (Pooled Mean)']:.4f}\" if not np.isnan(row['Cumulative RMSE (Pooled Mean)']) else \"N/A\"\n",
    "        delta_rmse_str = f\"{row['Error Reduction (ΔRMSE)']:+.4f}\" if not np.isnan(row['Error Reduction (ΔRMSE)']) else \"N/A\"\n",
    "        print(f\"{row['Model Block']:<12} {row['Included Features']:<35} {cum_rmse_str:>29} {delta_rmse_str:>24}\")\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Detailed Statistics:\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Model Block':<12} {'Included Features':<35} {'Cumulative RMSE':<20} {'RMSE Std':<15} {'Error Reduction (ΔRMSE)':<25} {'N_Evaluations':<15}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        cum_rmse_str = f\"{row['Cumulative RMSE (Pooled Mean)']:.4f}\" if not np.isnan(row['Cumulative RMSE (Pooled Mean)']) else \"N/A\"\n",
    "        # Get std from pooled_rmse_std\n",
    "        block_name = row['Model Block']\n",
    "        rmse_std_str = f\"±{pooled_rmse_std[block_name]:.4f}\" if block_name in pooled_rmse_std and not np.isnan(pooled_rmse_std[block_name]) else \"N/A\"\n",
    "        delta_rmse_str = f\"{row['Error Reduction (ΔRMSE)']:+.4f}\" if not np.isnan(row['Error Reduction (ΔRMSE)']) else \"N/A\"\n",
    "        print(f\"{row['Model Block']:<12} {row['Included Features']:<35} {cum_rmse_str:>19} {rmse_std_str:>14} {delta_rmse_str:>24} {row['N_Evaluations']:>14}\")\n",
    "    \n",
    "    # Error Reduction Analysis\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Error Reduction Analysis:\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    if not np.isnan(pooled_rmse['Block 1']):\n",
    "        baseline_rmse = pooled_rmse['Block 1']\n",
    "        print(f\"\\nBaseline RMSE (Block 1): {baseline_rmse:.4f}\")\n",
    "        print(f\"\\nError Reduction Breakdown:\")\n",
    "        \n",
    "        for block_name in feature_blocks.keys():\n",
    "            if block_name in delta_rmse and not np.isnan(delta_rmse[block_name]):\n",
    "                if delta_rmse[block_name] > 0:\n",
    "                    interpretation = \"error reduction (improvement)\"\n",
    "                elif delta_rmse[block_name] < 0:\n",
    "                    interpretation = \"error increase (worsening)\"\n",
    "    # Save results\n",
    "    results_filename = 'RQ1_Incremental_RMSE_LightGBM.csv'\n",
    "    results_df.to_csv(results_filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\n[OK] Results saved: {results_filename}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"[OK] RQ1 Incremental Contribution Analysis completed!\")\n",
    "    print(f\"  Processed {M} MICE datasets\")\n",
    "    print(f\"  Analyzed {len(feature_blocks)} feature blocks\")\n",
    "    print(f\"  Generated incremental contribution analysis table\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n[ERROR] Cannot proceed: MICE datasets not loaded successfully.\")\n",
    "    print(f\"  Expected {M} datasets, but loaded {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Cell 12: Pre-RQ2 - SHAP Calculation & Feature Extraction (LightGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task 1: Environment Verification & Data Loading\n",
      "============================================================\n",
      "[INFO] Case B: Reloading MICE datasets from CSV files\n",
      "[OK] M1: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "[OK] M2: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "[OK] M3: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "[OK] M4: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "[OK] M5: X shape (12939, 22), y shape (12939,), target: Delta_JobSatisfaction\n",
      "\n",
      "[OK] Successfully loaded 5 MICE datasets from CSV files\n",
      "  Total datasets: 5\n",
      "  Feature names preserved for SHAP analysis\n",
      "\n",
      "============================================================\n",
      "Task 2: Model Retraining and SHAP Calculation\n",
      "============================================================\n",
      "\n",
      "LightGBM Configuration:\n",
      "  n_estimators: 100\n",
      "  max_depth: 15\n",
      "  learning_rate: 0.1\n",
      "  num_leaves: 31\n",
      "  min_child_samples: 20\n",
      "  random_state: 42\n",
      "  n_jobs: 1\n",
      "  verbosity: -1\n",
      "\n",
      "============================================================\n",
      "Training Models and Computing SHAP Values...\n",
      "============================================================\n",
      "\n",
      "Processing MICE Dataset 1/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] Model trained\n",
      "  [OK] Feature names extracted: 22 features\n",
      "  Computing SHAP values (optimized: using booster_ and check_additivity=False)...\n",
      "  [OK] SHAP values computed: shape (12939, 22) (took 0.94 seconds)\n",
      "\n",
      "Processing MICE Dataset 2/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] Model trained\n",
      "  Computing SHAP values (optimized: using booster_ and check_additivity=False)...\n",
      "  [OK] SHAP values computed: shape (12939, 22) (took 0.98 seconds)\n",
      "\n",
      "Processing MICE Dataset 3/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] Model trained\n",
      "  Computing SHAP values (optimized: using booster_ and check_additivity=False)...\n",
      "  [OK] SHAP values computed: shape (12939, 22) (took 0.99 seconds)\n",
      "\n",
      "Processing MICE Dataset 4/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] Model trained\n",
      "  Computing SHAP values (optimized: using booster_ and check_additivity=False)...\n",
      "  [OK] SHAP values computed: shape (12939, 22) (took 0.91 seconds)\n",
      "\n",
      "Processing MICE Dataset 5/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] Model trained\n",
      "  Computing SHAP values (optimized: using booster_ and check_additivity=False)...\n",
      "  [OK] SHAP values computed: shape (12939, 22) (took 1.76 seconds)\n",
      "\n",
      "[OK] SHAP values computed for all 5 MICE datasets\n",
      "\n",
      "============================================================\n",
      "Task 3: SHAP Pooling and Feature Importance Calculation\n",
      "============================================================\n",
      "  Pooled SHAP values shape: (12939, 22)\n",
      "\n",
      "[OK] Feature importance calculated (pooled across 5 MICE datasets)\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "------------------------------------------------------------\n",
      "  1. cf23p455                            (Mean Abs SHAP: 0.091610)\n",
      "  2. Delta_Skills                        (Mean Abs SHAP: 0.048916)\n",
      "  3. cw23p003                            (Mean Abs SHAP: 0.044534)\n",
      "  4. C_Score                             (Mean Abs SHAP: 0.033872)\n",
      "  5. O_Score                             (Mean Abs SHAP: 0.028510)\n",
      "  6. Delta_Workload                      (Mean Abs SHAP: 0.028071)\n",
      "  7. Delta_Autonomy                      (Mean Abs SHAP: 0.026124)\n",
      "  8. N_Score                             (Mean Abs SHAP: 0.014149)\n",
      "  9. A_Score                             (Mean Abs SHAP: 0.011610)\n",
      "  10. ch23p004                            (Mean Abs SHAP: 0.011417)\n",
      "\n",
      "============================================================\n",
      "Task 4: Rubin's Rule Pooling for Top 10 Features\n",
      "============================================================\n",
      "\n",
      "Top 10 Features for Rubin's Rule Pooling:\n",
      "  1. cf23p455\n",
      "  2. Delta_Skills\n",
      "  3. cw23p003\n",
      "  4. C_Score\n",
      "  5. O_Score\n",
      "  6. Delta_Workload\n",
      "  7. Delta_Autonomy\n",
      "  8. N_Score\n",
      "  9. A_Score\n",
      "  10. ch23p004\n",
      "\n",
      "====================================================================================================\n",
      "Rubin's Rule Pooling Results (Top 10 Features)\n",
      "====================================================================================================\n",
      "\n",
      "Rank   Feature                             Pooled Mean Abs SHAP      Pooled SE      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1      cf23p455                                            0.091610       0.169582\n",
      "2      Delta_Skills                                        0.048916       0.125135\n",
      "3      cw23p003                                            0.044534       0.080738\n",
      "4      C_Score                                             0.033872       0.053731\n",
      "5      O_Score                                             0.028510       0.043372\n",
      "6      Delta_Workload                                      0.028071       0.073932\n",
      "7      Delta_Autonomy                                      0.026124       0.053648\n",
      "8      N_Score                                             0.014149       0.031423\n",
      "9      A_Score                                             0.011610       0.024445\n",
      "10     ch23p004                                            0.011417       0.017647\n",
      "\n",
      "[OK] Results saved: Rubins_Rule_SHAP_LightGBM.csv\n",
      "\n",
      "============================================================\n",
      "Task 5: Extract Top 10 Feature Names\n",
      "============================================================\n",
      "\n",
      "[OK] Top 10 Most Important Features (for RQ2 Refined Model):\n",
      "------------------------------------------------------------\n",
      "  1. cf23p455                            (Pooled Mean Abs SHAP: 0.091610)\n",
      "  2. Delta_Skills                        (Pooled Mean Abs SHAP: 0.048916)\n",
      "  3. cw23p003                            (Pooled Mean Abs SHAP: 0.044534)\n",
      "  4. C_Score                             (Pooled Mean Abs SHAP: 0.033872)\n",
      "  5. O_Score                             (Pooled Mean Abs SHAP: 0.028510)\n",
      "  6. Delta_Workload                      (Pooled Mean Abs SHAP: 0.028071)\n",
      "  7. Delta_Autonomy                      (Pooled Mean Abs SHAP: 0.026124)\n",
      "  8. N_Score                             (Pooled Mean Abs SHAP: 0.014149)\n",
      "  9. A_Score                             (Pooled Mean Abs SHAP: 0.011610)\n",
      "  10. ch23p004                            (Pooled Mean Abs SHAP: 0.011417)\n",
      "\n",
      "[INFO] These top 10 features will be used in Cell 13 (RQ2 Refined Model Comparison)\n",
      "\n",
      "[OK] Top 10 feature names saved to: Top_10_Features_LightGBM.txt\n",
      "\n",
      "============================================================\n",
      "[OK] Pre-RQ2 SHAP Calculation & Feature Extraction completed!\n",
      "  Processed 5 MICE datasets\n",
      "  Computed and pooled SHAP values\n",
      "  Applied Rubin's Rule for top 10 features\n",
      "  Extracted top 10 feature names for RQ2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 12:  Pre-RQ2 - SHAP Calculation & Feature Extraction (LightGBM)\n",
    "# Pre-RQ2: SHAP Calculation & Feature Extraction using LightGBM\n",
    "print(\"=\"*60)\n",
    "print(\"Cell 12: Pre-RQ2 - SHAP Calculation & Feature Extraction (LightGBM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if LightGBM is available\n",
    "try:\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    raise ImportError(\"LightGBM is required but could not be imported.\")\n",
    "\n",
    "# Set matplotlib style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# ============================================================================\n",
    "# Task 1: Environment Verification & Data Loading\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Task 1: Environment Verification & Data Loading\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Case A: Check if mice_X_datasets and mice_Y_datasets exist in environment\n",
    "try:\n",
    "    if 'mice_X_datasets' in globals() and 'mice_Y_datasets' in globals() and \\\n",
    "       mice_X_datasets is not None and mice_Y_datasets is not None and \\\n",
    "       len(mice_X_datasets) > 0 and len(mice_Y_datasets) > 0:\n",
    "        print(\"[INFO] Case A: Using existing mice_X_datasets and mice_Y_datasets from environment\")\n",
    "        print(f\"  Found {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n",
    "        M = len(mice_X_datasets)\n",
    "        # Ensure mice_X_dataframes exists\n",
    "        if 'mice_X_dataframes' not in globals() or mice_X_dataframes is None or len(mice_X_dataframes) == 0:\n",
    "            print(\"[WARNING] mice_X_dataframes not found, will reload to preserve feature names\")\n",
    "            raise NameError(\"mice_X_dataframes missing\")\n",
    "        else:\n",
    "            print(f\"  Found {len(mice_X_dataframes)} X dataframes with feature names\")\n",
    "            data_loaded = True\n",
    "    else:\n",
    "        raise NameError(\"Variables not available\")\n",
    "except (NameError, AttributeError):\n",
    "    # Case B: Reload from CSV files\n",
    "    print(\"[INFO] Case B: Reloading MICE datasets from CSV files\")\n",
    "    data_loaded = False\n",
    "\n",
    "if not data_loaded:\n",
    "    M = 5\n",
    "    mice_X_datasets = []\n",
    "    mice_Y_datasets = []\n",
    "    mice_X_dataframes = []\n",
    "\n",
    "    for m in range(1, M + 1):\n",
    "        X_file = f'LISS_Final_X_M{m}.csv'\n",
    "        Y_file = f'LISS_Final_Y_M{m}.csv'\n",
    "        \n",
    "        try:\n",
    "            X_m = pd.read_csv(X_file, encoding='utf-8', index_col=0)\n",
    "            Y_m = pd.read_csv(Y_file, encoding='utf-8', index_col=0)\n",
    "            \n",
    "            # Align indices\n",
    "            common_idx = X_m.index.intersection(Y_m.index)\n",
    "            X_m = X_m.loc[common_idx]\n",
    "            Y_m = Y_m.loc[common_idx]\n",
    "            \n",
    "            # Check for missing values\n",
    "            X_missing = X_m.isna().sum().sum()\n",
    "            Y_missing = Y_m.isna().sum().sum()\n",
    "            \n",
    "            if X_missing > 0 or Y_missing > 0:\n",
    "                valid_mask = ~(X_m.isna().any(axis=1) | Y_m.isna().any(axis=1))\n",
    "                X_m = X_m[valid_mask]\n",
    "                Y_m = Y_m[valid_mask]\n",
    "            \n",
    "            # Extract target variable\n",
    "            target_col = Y_m.columns[0]\n",
    "            y_m = Y_m[target_col].values\n",
    "            X_m_values = X_m.values\n",
    "            \n",
    "            mice_X_datasets.append(X_m_values)\n",
    "            mice_Y_datasets.append(y_m)\n",
    "            mice_X_dataframes.append(X_m.copy())  # Keep DataFrame for feature names\n",
    "            \n",
    "            print(f\"[OK] M{m}: X shape {X_m_values.shape}, y shape {y_m.shape}, target: {target_col}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"[ERROR] M{m}: Could not find {X_file} or {Y_file}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] M{m}: Error loading data: {e}\")\n",
    "            break\n",
    "\n",
    "    if len(mice_X_datasets) == M:\n",
    "        print(f\"\\n[OK] Successfully loaded {M} MICE datasets from CSV files\")\n",
    "        print(f\"  Total datasets: {len(mice_X_datasets)}\")\n",
    "        print(f\"  Feature names preserved for SHAP analysis\")\n",
    "    else:\n",
    "        print(f\"\\n[ERROR] Only loaded {len(mice_X_datasets)}/{M} datasets. Cannot proceed.\")\n",
    "        mice_X_datasets = []\n",
    "        mice_Y_datasets = []\n",
    "        mice_X_dataframes = []\n",
    "\n",
    "# Verify data is available\n",
    "if len(mice_X_datasets) == M and len(mice_Y_datasets) == M:\n",
    "    # ============================================================================\n",
    "    # Task 2: Model Retraining and SHAP Calculation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Model Retraining and SHAP Calculation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define LightGBM Regressor\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLightGBM Configuration:\")\n",
    "    print(f\"  n_estimators: 100\")\n",
    "    print(f\"  max_depth: 15\")\n",
    "    print(f\"  learning_rate: 0.1\")\n",
    "    print(f\"  num_leaves: 31\")\n",
    "    print(f\"  min_child_samples: 20\")\n",
    "    print(f\"  random_state: 42\")\n",
    "    print(f\"  n_jobs: 1\")\n",
    "    print(f\"  verbosity: -1\")\n",
    "    \n",
    "    # Store SHAP values and Mean Absolute SHAP from all MICE datasets\n",
    "    shap_values_list = []\n",
    "    mean_abs_shap_list = []  # Q_m for each MICE dataset\n",
    "    feature_names = None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Models and Computing SHAP Values...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Main loop: iterate over MICE datasets\n",
    "    for m_idx in range(M):\n",
    "        print(f\"\\nProcessing MICE Dataset {m_idx+1}/{M}...\")\n",
    "        \n",
    "        X = mice_X_datasets[m_idx]\n",
    "        y = mice_Y_datasets[m_idx]\n",
    "        X_df = mice_X_dataframes[m_idx]\n",
    "        \n",
    "        print(f\"  Dataset shape: X {X.shape}, y {y.shape}\")\n",
    "        \n",
    "        # Train LightGBM model on this dataset\n",
    "        lgb_model.fit(X, y)\n",
    "        print(f\"  [OK] Model trained\")\n",
    "        \n",
    "        # Get feature names (use from first dataset)\n",
    "        if feature_names is None:\n",
    "            feature_names = X_df.columns.tolist()\n",
    "            print(f\"  [OK] Feature names extracted: {len(feature_names)} features\")\n",
    "        \n",
    "        # Calculate SHAP values using TreeExplainer (optimized for speed)\n",
    "        print(f\"  Computing SHAP values (optimized: using booster_ and check_additivity=False)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use booster_ for faster computation and disable additivity check\n",
    "        try:\n",
    "            explainer = shap.TreeExplainer(lgb_model.booster_)\n",
    "            shap_values = explainer.shap_values(X, check_additivity=False)\n",
    "        except AttributeError:\n",
    "            # Fallback: if booster_ is not available, use model directly\n",
    "            print(f\"    [WARNING] booster_ not available, using model directly\")\n",
    "            explainer = shap.TreeExplainer(lgb_model)\n",
    "            shap_values = explainer.shap_values(X, check_additivity=False)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"  [OK] SHAP values computed: shape {shap_values.shape} (took {elapsed_time:.2f} seconds)\")\n",
    "        \n",
    "        # Calculate Mean Absolute SHAP values for each feature (Q_m)\n",
    "        mean_abs_shap = np.abs(shap_values).mean(axis=0)  # Shape: (n_features,)\n",
    "        mean_abs_shap_list.append(mean_abs_shap)\n",
    "        shap_values_list.append(shap_values)\n",
    "    \n",
    "    print(f\"\\n[OK] SHAP values computed for all {M} MICE datasets\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: SHAP Pooling and Feature Importance Calculation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: SHAP Pooling and Feature Importance Calculation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Pool SHAP values (average across MICE datasets)\n",
    "    pooled_shap_values = np.mean(shap_values_list, axis=0)\n",
    "    print(f\"  Pooled SHAP values shape: {pooled_shap_values.shape}\")\n",
    "    \n",
    "    # Calculate pooled Mean Absolute SHAP (simple average)\n",
    "    pooled_mean_abs_shap = np.mean(mean_abs_shap_list, axis=0)\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    feature_importance_data = []\n",
    "    for i, feat_name in enumerate(feature_names):\n",
    "        feature_importance_data.append({\n",
    "            'Feature': feat_name,\n",
    "            'Mean_Abs_SHAP': pooled_mean_abs_shap[i]\n",
    "        })\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame(feature_importance_data)\n",
    "    feature_importance_df = feature_importance_df.sort_values('Mean_Abs_SHAP', ascending=False)\n",
    "    feature_importance_df['Rank'] = range(1, len(feature_importance_df) + 1)\n",
    "    feature_importance_df = feature_importance_df[['Rank', 'Feature', 'Mean_Abs_SHAP']]\n",
    "    \n",
    "    print(f\"\\n[OK] Feature importance calculated (pooled across {M} MICE datasets)\")\n",
    "    print(f\"\\nTop 10 Most Important Features:\")\n",
    "    print(\"-\"*60)\n",
    "    for idx, row in feature_importance_df.head(10).iterrows():\n",
    "        print(f\"  {row['Rank']}. {row['Feature']:<35} (Mean Abs SHAP: {row['Mean_Abs_SHAP']:.6f})\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 4: Rubin's Rule Pooling for Top 10 Features\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 4: Rubin's Rule Pooling for Top 10 Features\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get top 10 features\n",
    "    top_10_features = feature_importance_df.head(10)['Feature'].tolist()\n",
    "    print(f\"\\nTop 10 Features for Rubin's Rule Pooling:\")\n",
    "    for i, feat in enumerate(top_10_features, 1):\n",
    "        print(f\"  {i}. {feat}\")\n",
    "    \n",
    "    # Extract Q_m (Mean Absolute SHAP) for top 10 features from each MICE dataset\n",
    "    Q_m_top10 = {}  # Q_m[feature_name] = [Q_m1, Q_m2, ..., Q_m5]\n",
    "    U_m_top10 = {}  # U_m[feature_name] = [U_m1, U_m2, ..., U_m5]\n",
    "    \n",
    "    for feat_name in top_10_features:\n",
    "        feat_idx = feature_names.index(feat_name)\n",
    "        Q_m_list = []\n",
    "        U_m_list = []\n",
    "        \n",
    "        for m_idx in range(M):\n",
    "            # Q_m: Mean Absolute SHAP for this feature in this MICE dataset\n",
    "            Q_m = mean_abs_shap_list[m_idx][feat_idx]\n",
    "            Q_m_list.append(Q_m)\n",
    "            \n",
    "            # U_m: Variance of absolute SHAP values for this feature in this MICE dataset\n",
    "            abs_shap_feat = np.abs(shap_values_list[m_idx][:, feat_idx])\n",
    "            U_m = np.var(abs_shap_feat)  # Variance across samples\n",
    "            U_m_list.append(U_m)\n",
    "        \n",
    "        Q_m_top10[feat_name] = Q_m_list\n",
    "        U_m_top10[feat_name] = U_m_list\n",
    "    \n",
    "    # Apply Rubin's Rule for each top 10 feature\n",
    "    rubin_results = []\n",
    "    \n",
    "    for feat_name in top_10_features:\n",
    "        Q_m = np.array(Q_m_top10[feat_name])  # Shape: (M,)\n",
    "        U_m = np.array(U_m_top10[feat_name])  # Shape: (M,)\n",
    "        \n",
    "        # Pooled Estimate: Q_bar = mean(Q_m)\n",
    "        Q_bar = np.mean(Q_m)\n",
    "        \n",
    "        # Between-imputation variance: B = var(Q_m)\n",
    "        B = np.var(Q_m, ddof=1)  # Sample variance\n",
    "        \n",
    "        # Within-imputation variance: U_bar = mean(U_m)\n",
    "        U_bar = np.mean(U_m)\n",
    "        \n",
    "        # Total variance: T = U_bar + (1 + 1/M) * B\n",
    "        T = U_bar + (1 + 1/M) * B\n",
    "        \n",
    "        # Pooled Standard Error: SE_pooled = sqrt(T)\n",
    "        SE_pooled = np.sqrt(T)\n",
    "        \n",
    "        rubin_results.append({\n",
    "            'Feature': feat_name,\n",
    "            'Pooled_Mean_Abs_SHAP': Q_bar,\n",
    "            'Pooled_SE': SE_pooled,\n",
    "            'Between_Variance': B,\n",
    "            'Within_Variance': U_bar,\n",
    "            'Total_Variance': T\n",
    "        })\n",
    "    \n",
    "    # Create Rubin's Rule results DataFrame\n",
    "    rubin_df = pd.DataFrame(rubin_results)\n",
    "    rubin_df = rubin_df.sort_values('Pooled_Mean_Abs_SHAP', ascending=False)\n",
    "    rubin_df['Rank'] = range(1, len(rubin_df) + 1)\n",
    "    rubin_df = rubin_df[['Rank', 'Feature', 'Pooled_Mean_Abs_SHAP', 'Pooled_SE', 'Between_Variance', 'Within_Variance', 'Total_Variance']]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Rubin's Rule Pooling Results (Top 10 Features)\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Rank':<6} {'Feature':<35} {'Pooled Mean Abs SHAP':<25} {'Pooled SE':<15}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for idx, row in rubin_df.iterrows():\n",
    "        print(f\"{row['Rank']:<6} {row['Feature']:<35} {row['Pooled_Mean_Abs_SHAP']:>24.6f} {row['Pooled_SE']:>14.6f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_filename = 'Rubins_Rule_SHAP_LightGBM.csv'\n",
    "    rubin_df.to_csv(results_filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\n[OK] Results saved: {results_filename}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 5: Extract and Print Top 10 Feature Names\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 5: Extract Top 10 Feature Names\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    top_10_feature_names = rubin_df['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\n[OK] Top 10 Most Important Features (for RQ2 Refined Model):\")\n",
    "    print(\"-\"*60)\n",
    "    for i, feat_name in enumerate(top_10_feature_names, 1):\n",
    "        feat_importance = rubin_df[rubin_df['Feature'] == feat_name]['Pooled_Mean_Abs_SHAP'].values[0]\n",
    "        print(f\"  {i}. {feat_name:<35} (Pooled Mean Abs SHAP: {feat_importance:.6f})\")\n",
    "    \n",
    "    print(f\"\\n[INFO] These top 10 features will be used in Cell 13 (RQ2 Refined Model Comparison)\")\n",
    "    \n",
    "    # Save top 10 feature names to a simple text file for easy access\n",
    "    with open('Top_10_Features_LightGBM.txt', 'w', encoding='utf-8') as f:\n",
    "        for feat_name in top_10_feature_names:\n",
    "            f.write(f\"{feat_name}\\n\")\n",
    "    print(f\"\\n[OK] Top 10 feature names saved to: Top_10_Features_LightGBM.txt\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"[OK] Pre-RQ2 SHAP Calculation & Feature Extraction completed!\")\n",
    "    print(f\"  Processed {M} MICE datasets\")\n",
    "    print(f\"  Computed and pooled SHAP values\")\n",
    "    print(f\"  Applied Rubin's Rule for top 10 features\")\n",
    "    print(f\"  Extracted top 10 feature names for RQ2\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n[ERROR] Cannot proceed: MICE datasets not loaded successfully.\")\n",
    "    print(f\"  Expected {M} datasets, but loaded {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Cell 13: RQ2 - Refined Model Comparison (LightGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task 1: Load Data and SHAP Rankings\n",
      "============================================================\n",
      "[OK] Loaded SHAP importance rankings from: Rubins_Rule_SHAP_LightGBM.csv\n",
      "  Total features in rankings: 10\n",
      "\n",
      "[OK] Extracted top 10 features:\n",
      "  1. cf23p455\n",
      "  2. Delta_Skills\n",
      "  3. cw23p003\n",
      "  4. C_Score\n",
      "  5. O_Score\n",
      "  6. Delta_Workload\n",
      "  7. Delta_Autonomy\n",
      "  8. N_Score\n",
      "  9. A_Score\n",
      "  10. ch23p004\n",
      "\n",
      "[INFO] Loading MICE datasets...\n",
      "[INFO] Case A: Using existing mice_X_datasets and mice_Y_datasets from environment\n",
      "  Found 5 X datasets and 5 Y datasets\n",
      "  Found 5 X dataframes with feature names\n",
      "\n",
      "============================================================\n",
      "Task 2: Train Refined Model (Top 10 Features)\n",
      "============================================================\n",
      "\n",
      "LightGBM Configuration (consistent with Cell 11):\n",
      "  n_estimators: 100\n",
      "  max_depth: 15\n",
      "  learning_rate: 0.1\n",
      "  num_leaves: 31\n",
      "  min_child_samples: 20\n",
      "  random_state: 42\n",
      "  n_jobs: 1\n",
      "  verbosity: -1\n",
      "\n",
      "Cross-Validation Setup:\n",
      "  CV: 5-fold KFold\n",
      "  MICE Datasets: 5\n",
      "  Refined Model Features: 10\n",
      "  Total evaluations: 5 datasets × 5 folds = 25\n",
      "\n",
      "============================================================\n",
      "Training Refined Model Across MICE Datasets...\n",
      "============================================================\n",
      "\n",
      "Processing MICE Dataset 1/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  Refined model features: 10\n",
      "  Mean RMSE: 0.4865 (±0.0305)\n",
      "\n",
      "Processing MICE Dataset 2/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  Refined model features: 10\n",
      "  Mean RMSE: 0.4865 (±0.0305)\n",
      "\n",
      "Processing MICE Dataset 3/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  Refined model features: 10\n",
      "  Mean RMSE: 0.4865 (±0.0305)\n",
      "\n",
      "Processing MICE Dataset 4/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  Refined model features: 10\n",
      "  Mean RMSE: 0.4865 (±0.0305)\n",
      "\n",
      "Processing MICE Dataset 5/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  Refined model features: 10\n",
      "  Mean RMSE: 0.4865 (±0.0305)\n",
      "\n",
      "[OK] Refined model evaluation completed\n",
      "  Pooled Mean RMSE: 0.4865 (±0.0305)\n",
      "  Total evaluations: 25\n",
      "\n",
      "============================================================\n",
      "Task 3: Compare with Full Model\n",
      "============================================================\n",
      "[OK] Loaded Full Model RMSE from: RQ1_Incremental_RMSE_LightGBM.csv\n",
      "  Full Model (Block 4) RMSE: 0.4893\n",
      "\n",
      "====================================================================================================\n",
      "RQ2: Refined Model vs Full Model Comparison\n",
      "====================================================================================================\n",
      "\n",
      "Model Version        Number of Features   Mean RMSE       RMSE Std        Difference (ΔRMSE)   Performance Loss (%)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Full Model           22                           0.4893            N/A             +0.0000              +0.00%\n",
      "Refined Model        10                           0.4865        ±0.0305             -0.0028              -0.58%\n",
      "\n",
      "====================================================================================================\n",
      "Interpretation:\n",
      "====================================================================================================\n",
      "  Refined Model has lower RMSE (-0.0028), indicating improved performance.\n",
      "  This suggests that the top 10 features are sufficient and other features may add noise.\n",
      "\n",
      "[OK] Results saved: RQ2_Refined_Model_LightGBM.csv\n",
      "\n",
      "============================================================\n",
      "[OK] RQ2 Refined Model Comparison completed!\n",
      "  Processed 5 MICE datasets\n",
      "  Evaluated Refined Model with 10 features\n",
      "  Generated comparison table\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 13:  RQ2 - Refined Model Comparison (LightGBM)\n",
    "# RQ2: Refined Model Comparison using LightGBM\n",
    "print(\"=\"*60)\n",
    "print(\"Cell 13: RQ2 - Refined Model Comparison (LightGBM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if LightGBM is available\n",
    "try:\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    raise ImportError(\"LightGBM is required but could not be imported.\")\n",
    "\n",
    "# ============================================================================\n",
    "# Task 1: Load Data and SHAP Rankings\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Task 1: Load Data and SHAP Rankings\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load SHAP importance rankings\n",
    "shap_importance_file = 'Rubins_Rule_SHAP_LightGBM.csv'\n",
    "try:\n",
    "    shap_importance_df = pd.read_csv(shap_importance_file, encoding='utf-8')\n",
    "    print(f\"[OK] Loaded SHAP importance rankings from: {shap_importance_file}\")\n",
    "    print(f\"  Total features in rankings: {len(shap_importance_df)}\")\n",
    "    \n",
    "    # Extract top 10 feature names\n",
    "    top_10_features = shap_importance_df.head(10)['Feature'].tolist()\n",
    "    print(f\"\\n[OK] Extracted top 10 features:\")\n",
    "    for i, feat in enumerate(top_10_features, 1):\n",
    "        print(f\"  {i}. {feat}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Could not find {shap_importance_file}\")\n",
    "    print(f\"  Please run Cell 12 first to generate SHAP importance rankings.\")\n",
    "    top_10_features = []\n",
    "\n",
    "# Load MICE datasets\n",
    "print(f\"\\n[INFO] Loading MICE datasets...\")\n",
    "\n",
    "# Case A: Check if mice_X_datasets exist in environment\n",
    "try:\n",
    "    if 'mice_X_datasets' in globals() and 'mice_Y_datasets' in globals() and \\\n",
    "       mice_X_datasets is not None and mice_Y_datasets is not None and \\\n",
    "       len(mice_X_datasets) > 0 and len(mice_Y_datasets) > 0:\n",
    "        print(\"[INFO] Case A: Using existing mice_X_datasets and mice_Y_datasets from environment\")\n",
    "        print(f\"  Found {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n",
    "        M = len(mice_X_datasets)\n",
    "        if 'mice_X_dataframes' not in globals() or mice_X_dataframes is None or len(mice_X_dataframes) == 0:\n",
    "            print(\"[WARNING] mice_X_dataframes not found, will reload to preserve feature names\")\n",
    "            raise NameError(\"mice_X_dataframes missing\")\n",
    "        else:\n",
    "            print(f\"  Found {len(mice_X_dataframes)} X dataframes with feature names\")\n",
    "            data_loaded = True\n",
    "    else:\n",
    "        raise NameError(\"Variables not available\")\n",
    "except (NameError, AttributeError):\n",
    "    # Case B: Reload from CSV files\n",
    "    print(\"[INFO] Case B: Reloading MICE datasets from CSV files\")\n",
    "    data_loaded = False\n",
    "\n",
    "if not data_loaded:\n",
    "    M = 5\n",
    "    mice_X_datasets = []\n",
    "    mice_Y_datasets = []\n",
    "    mice_X_dataframes = []\n",
    "\n",
    "    for m in range(1, M + 1):\n",
    "        X_file = f'LISS_Final_X_M{m}.csv'\n",
    "        Y_file = f'LISS_Final_Y_M{m}.csv'\n",
    "        \n",
    "        try:\n",
    "            X_m = pd.read_csv(X_file, encoding='utf-8', index_col=0)\n",
    "            Y_m = pd.read_csv(Y_file, encoding='utf-8', index_col=0)\n",
    "            \n",
    "            # Align indices\n",
    "            common_idx = X_m.index.intersection(Y_m.index)\n",
    "            X_m = X_m.loc[common_idx]\n",
    "            Y_m = Y_m.loc[common_idx]\n",
    "            \n",
    "            # Check for missing values\n",
    "            X_missing = X_m.isna().sum().sum()\n",
    "            Y_missing = Y_m.isna().sum().sum()\n",
    "            \n",
    "            if X_missing > 0 or Y_missing > 0:\n",
    "                valid_mask = ~(X_m.isna().any(axis=1) | Y_m.isna().any(axis=1))\n",
    "                X_m = X_m[valid_mask]\n",
    "                Y_m = Y_m[valid_mask]\n",
    "            \n",
    "            # Extract target variable\n",
    "            target_col = Y_m.columns[0]\n",
    "            y_m = Y_m[target_col].values\n",
    "            X_m_values = X_m.values\n",
    "            \n",
    "            mice_X_datasets.append(X_m_values)\n",
    "            mice_Y_datasets.append(y_m)\n",
    "            mice_X_dataframes.append(X_m.copy())\n",
    "            \n",
    "            print(f\"[OK] M{m}: X shape {X_m_values.shape}, y shape {y_m.shape}, target: {target_col}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"[ERROR] M{m}: Could not find {X_file} or {Y_file}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] M{m}: Error loading data: {e}\")\n",
    "            break\n",
    "\n",
    "    if len(mice_X_datasets) == M:\n",
    "        print(f\"\\n[OK] Successfully loaded {M} MICE datasets from CSV files\")\n",
    "    else:\n",
    "        print(f\"\\n[ERROR] Only loaded {len(mice_X_datasets)}/{M} datasets. Cannot proceed.\")\n",
    "        mice_X_datasets = []\n",
    "        mice_Y_datasets = []\n",
    "        mice_X_dataframes = []\n",
    "\n",
    "# Verify data is available\n",
    "if len(mice_X_datasets) == M and len(mice_Y_datasets) == M and len(top_10_features) == 10:\n",
    "    # ============================================================================\n",
    "    # Task 2: Train Refined Model (Top 10 Features)\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Train Refined Model (Top 10 Features)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define LightGBM Regressor (same parameters as Cell 11)\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLightGBM Configuration (consistent with Cell 11):\")\n",
    "    print(f\"  n_estimators: 100\")\n",
    "    print(f\"  max_depth: 15\")\n",
    "    print(f\"  learning_rate: 0.1\")\n",
    "    print(f\"  num_leaves: 31\")\n",
    "    print(f\"  min_child_samples: 20\")\n",
    "    print(f\"  random_state: 42\")\n",
    "    print(f\"  n_jobs: 1\")\n",
    "    print(f\"  verbosity: -1\")\n",
    "    \n",
    "    # 5-Fold Cross-Validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Store RMSE scores for refined model across all MICE datasets and CV folds\n",
    "    refined_rmse_scores = []\n",
    "    \n",
    "    print(f\"\\nCross-Validation Setup:\")\n",
    "    print(f\"  CV: {cv.n_splits}-fold KFold\")\n",
    "    print(f\"  MICE Datasets: {M}\")\n",
    "    print(f\"  Refined Model Features: {len(top_10_features)}\")\n",
    "    print(f\"  Total evaluations: {M} datasets × {cv.n_splits} folds = {M * cv.n_splits}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Refined Model Across MICE Datasets...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Main loop: iterate over MICE datasets\n",
    "    for m_idx in range(M):\n",
    "        print(f\"\\nProcessing MICE Dataset {m_idx+1}/{M}...\")\n",
    "        \n",
    "        X_df = mice_X_dataframes[m_idx]\n",
    "        y = mice_Y_datasets[m_idx]\n",
    "        \n",
    "        print(f\"  Dataset shape: X {X_df.shape}, y {y.shape}\")\n",
    "        \n",
    "        # Select only top 10 features\n",
    "        available_top10 = [f for f in top_10_features if f in X_df.columns]\n",
    "        \n",
    "        if len(available_top10) < len(top_10_features):\n",
    "            print(f\"  [WARNING] Only {len(available_top10)}/{len(top_10_features)} top 10 features available\")\n",
    "            missing = set(top_10_features) - set(available_top10)\n",
    "            print(f\"    Missing: {missing}\")\n",
    "        \n",
    "        if len(available_top10) == 0:\n",
    "            print(f\"  [ERROR] No top 10 features available, skipping this dataset\")\n",
    "            continue\n",
    "        \n",
    "        X_refined = X_df[available_top10].values\n",
    "        \n",
    "        print(f\"  Refined model features: {len(available_top10)}\")\n",
    "        \n",
    "        # Cross-validation (using neg_mean_squared_error, then convert to RMSE)\n",
    "        neg_mse_scores = cross_val_score(\n",
    "            lgb_model,\n",
    "            X_refined,\n",
    "            y,\n",
    "            cv=cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=1\n",
    "        )\n",
    "        \n",
    "        # Convert to RMSE\n",
    "        rmse_scores = np.sqrt(-neg_mse_scores)\n",
    "        refined_rmse_scores.extend(rmse_scores)\n",
    "        \n",
    "        mean_rmse = np.mean(rmse_scores)\n",
    "        std_rmse = np.std(rmse_scores)\n",
    "        print(f\"  Mean RMSE: {mean_rmse:.4f} (±{std_rmse:.4f})\")\n",
    "    \n",
    "    # Calculate pooled RMSE for refined model\n",
    "    refined_model_rmse = np.mean(refined_rmse_scores)\n",
    "    refined_model_rmse_std = np.std(refined_rmse_scores)\n",
    "    \n",
    "    print(f\"\\n[OK] Refined model evaluation completed\")\n",
    "    print(f\"  Pooled Mean RMSE: {refined_model_rmse:.4f} (±{refined_model_rmse_std:.4f})\")\n",
    "    print(f\"  Total evaluations: {len(refined_rmse_scores)}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: Compare with Full Model\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Compare with Full Model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load full model RMSE from Cell 11 results (RQ1 Block 4 - Full Model)\n",
    "    benchmarking_file = 'RQ1_Incremental_RMSE_LightGBM.csv'\n",
    "    try:\n",
    "        rq1_results = pd.read_csv(benchmarking_file, encoding='utf-8')\n",
    "        # Get Block 4 (Full Model) RMSE\n",
    "        block4_row = rq1_results[rq1_results['Model Block'] == 'Block 4']\n",
    "        if len(block4_row) > 0:\n",
    "            full_model_rmse = block4_row.iloc[0]['Cumulative RMSE (Pooled Mean)']\n",
    "            # Try to get std if available\n",
    "            if 'RMSE Std' in rq1_results.columns:\n",
    "                full_model_rmse_std = block4_row.iloc[0].get('RMSE Std', np.nan)\n",
    "            else:\n",
    "                full_model_rmse_std = np.nan\n",
    "            print(f\"[OK] Loaded Full Model RMSE from: {benchmarking_file}\")\n",
    "            print(f\"  Full Model (Block 4) RMSE: {full_model_rmse:.4f}\")\n",
    "        else:\n",
    "            print(f\"[WARNING] Could not find Block 4 in {benchmarking_file}\")\n",
    "            print(f\"  Using default value or calculating from Cell 11 results\")\n",
    "            # Fallback: use a reasonable default or calculate\n",
    "            full_model_rmse = None\n",
    "            full_model_rmse_std = None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[WARNING] Could not find {benchmarking_file}\")\n",
    "        print(f\"  Please run Cell 11 first to generate RQ1 results.\")\n",
    "        print(f\"  Will use Refined Model RMSE only for comparison.\")\n",
    "        full_model_rmse = None\n",
    "        full_model_rmse_std = None\n",
    "    \n",
    "    # If full model RMSE not available, we can still report refined model results\n",
    "    if full_model_rmse is not None:\n",
    "        # Calculate performance loss\n",
    "        delta_rmse = refined_model_rmse - full_model_rmse\n",
    "        performance_loss_pct = (delta_rmse / full_model_rmse) * 100 if full_model_rmse > 0 else 0\n",
    "        \n",
    "        # Create comparison table\n",
    "        results_data = [\n",
    "            {\n",
    "                'Model Version': 'Full Model',\n",
    "                'Number of Features': len(mice_X_dataframes[0].columns),\n",
    "                'Mean RMSE': full_model_rmse,\n",
    "                'RMSE Std': full_model_rmse_std if not np.isnan(full_model_rmse_std) else np.nan,\n",
    "                'Difference': 0.0,\n",
    "                'Performance_Loss_Pct': 0.0\n",
    "            },\n",
    "            {\n",
    "                'Model Version': 'Refined Model',\n",
    "                'Number of Features': len(top_10_features),\n",
    "                'Mean RMSE': refined_model_rmse,\n",
    "                'RMSE Std': refined_model_rmse_std,\n",
    "                'Difference': delta_rmse,\n",
    "                'Performance_Loss_Pct': performance_loss_pct\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        \n",
    "        # Print comparison table\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"RQ2: Refined Model vs Full Model Comparison\")\n",
    "        print(\"=\"*100)\n",
    "        print(f\"\\n{'Model Version':<20} {'Number of Features':<20} {'Mean RMSE':<15} {'RMSE Std':<15} {'Difference (ΔRMSE)':<20} {'Performance Loss (%)':<20}\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        for idx, row in results_df.iterrows():\n",
    "            diff_str = f\"{row['Difference']:+.4f}\" if not np.isnan(row['Difference']) else \"N/A\"\n",
    "            loss_str = f\"{row['Performance_Loss_Pct']:+.2f}%\" if not np.isnan(row['Performance_Loss_Pct']) else \"N/A\"\n",
    "            rmse_std_str = f\"±{row['RMSE Std']:.4f}\" if not np.isnan(row['RMSE Std']) else \"N/A\"\n",
    "            print(f\"{row['Model Version']:<20} {row['Number of Features']:<20} {row['Mean RMSE']:>14.4f} {rmse_std_str:>14} {diff_str:>19} {loss_str:>19}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"Interpretation:\")\n",
    "        print(\"=\"*100)\n",
    "        if delta_rmse > 0:\n",
    "            print(f\"  Refined Model has higher RMSE (+{delta_rmse:.4f}), indicating {performance_loss_pct:.2f}% performance loss.\")\n",
    "            print(f\"  This suggests that the top 10 features alone cannot fully capture the predictive power of all features.\")\n",
    "        elif delta_rmse < 0:\n",
    "            print(f\"  Refined Model has lower RMSE ({delta_rmse:.4f}), indicating improved performance.\")\n",
    "            print(f\"  This suggests that the top 10 features are sufficient and other features may add noise.\")\n",
    "        else:\n",
    "            print(f\"  Refined Model has similar RMSE, indicating that top 10 features are sufficient.\")\n",
    "        \n",
    "        # Save results\n",
    "        results_filename = 'RQ2_Refined_Model_LightGBM.csv'\n",
    "        results_df.to_csv(results_filename, index=False, encoding='utf-8')\n",
    "        print(f\"\\n[OK] Results saved: {results_filename}\")\n",
    "    else:\n",
    "        # Only refined model results available\n",
    "        results_data = [\n",
    "            {\n",
    "                'Model Version': 'Refined Model',\n",
    "                'Number of Features': len(top_10_features),\n",
    "                'Mean RMSE': refined_model_rmse,\n",
    "                'RMSE Std': refined_model_rmse_std,\n",
    "                'Difference': np.nan,\n",
    "                'Performance_Loss_Pct': np.nan\n",
    "            }\n",
    "        ]\n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        \n",
    "        print(\"\\n[INFO] Full Model RMSE not available. Showing Refined Model results only:\")\n",
    "        print(f\"  Refined Model RMSE: {refined_model_rmse:.4f} (±{refined_model_rmse_std:.4f})\")\n",
    "        \n",
    "        results_filename = 'RQ2_Refined_Model_LightGBM.csv'\n",
    "        results_df.to_csv(results_filename, index=False, encoding='utf-8')\n",
    "        print(f\"\\n[OK] Results saved: {results_filename}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"[OK] RQ2 Refined Model Comparison completed!\")\n",
    "    print(f\"  Processed {M} MICE datasets\")\n",
    "    print(f\"  Evaluated Refined Model with {len(top_10_features)} features\")\n",
    "    print(f\"  Generated comparison table\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n[ERROR] Cannot proceed:\")\n",
    "    if len(mice_X_datasets) != M or len(mice_Y_datasets) != M:\n",
    "        print(f\"  MICE datasets not loaded: {len(mice_X_datasets)} X, {len(mice_Y_datasets)} Y\")\n",
    "    if len(top_10_features) != 10:\n",
    "        print(f\"  Top 10 features not available: {len(top_10_features)} features found\")\n",
    "        print(f\"  Please run Cell 12 first to generate SHAP importance rankings.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Cell 14: RQ3 - Subgroup Fairness Analysis (LightGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task 1: Environment Verification & Data Loading\n",
      "============================================================\n",
      "[INFO] Case A: Using existing mice_X_datasets and mice_Y_datasets from environment\n",
      "  Found 5 X datasets and 5 Y datasets\n",
      "  Found 5 X dataframes with feature names\n",
      "\n",
      "============================================================\n",
      "Task 2: Define Model and Evaluation Metric\n",
      "============================================================\n",
      "\n",
      "LightGBM Configuration:\n",
      "  n_estimators: 100\n",
      "  max_depth: 15\n",
      "  learning_rate: 0.1\n",
      "  num_leaves: 31\n",
      "  min_child_samples: 20\n",
      "  random_state: 42\n",
      "  n_jobs: 1\n",
      "  verbosity: -1\n",
      "\n",
      "Evaluation Metric:\n",
      "  MAE (Mean Absolute Error) - more interpretable for subgroup comparisons\n",
      "\n",
      "============================================================\n",
      "Task 3: Define Subgroups\n",
      "============================================================\n",
      "\n",
      "Defined 11 subgroups:\n",
      "  Gender: 3 subgroups\n",
      "  Income: 4 subgroups\n",
      "  Education: 4 subgroups\n",
      "\n",
      "============================================================\n",
      "Task 4: Execute Subgroup Cross-Validation\n",
      "============================================================\n",
      "\n",
      "Cross-Validation Setup:\n",
      "  CV: 5-fold KFold\n",
      "  MICE Datasets: 5\n",
      "  Total evaluations: 5 datasets × 5 folds = 25 per subgroup\n",
      "\n",
      "============================================================\n",
      "Computing Subgroup MAE Across MICE Datasets...\n",
      "============================================================\n",
      "\n",
      "Processing MICE Dataset 1/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] M1: Completed 5 CV folds\n",
      "\n",
      "Processing MICE Dataset 2/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] M2: Completed 5 CV folds\n",
      "\n",
      "Processing MICE Dataset 3/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] M3: Completed 5 CV folds\n",
      "\n",
      "Processing MICE Dataset 4/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] M4: Completed 5 CV folds\n",
      "\n",
      "Processing MICE Dataset 5/5...\n",
      "  Dataset shape: X (12939, 22), y (12939,)\n",
      "  [OK] M5: Completed 5 CV folds\n",
      "\n",
      "[OK] Subgroup MAE computed for all 5 MICE datasets\n",
      "\n",
      "============================================================\n",
      "Task 5: Results Pooling and Output\n",
      "============================================================\n",
      "\n",
      "====================================================================================================\n",
      "RQ3: Subgroup Fairness Analysis Results (MAE)\n",
      "====================================================================================================\n",
      "\n",
      "Category     Subgroup                                 MAE          MAE_Std      Diff_from_Baseline   Bias_Rate_Pct   N_Evaluations  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gender       Male (baseline)                               0.1138      0.0073             +0.0000         +0.00%             25\n",
      "Gender       Female (ch23p001_2.0)                         0.3269      0.0234             +0.2130       +187.17%             25\n",
      "Gender       Other (ch23p001_3.0)                          0.2625      0.2932             +0.1487       +130.62%             15\n",
      "Income       Income Quartile 1 (baseline, lowest)          0.1063      0.0081             +0.0000         +0.00%             25\n",
      "Income       Income Quartile 2 (Income_Quartile_2.0)       0.3800      0.0338             +0.2737       +257.58%             25\n",
      "Income       Income Quartile 3 (Income_Quartile_3.0)       0.2647      0.0344             +0.1584       +149.06%             25\n",
      "Income       Income Quartile 4 (Income_Quartile_4.0, highest)      0.4756      0.0329             +0.3693       +347.56%             25\n",
      "Education    Education Level 1 (baseline, lowest)          0.0339      0.0043             +0.0000         +0.00%             25\n",
      "Education    Education Level 2 (Education_Recode_2.0)      0.2997      0.0558             +0.2658       +783.29%             25\n",
      "Education    Education Level 3 (Education_Recode_3.0)      0.4630      0.0187             +0.4291      +1264.72%             25\n",
      "Education    Education Level 4 (Education_Recode_4.0, highest)      0.4485      0.0602             +0.4146      +1221.81%             25\n",
      "\n",
      "====================================================================================================\n",
      "Bias Analysis:\n",
      "====================================================================================================\n",
      "\n",
      "Gender (Baseline MAE: 0.1138):\n",
      "  Female (ch23p001_2.0)                    MAE: 0.3269 (Bias: +187.17%) - overestimated\n",
      "  Other (ch23p001_3.0)                     MAE: 0.2625 (Bias: +130.62%) - overestimated\n",
      "\n",
      "Income (Baseline MAE: 0.1063):\n",
      "  Income Quartile 2 (Income_Quartile_2.0)  MAE: 0.3800 (Bias: +257.58%) - overestimated\n",
      "  Income Quartile 3 (Income_Quartile_3.0)  MAE: 0.2647 (Bias: +149.06%) - overestimated\n",
      "  Income Quartile 4 (Income_Quartile_4.0, highest) MAE: 0.4756 (Bias: +347.56%) - overestimated\n",
      "\n",
      "Education (Baseline MAE: 0.0339):\n",
      "  Education Level 2 (Education_Recode_2.0) MAE: 0.2997 (Bias: +783.29%) - overestimated\n",
      "  Education Level 3 (Education_Recode_3.0) MAE: 0.4630 (Bias: +1264.72%) - overestimated\n",
      "  Education Level 4 (Education_Recode_4.0, highest) MAE: 0.4485 (Bias: +1221.81%) - overestimated\n",
      "\n",
      "[OK] Results saved: RQ3_Subgroup_MAE_LightGBM.csv\n",
      "\n",
      "============================================================\n",
      "[OK] RQ3 Subgroup Fairness Analysis completed!\n",
      "  Processed 5 MICE datasets\n",
      "  Analyzed 11 subgroups\n",
      "  Generated subgroup fairness analysis table\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 14:  RQ3 - Subgroup Fairness Analysis (LightGBM)\n",
    "# RQ3: Subgroup Fairness Analysis using LightGBM\n",
    "print(\"=\"*60)\n",
    "print(\"Cell 14: RQ3 - Subgroup Fairness Analysis (LightGBM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if LightGBM is available\n",
    "try:\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    raise ImportError(\"LightGBM is required but could not be imported.\")\n",
    "\n",
    "# ============================================================================\n",
    "# Task 1: Environment Verification & Data Loading\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Task 1: Environment Verification & Data Loading\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Case A: Check if mice_X_datasets and mice_Y_datasets exist in environment\n",
    "try:\n",
    "    if 'mice_X_datasets' in globals() and 'mice_Y_datasets' in globals() and \\\n",
    "       mice_X_datasets is not None and mice_Y_datasets is not None and \\\n",
    "       len(mice_X_datasets) > 0 and len(mice_Y_datasets) > 0:\n",
    "        print(\"[INFO] Case A: Using existing mice_X_datasets and mice_Y_datasets from environment\")\n",
    "        print(f\"  Found {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n",
    "        M = len(mice_X_datasets)\n",
    "        if 'mice_X_dataframes' not in globals() or mice_X_dataframes is None or len(mice_X_dataframes) == 0:\n",
    "            print(\"[WARNING] mice_X_dataframes not found, will reload to preserve feature names\")\n",
    "            raise NameError(\"mice_X_dataframes missing\")\n",
    "        else:\n",
    "            print(f\"  Found {len(mice_X_dataframes)} X dataframes with feature names\")\n",
    "            data_loaded = True\n",
    "    else:\n",
    "        raise NameError(\"Variables not available\")\n",
    "except (NameError, AttributeError):\n",
    "    # Case B: Reload from CSV files\n",
    "    print(\"[INFO] Case B: Reloading MICE datasets from CSV files\")\n",
    "    data_loaded = False\n",
    "\n",
    "if not data_loaded:\n",
    "    M = 5\n",
    "    mice_X_datasets = []\n",
    "    mice_Y_datasets = []\n",
    "    mice_X_dataframes = []\n",
    "\n",
    "    for m in range(1, M + 1):\n",
    "        X_file = f'LISS_Final_X_M{m}.csv'\n",
    "        Y_file = f'LISS_Final_Y_M{m}.csv'\n",
    "        \n",
    "        try:\n",
    "            X_m = pd.read_csv(X_file, encoding='utf-8', index_col=0)\n",
    "            Y_m = pd.read_csv(Y_file, encoding='utf-8', index_col=0)\n",
    "            \n",
    "            # Align indices\n",
    "            common_idx = X_m.index.intersection(Y_m.index)\n",
    "            X_m = X_m.loc[common_idx]\n",
    "            Y_m = Y_m.loc[common_idx]\n",
    "            \n",
    "            # Check for missing values\n",
    "            X_missing = X_m.isna().sum().sum()\n",
    "            Y_missing = Y_m.isna().sum().sum()\n",
    "            \n",
    "            if X_missing > 0 or Y_missing > 0:\n",
    "                valid_mask = ~(X_m.isna().any(axis=1) | Y_m.isna().any(axis=1))\n",
    "                X_m = X_m[valid_mask]\n",
    "                Y_m = Y_m[valid_mask]\n",
    "            \n",
    "            # Extract target variable\n",
    "            target_col = Y_m.columns[0]\n",
    "            y_m = Y_m[target_col].values\n",
    "            X_m_values = X_m.values\n",
    "            \n",
    "            mice_X_datasets.append(X_m_values)\n",
    "            mice_Y_datasets.append(y_m)\n",
    "            mice_X_dataframes.append(X_m.copy())\n",
    "            \n",
    "            print(f\"[OK] M{m}: X shape {X_m_values.shape}, y shape {y_m.shape}, target: {target_col}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"[ERROR] M{m}: Could not find {X_file} or {Y_file}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] M{m}: Error loading data: {e}\")\n",
    "            break\n",
    "\n",
    "    if len(mice_X_datasets) == M:\n",
    "        print(f\"\\n[OK] Successfully loaded {M} MICE datasets from CSV files\")\n",
    "        print(f\"  Total datasets: {len(mice_X_datasets)}\")\n",
    "        print(f\"  Feature names preserved for subgroup analysis\")\n",
    "    else:\n",
    "        print(f\"\\n[ERROR] Only loaded {len(mice_X_datasets)}/{M} datasets. Cannot proceed.\")\n",
    "        mice_X_datasets = []\n",
    "        mice_Y_datasets = []\n",
    "        mice_X_dataframes = []\n",
    "\n",
    "# Verify data is available\n",
    "if len(mice_X_datasets) == M and len(mice_Y_datasets) == M:\n",
    "    # ============================================================================\n",
    "    # Task 2: Define Model and Evaluation Metric\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Define Model and Evaluation Metric\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define LightGBM Regressor\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLightGBM Configuration:\")\n",
    "    print(f\"  n_estimators: 100\")\n",
    "    print(f\"  max_depth: 15\")\n",
    "    print(f\"  learning_rate: 0.1\")\n",
    "    print(f\"  num_leaves: 31\")\n",
    "    print(f\"  min_child_samples: 20\")\n",
    "    print(f\"  random_state: 42\")\n",
    "    print(f\"  n_jobs: 1\")\n",
    "    print(f\"  verbosity: -1\")\n",
    "    \n",
    "    print(f\"\\nEvaluation Metric:\")\n",
    "    print(f\"  MAE (Mean Absolute Error) - more interpretable for subgroup comparisons\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: Define Subgroups\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Define Subgroups\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = mice_X_dataframes[0].columns.tolist()\n",
    "    \n",
    "    # Define subgroups using lambda functions\n",
    "    subgroups = {}\n",
    "    \n",
    "    # Gender subgroups\n",
    "    gender_subgroups = {\n",
    "        'Gender_Male (Baseline)': {\n",
    "            'condition': lambda df: (df.get('ch23p001_2.0', pd.Series([False]*len(df), index=df.index)) == 0) &\n",
    "                                    (df.get('ch23p001_3.0', pd.Series([False]*len(df), index=df.index)) == 0),\n",
    "            'description': 'Male (baseline)'\n",
    "        },\n",
    "        'Gender_Female': {\n",
    "            'condition': lambda df: df.get('ch23p001_2.0', pd.Series([False]*len(df), index=df.index)) == 1,\n",
    "            'description': 'Female (ch23p001_2.0)'\n",
    "        },\n",
    "        'Gender_Other': {\n",
    "            'condition': lambda df: df.get('ch23p001_3.0', pd.Series([False]*len(df), index=df.index)) == 1,\n",
    "            'description': 'Other (ch23p001_3.0)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Income subgroups\n",
    "    income_subgroups = {\n",
    "        'Income_Q1 (Baseline)': {\n",
    "            'condition': lambda df: (df.get('Income_Quartile_2.0', pd.Series([False]*len(df), index=df.index)) == 0) &\n",
    "                                    (df.get('Income_Quartile_3.0', pd.Series([False]*len(df), index=df.index)) == 0) &\n",
    "                                    (df.get('Income_Quartile_4.0', pd.Series([False]*len(df), index=df.index)) == 0),\n",
    "            'description': 'Income Quartile 1 (baseline, lowest)'\n",
    "        },\n",
    "        'Income_Q2': {\n",
    "            'condition': lambda df: df.get('Income_Quartile_2.0', pd.Series([False]*len(df), index=df.index)) == 1,\n",
    "            'description': 'Income Quartile 2 (Income_Quartile_2.0)'\n",
    "        },\n",
    "        'Income_Q3': {\n",
    "            'condition': lambda df: df.get('Income_Quartile_3.0', pd.Series([False]*len(df), index=df.index)) == 1,\n",
    "            'description': 'Income Quartile 3 (Income_Quartile_3.0)'\n",
    "        },\n",
    "        'Income_Q4': {\n",
    "            'condition': lambda df: df.get('Income_Quartile_4.0', pd.Series([False]*len(df), index=df.index)) == 1,\n",
    "            'description': 'Income Quartile 4 (Income_Quartile_4.0, highest)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Education subgroups\n",
    "    education_subgroups = {\n",
    "        'Education_Low (Baseline)': {\n",
    "            'condition': lambda df: (df.get('Education_Recode_2.0', pd.Series([False]*len(df), index=df.index)) == 0) &\n",
    "                                    (df.get('Education_Recode_3.0', pd.Series([False]*len(df), index=df.index)) == 0) &\n",
    "                                    (df.get('Education_Recode_4.0', pd.Series([False]*len(df), index=df.index)) == 0),\n",
    "            'description': 'Education Level 1 (baseline, lowest)'\n",
    "        },\n",
    "        'Education_Medium': {\n",
    "            'condition': lambda df: df.get('Education_Recode_2.0', pd.Series([False]*len(df), index=df.index)) == 1,\n",
    "            'description': 'Education Level 2 (Education_Recode_2.0)'\n",
    "        },\n",
    "        'Education_High': {\n",
    "            'condition': lambda df: df.get('Education_Recode_3.0', pd.Series([False]*len(df), index=df.index)) == 1,\n",
    "            'description': 'Education Level 3 (Education_Recode_3.0)'\n",
    "        },\n",
    "        'Education_Highest': {\n",
    "            'condition': lambda df: df.get('Education_Recode_4.0', pd.Series([False]*len(df), index=df.index)) == 1,\n",
    "            'description': 'Education Level 4 (Education_Recode_4.0, highest)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Combine all subgroups\n",
    "    subgroups.update(gender_subgroups)\n",
    "    subgroups.update(income_subgroups)\n",
    "    subgroups.update(education_subgroups)\n",
    "    \n",
    "    print(f\"\\nDefined {len(subgroups)} subgroups:\")\n",
    "    print(f\"  Gender: {len(gender_subgroups)} subgroups\")\n",
    "    print(f\"  Income: {len(income_subgroups)} subgroups\")\n",
    "    print(f\"  Education: {len(education_subgroups)} subgroups\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 4: Execute Subgroup Cross-Validation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 4: Execute Subgroup Cross-Validation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 5-Fold Cross-Validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Store MAE for each subgroup across all MICE datasets and CV folds\n",
    "    subgroup_mae = {subgroup_name: [] for subgroup_name in subgroups.keys()}\n",
    "    \n",
    "    print(f\"\\nCross-Validation Setup:\")\n",
    "    print(f\"  CV: {cv.n_splits}-fold KFold\")\n",
    "    print(f\"  MICE Datasets: {M}\")\n",
    "    print(f\"  Total evaluations: {M} datasets × {cv.n_splits} folds = {M * cv.n_splits} per subgroup\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Computing Subgroup MAE Across MICE Datasets...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Main loop: iterate over MICE datasets\n",
    "    for m_idx in range(M):\n",
    "        print(f\"\\nProcessing MICE Dataset {m_idx+1}/{M}...\")\n",
    "        \n",
    "        X = mice_X_datasets[m_idx]\n",
    "        y = mice_Y_datasets[m_idx]\n",
    "        X_df = mice_X_dataframes[m_idx]\n",
    "        \n",
    "        print(f\"  Dataset shape: X {X.shape}, y {y.shape}\")\n",
    "        \n",
    "        # Cross-validation loop\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X), 1):\n",
    "            # Split data\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            X_test_df = X_df.iloc[test_idx]\n",
    "            \n",
    "            # Train model\n",
    "            lgb_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict on test set\n",
    "            y_pred = lgb_model.predict(X_test)\n",
    "            \n",
    "            # Calculate MAE for each subgroup in this test fold\n",
    "            for subgroup_name, subgroup_info in subgroups.items():\n",
    "                # Get subgroup mask for test set\n",
    "                try:\n",
    "                    subgroup_mask = subgroup_info['condition'](X_test_df)\n",
    "                    \n",
    "                    if subgroup_mask.sum() > 0:  # Only calculate if subgroup has samples\n",
    "                        y_test_subgroup = y_test[subgroup_mask]\n",
    "                        y_pred_subgroup = y_pred[subgroup_mask]\n",
    "                        mae = mean_absolute_error(y_test_subgroup, y_pred_subgroup)\n",
    "                        subgroup_mae[subgroup_name].append(mae)\n",
    "                except Exception as e:\n",
    "                    # If subgroup condition fails, skip\n",
    "                    pass\n",
    "        \n",
    "        print(f\"  [OK] M{m_idx+1}: Completed {cv.n_splits} CV folds\")\n",
    "    \n",
    "    print(f\"\\n[OK] Subgroup MAE computed for all {M} MICE datasets\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 5: Results Pooling and Output\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 5: Results Pooling and Output\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate pooled MAE (average across all MICE datasets and CV folds)\n",
    "    pooled_mae = {}\n",
    "    pooled_mae_std = {}\n",
    "    pooled_mae_count = {}\n",
    "    \n",
    "    for subgroup_name in subgroups.keys():\n",
    "        if len(subgroup_mae[subgroup_name]) > 0:\n",
    "            pooled_mae[subgroup_name] = np.mean(subgroup_mae[subgroup_name])\n",
    "            pooled_mae_std[subgroup_name] = np.std(subgroup_mae[subgroup_name])\n",
    "            pooled_mae_count[subgroup_name] = len(subgroup_mae[subgroup_name])\n",
    "        else:\n",
    "            pooled_mae[subgroup_name] = np.nan\n",
    "            pooled_mae_std[subgroup_name] = np.nan\n",
    "            pooled_mae_count[subgroup_name] = 0\n",
    "    \n",
    "    # Create results table\n",
    "    results_data = []\n",
    "    \n",
    "    # Gender comparison\n",
    "    gender_baseline = 'Gender_Male (Baseline)'\n",
    "    if gender_baseline in pooled_mae and not np.isnan(pooled_mae[gender_baseline]):\n",
    "        baseline_mae_gender = pooled_mae[gender_baseline]\n",
    "        for subgroup_name in gender_subgroups.keys():\n",
    "            if subgroup_name in pooled_mae:\n",
    "                mae = pooled_mae[subgroup_name]\n",
    "                mae_std = pooled_mae_std[subgroup_name]\n",
    "                count = pooled_mae_count[subgroup_name]\n",
    "                if not np.isnan(mae):\n",
    "                    diff_from_baseline = mae - baseline_mae_gender\n",
    "                    bias_rate = (diff_from_baseline / baseline_mae_gender) * 100 if baseline_mae_gender > 0 else 0\n",
    "                    results_data.append({\n",
    "                        'Category': 'Gender',\n",
    "                        'Subgroup': subgroups[subgroup_name]['description'],\n",
    "                        'MAE': mae,\n",
    "                        'MAE_Std': mae_std,\n",
    "                        'N_Evaluations': count,\n",
    "                        'Diff_from_Baseline': diff_from_baseline,\n",
    "                        'Bias_Rate_Pct': bias_rate,\n",
    "                        'Baseline_MAE': baseline_mae_gender\n",
    "                    })\n",
    "    \n",
    "    # Income comparison\n",
    "    income_baseline = 'Income_Q1 (Baseline)'\n",
    "    if income_baseline in pooled_mae and not np.isnan(pooled_mae[income_baseline]):\n",
    "        baseline_mae_income = pooled_mae[income_baseline]\n",
    "        for subgroup_name in income_subgroups.keys():\n",
    "            if subgroup_name in pooled_mae:\n",
    "                mae = pooled_mae[subgroup_name]\n",
    "                mae_std = pooled_mae_std[subgroup_name]\n",
    "                count = pooled_mae_count[subgroup_name]\n",
    "                if not np.isnan(mae):\n",
    "                    diff_from_baseline = mae - baseline_mae_income\n",
    "                    bias_rate = (diff_from_baseline / baseline_mae_income) * 100 if baseline_mae_income > 0 else 0\n",
    "                    results_data.append({\n",
    "                        'Category': 'Income',\n",
    "                        'Subgroup': subgroups[subgroup_name]['description'],\n",
    "                        'MAE': mae,\n",
    "                        'MAE_Std': mae_std,\n",
    "                        'N_Evaluations': count,\n",
    "                        'Diff_from_Baseline': diff_from_baseline,\n",
    "                        'Bias_Rate_Pct': bias_rate,\n",
    "                        'Baseline_MAE': baseline_mae_income\n",
    "                    })\n",
    "    \n",
    "    # Education comparison\n",
    "    education_baseline = 'Education_Low (Baseline)'\n",
    "    if education_baseline in pooled_mae and not np.isnan(pooled_mae[education_baseline]):\n",
    "        baseline_mae_education = pooled_mae[education_baseline]\n",
    "        for subgroup_name in education_subgroups.keys():\n",
    "            if subgroup_name in pooled_mae:\n",
    "                mae = pooled_mae[subgroup_name]\n",
    "                mae_std = pooled_mae_std[subgroup_name]\n",
    "                count = pooled_mae_count[subgroup_name]\n",
    "                if not np.isnan(mae):\n",
    "                    diff_from_baseline = mae - baseline_mae_education\n",
    "                    bias_rate = (diff_from_baseline / baseline_mae_education) * 100 if baseline_mae_education > 0 else 0\n",
    "                    results_data.append({\n",
    "                        'Category': 'Education',\n",
    "                        'Subgroup': subgroups[subgroup_name]['description'],\n",
    "                        'MAE': mae,\n",
    "                        'MAE_Std': mae_std,\n",
    "                        'N_Evaluations': count,\n",
    "                        'Diff_from_Baseline': diff_from_baseline,\n",
    "                        'Bias_Rate_Pct': bias_rate,\n",
    "                        'Baseline_MAE': baseline_mae_education\n",
    "                    })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Print results table\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"RQ3: Subgroup Fairness Analysis Results (MAE)\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Category':<12} {'Subgroup':<40} {'MAE':<12} {'MAE_Std':<12} {'Diff_from_Baseline':<20} {'Bias_Rate_Pct':<15} {'N_Evaluations':<15}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        diff_str = f\"{row['Diff_from_Baseline']:+.4f}\" if not np.isnan(row['Diff_from_Baseline']) else \"N/A\"\n",
    "        bias_str = f\"{row['Bias_Rate_Pct']:+.2f}%\" if not np.isnan(row['Bias_Rate_Pct']) else \"N/A\"\n",
    "        print(f\"{row['Category']:<12} {row['Subgroup']:<40} {row['MAE']:>11.4f} {row['MAE_Std']:>11.4f} {diff_str:>19} {bias_str:>14} {row['N_Evaluations']:>14}\")\n",
    "    \n",
    "    # Identify significant biases\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Bias Analysis:\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    significant_bias_threshold = 5.0  # 5% difference threshold\n",
    "    \n",
    "    for category in ['Gender', 'Income', 'Education']:\n",
    "        category_results = results_df[results_df['Category'] == category]\n",
    "        if len(category_results) > 0:\n",
    "            baseline_row = category_results[category_results['Subgroup'].str.contains('baseline', case=False)]\n",
    "            if len(baseline_row) > 0:\n",
    "                baseline_mae = baseline_row.iloc[0]['MAE']\n",
    "                print(f\"\\n{category} (Baseline MAE: {baseline_mae:.4f}):\")\n",
    "                \n",
    "                for idx, row in category_results.iterrows():\n",
    "                    if 'baseline' not in row['Subgroup'].lower():\n",
    "                        bias_pct = row['Bias_Rate_Pct']\n",
    "                        if not np.isnan(bias_pct) and abs(bias_pct) > significant_bias_threshold:\n",
    "                            direction = \"overestimated\" if row['Diff_from_Baseline'] > 0 else \"underestimated\"\n",
    "                            print(f\"  {row['Subgroup']:<40} MAE: {row['MAE']:.4f} (Bias: {bias_pct:+.2f}%) - {direction}\")\n",
    "                        elif not np.isnan(bias_pct):\n",
    "                            print(f\"  {row['Subgroup']:<40} MAE: {row['MAE']:.4f} (Bias: {bias_pct:+.2f}%) - similar to baseline\")\n",
    "    \n",
    "    # Save results\n",
    "    results_filename = 'RQ3_Subgroup_MAE_LightGBM.csv'\n",
    "    results_df.to_csv(results_filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\n[OK] Results saved: {results_filename}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"[OK] RQ3 Subgroup Fairness Analysis completed!\")\n",
    "    print(f\"  Processed {M} MICE datasets\")\n",
    "    print(f\"  Analyzed {len(subgroups)} subgroups\")\n",
    "    print(f\"  Generated subgroup fairness analysis table\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n[ERROR] Cannot proceed: MICE datasets not loaded successfully.\")\n",
    "    print(f\"  Expected {M} datasets, but loaded {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Cell 15: Visualization - Prediction Performance & SHAP (LightGBM)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Cell 15:  Visualization - Prediction Performance & SHAP (LightGBM)\n",
    "# Visualization: Prediction Performance & SHAP using LightGBM\n",
    "print(\"=\"*60)\n",
    "print(\"Cell 15: Visualization - Prediction Performance & SHAP (LightGBM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if LightGBM is available\n",
    "try:\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    raise ImportError(\"LightGBM is required but could not be imported.\")\n",
    "\n",
    "# Set style (using matplotlib instead of seaborn)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "        # Manually set whitegrid-like style\n",
    "        plt.rcParams['axes.grid'] = True\n",
    "        plt.rcParams['grid.alpha'] = 0.3\n",
    "        plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# ============================================================================\n",
    "# Task 1: Environment Verification & Data Loading\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Task 1: Environment Verification & Data Loading\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Case A: Check if mice_X_datasets exist in environment\n",
    "try:\n",
    "    if 'mice_X_datasets' in globals() and 'mice_Y_datasets' in globals() and \\\n",
    "       mice_X_datasets is not None and mice_Y_datasets is not None and \\\n",
    "       len(mice_X_datasets) > 0 and len(mice_Y_datasets) > 0:\n",
    "        print(\"[INFO] Case A: Using existing mice_X_datasets and mice_Y_datasets from environment\")\n",
    "        print(f\"  Found {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n",
    "        M = len(mice_X_datasets)\n",
    "        if 'mice_X_dataframes' not in globals() or mice_X_dataframes is None or len(mice_X_dataframes) == 0:\n",
    "            print(\"[WARNING] mice_X_dataframes not found, will reload to preserve feature names\")\n",
    "            raise NameError(\"mice_X_dataframes missing\")\n",
    "        else:\n",
    "            print(f\"  Found {len(mice_X_dataframes)} X dataframes with feature names\")\n",
    "            data_loaded = True\n",
    "    else:\n",
    "        raise NameError(\"Variables not available\")\n",
    "except (NameError, AttributeError):\n",
    "    # Case B: Reload from CSV files\n",
    "    print(\"[INFO] Case B: Reloading MICE datasets from CSV files\")\n",
    "    data_loaded = False\n",
    "\n",
    "if not data_loaded:\n",
    "    M = 5\n",
    "    mice_X_datasets = []\n",
    "    mice_Y_datasets = []\n",
    "    mice_X_dataframes = []\n",
    "\n",
    "    for m in range(1, M + 1):\n",
    "        X_file = f'LISS_Final_X_M{m}.csv'\n",
    "        Y_file = f'LISS_Final_Y_M{m}.csv'\n",
    "        \n",
    "        try:\n",
    "            X_m = pd.read_csv(X_file, encoding='utf-8', index_col=0)\n",
    "            Y_m = pd.read_csv(Y_file, encoding='utf-8', index_col=0)\n",
    "            \n",
    "            # Align indices\n",
    "            common_idx = X_m.index.intersection(Y_m.index)\n",
    "            X_m = X_m.loc[common_idx]\n",
    "            Y_m = Y_m.loc[common_idx]\n",
    "            \n",
    "            # Check for missing values\n",
    "            X_missing = X_m.isna().sum().sum()\n",
    "            Y_missing = Y_m.isna().sum().sum()\n",
    "            \n",
    "            if X_missing > 0 or Y_missing > 0:\n",
    "                valid_mask = ~(X_m.isna().any(axis=1) | Y_m.isna().any(axis=1))\n",
    "                X_m = X_m[valid_mask]\n",
    "                Y_m = Y_m[valid_mask]\n",
    "            \n",
    "            # Extract target variable\n",
    "            target_col = Y_m.columns[0]\n",
    "            y_m = Y_m[target_col].values\n",
    "            X_m_values = X_m.values\n",
    "            \n",
    "            mice_X_datasets.append(X_m_values)\n",
    "            mice_Y_datasets.append(y_m)\n",
    "            mice_X_dataframes.append(X_m.copy())\n",
    "            \n",
    "            print(f\"[OK] M{m}: X shape {X_m_values.shape}, y shape {y_m.shape}, target: {target_col}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"[ERROR] M{m}: Could not find {X_file} or {Y_file}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] M{m}: Error loading data: {e}\")\n",
    "            break\n",
    "\n",
    "    if len(mice_X_datasets) == M:\n",
    "        print(f\"\\n[OK] Successfully loaded {M} MICE datasets from CSV files\")\n",
    "    else:\n",
    "        print(f\"\\n[ERROR] Only loaded {len(mice_X_datasets)}/{M} datasets. Cannot proceed.\")\n",
    "        mice_X_datasets = []\n",
    "        mice_Y_datasets = []\n",
    "        mice_X_dataframes = []\n",
    "\n",
    "# Verify data is available\n",
    "if len(mice_X_datasets) == M and len(mice_Y_datasets) == M:\n",
    "    # ============================================================================\n",
    "    # Task 2: Generate Pooled Predictions\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Generate Pooled Predictions\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define LightGBM Regressor\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1\n",
    "    )\n",
    "    \n",
    "    # 5-Fold Cross-Validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Store all actual and predicted values across all MICE datasets\n",
    "    pooled_actual = []\n",
    "    pooled_predicted = []\n",
    "    pooled_train_y = []  # For distribution check\n",
    "    pooled_test_y = []   # For distribution check\n",
    "    \n",
    "    print(f\"\\nCross-Validation Setup:\")\n",
    "    print(f\"  CV: {cv.n_splits}-fold KFold\")\n",
    "    print(f\"  MICE Datasets: {M}\")\n",
    "    print(f\"  Total predictions: {M} datasets × {cv.n_splits} folds\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Generating Predictions Across MICE Datasets...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Main loop: iterate over MICE datasets\n",
    "    for m_idx in range(M):\n",
    "        print(f\"\\nProcessing MICE Dataset {m_idx+1}/{M}...\")\n",
    "        \n",
    "        X = mice_X_datasets[m_idx]\n",
    "        y = mice_Y_datasets[m_idx]\n",
    "        \n",
    "        print(f\"  Dataset shape: X {X.shape}, y {y.shape}\")\n",
    "        \n",
    "        # Use cross_val_predict to get predictions\n",
    "        y_pred = cross_val_predict(lgb_model, X, y, cv=cv, n_jobs=1)\n",
    "        \n",
    "        # Collect actual and predicted values\n",
    "        pooled_actual.extend(y)\n",
    "        pooled_predicted.extend(y_pred)\n",
    "        \n",
    "        # Collect train/test distributions (approximate)\n",
    "        for train_idx, test_idx in cv.split(X):\n",
    "            pooled_train_y.extend(y[train_idx])\n",
    "            pooled_test_y.extend(y[test_idx])\n",
    "        \n",
    "        # Calculate metrics for this dataset\n",
    "        rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        print(f\"  M{m_idx+1} RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\n[OK] Predictions generated for all {M} MICE datasets\")\n",
    "    print(f\"  Total predictions: {len(pooled_actual):,}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: Actual vs Predicted Plot\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Actual vs Predicted Plot\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Filter data for visualization (limit to [-10, 10])\n",
    "    valid_range = [-10, 10]\n",
    "    valid_mask = (np.array(pooled_actual) >= valid_range[0]) & (np.array(pooled_actual) <= valid_range[1]) & \\\n",
    "                 (np.array(pooled_predicted) >= valid_range[0]) & (np.array(pooled_predicted) <= valid_range[1])\n",
    "    \n",
    "    actual_plot = np.array(pooled_actual)[valid_mask]\n",
    "    predicted_plot = np.array(pooled_predicted)[valid_mask]\n",
    "    n_outliers = len(pooled_actual) - len(actual_plot)\n",
    "    \n",
    "    print(f\"  Points within range [{valid_range[0]}, {valid_range[1]}]: {len(actual_plot):,}\")\n",
    "    print(f\"  Outliers removed: {n_outliers:,}\")\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.scatter(actual_plot, predicted_plot, alpha=0.3, s=20, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Perfect fit line (y = x)\n",
    "    ax.plot([valid_range[0], valid_range[1]], [valid_range[0], valid_range[1]], \n",
    "           'r--', linewidth=2, label='Perfect Fit (y=x)', zorder=1)\n",
    "    \n",
    "    # Calculate and plot regression line\n",
    "    z = np.polyfit(actual_plot, predicted_plot, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(valid_range[0], valid_range[1], 100)\n",
    "    ax.plot(x_line, p(x_line), 'b-', linewidth=2, label=f'Regression (slope={z[0]:.3f})', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlim(valid_range)\n",
    "    ax.set_ylim(valid_range)\n",
    "    ax.set_xlabel('Actual Δ JobSatisfaction', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Predicted Δ JobSatisfaction', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Actual vs Predicted Δ JobSatisfaction\\nOutliers removed for visualization clarity', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper left', fontsize=12)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig1_filename = 'Pooled_Actual_vs_Predicted_LightGBM.png'\n",
    "    plt.savefig(fig1_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  [OK] Saved: {fig1_filename}\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 4: Residual Plot\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 4: Residual Plot\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals_plot = actual_plot - predicted_plot\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.scatter(predicted_plot, residuals_plot, alpha=0.3, s=20, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Zero residual line\n",
    "    ax.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Zero Residual', zorder=1)\n",
    "    \n",
    "    ax.set_xlim(valid_range)\n",
    "    ax.set_ylim(valid_range)\n",
    "    ax.set_xlabel('Predicted Δ JobSatisfaction', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Residuals (Actual - Predicted)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Residual Plot\\nOutliers removed for visualization clarity', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper right', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig2_filename = 'Pooled_Residual_Plot_LightGBM.png'\n",
    "    plt.savefig(fig2_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  [OK] Saved: {fig2_filename}\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 5: SHAP Summary Plot\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 5: SHAP Summary Plot\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train model on first MICE dataset for SHAP\n",
    "    X_shap = mice_X_datasets[0]\n",
    "    y_shap = mice_Y_datasets[0]\n",
    "    X_shap_df = mice_X_dataframes[0]\n",
    "    feature_names = X_shap_df.columns.tolist()\n",
    "    \n",
    "    print(f\"  Training model on M1 for SHAP calculation...\")\n",
    "    lgb_model.fit(X_shap, y_shap)\n",
    "    print(f\"  [OK] Model trained\")\n",
    "    \n",
    "    # Calculate SHAP values (sample for efficiency)\n",
    "    print(f\"  Computing SHAP values (sampling 1000 instances for efficiency)...\")\n",
    "    sample_size = min(1000, len(X_shap))\n",
    "    sample_idx = np.random.choice(len(X_shap), sample_size, replace=False)\n",
    "    X_shap_sample = X_shap[sample_idx]\n",
    "    X_shap_df_sample = X_shap_df.iloc[sample_idx]\n",
    "    \n",
    "    explainer = shap.TreeExplainer(lgb_model)\n",
    "    shap_values = explainer.shap_values(X_shap_sample)\n",
    "    \n",
    "    print(f\"  [OK] SHAP values computed: shape {shap_values.shape}\")\n",
    "    \n",
    "    # Create SHAP Summary Plot\n",
    "    print(f\"  Generating SHAP Summary Plot...\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, X_shap_df_sample, feature_names=feature_names, \n",
    "                     show=False, plot_size=(12, 10))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig3_filename = 'SHAP_Summary_Plot_LightGBM.png'\n",
    "    plt.savefig(fig3_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  [OK] Saved: {fig3_filename}\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 6: SHAP Dependence Plot (Top 1 Feature)\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 6: SHAP Dependence Plot (Top 1 Feature)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get top 1 feature from SHAP importance\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "    top_feature_idx = np.argmax(mean_abs_shap)\n",
    "    top_feature_name = feature_names[top_feature_idx]\n",
    "    \n",
    "    print(f\"  Top 1 Feature: {top_feature_name} (Mean Abs SHAP: {mean_abs_shap[top_feature_idx]:.6f})\")\n",
    "    \n",
    "    # Create SHAP Dependence Plot\n",
    "    print(f\"  Generating SHAP Dependence Plot...\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    shap.dependence_plot(\n",
    "        top_feature_idx,\n",
    "        shap_values,\n",
    "        X_shap_df_sample,\n",
    "        feature_names=feature_names,\n",
    "        show=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig4_filename = f'SHAP_Dependence_{top_feature_name}_LightGBM.png'\n",
    "    plt.savefig(fig4_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  [OK] Saved: {fig4_filename}\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 7: Target Distribution Check (Train vs Test KDE)\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 7: Target Distribution Check (Train vs Test KDE)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create KDE plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot train and test distributions\n",
    "    from scipy import stats\n",
    "    \n",
    "    train_y_array = np.array(pooled_train_y)\n",
    "    test_y_array = np.array(pooled_test_y)\n",
    "    \n",
    "    # Filter to valid range\n",
    "    train_y_valid = train_y_array[(train_y_array >= -10) & (train_y_array <= 10)]\n",
    "    test_y_valid = test_y_array[(test_y_array >= -10) & (test_y_array <= 10)]\n",
    "    \n",
    "    # KDE\n",
    "    train_kde = stats.gaussian_kde(train_y_valid)\n",
    "    test_kde = stats.gaussian_kde(test_y_valid)\n",
    "    \n",
    "    x_range = np.linspace(-10, 10, 200)\n",
    "    train_density = train_kde(x_range)\n",
    "    test_density = test_kde(x_range)\n",
    "    \n",
    "    ax.plot(x_range, train_density, 'b-', linewidth=2, label='Train Set', alpha=0.7)\n",
    "    ax.fill_between(x_range, train_density, alpha=0.3, color='blue')\n",
    "    \n",
    "    ax.plot(x_range, test_density, 'r-', linewidth=2, label='Test Set', alpha=0.7)\n",
    "    ax.fill_between(x_range, test_density, alpha=0.3, color='red')\n",
    "    \n",
    "    ax.set_xlabel('Δ JobSatisfaction', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Density', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Target Distribution: Train vs Test (KDE)', fontsize=16, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper right', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig5_filename = 'Target_Distribution_Train_vs_Test_LightGBM.png'\n",
    "    plt.savefig(fig5_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  [OK] Saved: {fig5_filename}\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 8: Output Summary\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 8: Output Summary\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_rmse = np.sqrt(mean_squared_error(pooled_actual, pooled_predicted))\n",
    "    overall_r2 = r2_score(pooled_actual, pooled_predicted)\n",
    "    \n",
    "    print(f\"\\nOverall Model Performance (Pooled across {M} MICE datasets):\")\n",
    "    print(f\"  RMSE: {overall_rmse:.4f}\")\n",
    "    print(f\"  R²: {overall_r2:.4f}\")\n",
    "    print(f\"  Total predictions: {len(pooled_actual):,}\")\n",
    "    \n",
    "    print(f\"\\nGenerated Visualization Files:\")\n",
    "    files = [\n",
    "        ('Pooled_Actual_vs_Predicted_LightGBM.png', 'Actual vs Predicted Scatter Plot'),\n",
    "        ('Pooled_Residual_Plot_LightGBM.png', 'Residual Plot'),\n",
    "        ('SHAP_Summary_Plot_LightGBM.png', 'SHAP Summary Plot'),\n",
    "        (f'SHAP_Dependence_{top_feature_name}_LightGBM.png', f'SHAP Dependence Plot ({top_feature_name})'),\n",
    "        ('Target_Distribution_Train_vs_Test_LightGBM.png', 'Target Distribution Check (Train vs Test)')\n",
    "    ]\n",
    "    \n",
    "    import os\n",
    "    for filename, description in files:\n",
    "        if os.path.exists(filename):\n",
    "            file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "            print(f\"  {filename}\")\n",
    "            print(f\"    Description: {description}\")\n",
    "            print(f\"    Size: {file_size:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"[OK] Visualization completed!\")\n",
    "    print(f\"  Processed {M} MICE datasets\")\n",
    "    print(f\"  Generated 5 visualization figures\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n[ERROR] Cannot proceed: MICE datasets not loaded successfully.\")\n",
    "    print(f\"  Expected {M} datasets, but loaded {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16:  Appendix - Correlation Matrix Analysis\n",
    "# Appendix: Correlation Matrix Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"Cell 16: Appendix - Correlation Matrix Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style (using matplotlib instead of seaborn)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "        # Manually set whitegrid-like style\n",
    "        plt.rcParams['axes.grid'] = True\n",
    "        plt.rcParams['grid.alpha'] = 0.3\n",
    "        plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = (14, 12)\n",
    "\n",
    "# ============================================================================\n",
    "# Task 1: Load MICE Datasets\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Task 1: Load MICE Datasets\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Case A: Check if mice_X_datasets exist in environment\n",
    "try:\n",
    "    if 'mice_X_datasets' in globals() and 'mice_Y_datasets' in globals() and \\\n",
    "       mice_X_datasets is not None and mice_Y_datasets is not None and \\\n",
    "       len(mice_X_datasets) > 0 and len(mice_Y_datasets) > 0:\n",
    "        print(\"[INFO] Case A: Using existing mice_X_datasets and mice_Y_datasets from environment\")\n",
    "        print(f\"  Found {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n",
    "        M = len(mice_X_datasets)\n",
    "        if 'mice_X_dataframes' not in globals() or mice_X_dataframes is None or len(mice_X_dataframes) == 0:\n",
    "            print(\"[WARNING] mice_X_dataframes not found, will reload to preserve feature names\")\n",
    "            raise NameError(\"mice_X_dataframes missing\")\n",
    "        else:\n",
    "            print(f\"  Found {len(mice_X_dataframes)} X dataframes with feature names\")\n",
    "            data_loaded = True\n",
    "    else:\n",
    "        raise NameError(\"Variables not available\")\n",
    "except (NameError, AttributeError):\n",
    "    # Case B: Reload from CSV files\n",
    "    print(\"[INFO] Case B: Reloading MICE datasets from CSV files\")\n",
    "    data_loaded = False\n",
    "\n",
    "if not data_loaded:\n",
    "    M = 5\n",
    "    mice_X_datasets = []\n",
    "    mice_Y_datasets = []\n",
    "    mice_X_dataframes = []\n",
    "\n",
    "    for m in range(1, M + 1):\n",
    "        X_file = f'LISS_Final_X_M{m}.csv'\n",
    "        Y_file = f'LISS_Final_Y_M{m}.csv'\n",
    "        \n",
    "        try:\n",
    "            X_m = pd.read_csv(X_file, encoding='utf-8', index_col=0)\n",
    "            Y_m = pd.read_csv(Y_file, encoding='utf-8', index_col=0)\n",
    "            \n",
    "            # Align indices\n",
    "            common_idx = X_m.index.intersection(Y_m.index)\n",
    "            X_m = X_m.loc[common_idx]\n",
    "            Y_m = Y_m.loc[common_idx]\n",
    "            \n",
    "            # Check for missing values\n",
    "            X_missing = X_m.isna().sum().sum()\n",
    "            Y_missing = Y_m.isna().sum().sum()\n",
    "            \n",
    "            if X_missing > 0 or Y_missing > 0:\n",
    "                valid_mask = ~(X_m.isna().any(axis=1) | Y_m.isna().any(axis=1))\n",
    "                X_m = X_m[valid_mask]\n",
    "                Y_m = Y_m[valid_mask]\n",
    "            \n",
    "            # Extract target variable\n",
    "            target_col = Y_m.columns[0]\n",
    "            y_m = Y_m[target_col].values\n",
    "            X_m_values = X_m.values\n",
    "            \n",
    "            mice_X_datasets.append(X_m_values)\n",
    "            mice_Y_datasets.append(y_m)\n",
    "            mice_X_dataframes.append(X_m.copy())\n",
    "            \n",
    "            print(f\"[OK] M{m}: X shape {X_m_values.shape}, y shape {y_m.shape}, target: {target_col}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"[ERROR] M{m}: Could not find {X_file} or {Y_file}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] M{m}: Error loading data: {e}\")\n",
    "            break\n",
    "\n",
    "    if len(mice_X_datasets) == M:\n",
    "        print(f\"\\n[OK] Successfully loaded {M} MICE datasets from CSV files\")\n",
    "    else:\n",
    "        print(f\"\\n[ERROR] Only loaded {len(mice_X_datasets)}/{M} datasets. Cannot proceed.\")\n",
    "        mice_X_datasets = []\n",
    "        mice_Y_datasets = []\n",
    "        mice_X_dataframes = []\n",
    "\n",
    "# Verify data is available\n",
    "if len(mice_X_datasets) == M and len(mice_Y_datasets) == M:\n",
    "    # ============================================================================\n",
    "    # Task 2: Select Continuous Variables\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 2: Select Continuous Variables\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = mice_X_dataframes[0].columns.tolist()\n",
    "    \n",
    "    # Identify continuous variables (exclude OHE dummy variables)\n",
    "    # Continuous variables: Big Five scores, Delta variables, baseline controls\n",
    "    continuous_features = []\n",
    "    \n",
    "    # Big Five scores\n",
    "    big_five = ['N_Score', 'E_Score', 'C_Score', 'A_Score', 'O_Score']\n",
    "    continuous_features.extend([f for f in big_five if f in feature_names])\n",
    "    \n",
    "    # Delta variables\n",
    "    delta_vars = ['Delta_Autonomy', 'Delta_Workload', 'Delta_Skills', 'Delta_RemoteHours', \n",
    "                   'Delta_WFC_Proxy', 'Delta_Health_Hindrance']\n",
    "    continuous_features.extend([f for f in delta_vars if f in feature_names])\n",
    "    \n",
    "    # Baseline controls\n",
    "    baseline_controls = ['cw23p003', 'cf23p455', 'ch23p004']  # Age, Children Count, Self-rated Health\n",
    "    continuous_features.extend([f for f in baseline_controls if f in feature_names])\n",
    "    \n",
    "    # Target variable\n",
    "    target_name = 'Delta_JobSatisfaction'\n",
    "    \n",
    "    print(f\"\\nSelected Continuous Variables:\")\n",
    "    print(f\"  Big Five Scores: {len([f for f in big_five if f in continuous_features])}/{len(big_five)}\")\n",
    "    print(f\"  Delta Variables: {len([f for f in delta_vars if f in continuous_features])}/{len(delta_vars)}\")\n",
    "    print(f\"  Baseline Controls: {len([f for f in baseline_controls if f in continuous_features])}/{len(baseline_controls)}\")\n",
    "    print(f\"  Total continuous features: {len(continuous_features)}\")\n",
    "    print(f\"  Target variable: {target_name}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Task 3: Calculate Pooled Pearson Correlation\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Task 3: Calculate Pooled Pearson Correlation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Store correlation matrices from each MICE dataset\n",
    "    correlation_matrices = []\n",
    "    \n",
    "    for m_idx in range(M):\n",
    "        print(f\"\\nProcessing MICE Dataset {m_idx+1}/{M}...\")\n",
    "        \n",
    "        X_df = mice_X_dataframes[m_idx]\n",
    "        y = mice_Y_datasets[m_idx]\n",
    "        \n",
    "        # Select continuous features\n",
    "        available_continuous = [f for f in continuous_features if f in X_df.columns]\n",
    "        \n",
    "        if len(available_continuous) == 0:\n",
    "            print(f\"  [WARNING] No continuous features available, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Create DataFrame with continuous features and target\n",
    "        data_for_corr = X_df[available_continuous].copy()\n",
    "        data_for_corr[target_name] = y\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = data_for_corr.corr(method='pearson')\n",
    "        correlation_matrices.append(corr_matrix)\n",
    "        \n",
    "        print(f\"  [OK] Correlation matrix computed: shape {corr_matrix.shape}\")\n",
    "    \n",
    "    # Pool correlation matrices (average across MICE datasets)\n",
    "    if len(correlation_matrices) > 0:\n",
    "        # Align all correlation matrices to have the same columns/rows\n",
    "        all_features = set()\n",
    "        for corr_mat in correlation_matrices:\n",
    "            all_features.update(corr_mat.columns)\n",
    "        \n",
    "        all_features = sorted(list(all_features))\n",
    "        \n",
    "        # Reindex all matrices to have the same structure\n",
    "        aligned_matrices = []\n",
    "        for corr_mat in correlation_matrices:\n",
    "            aligned = corr_mat.reindex(index=all_features, columns=all_features)\n",
    "            aligned_matrices.append(aligned)\n",
    "        \n",
    "        # Average across MICE datasets\n",
    "        pooled_corr = pd.DataFrame(np.mean([m.values for m in aligned_matrices], axis=0),\n",
    "                                   index=all_features, columns=all_features)\n",
    "        \n",
    "        print(f\"\\n[OK] Pooled correlation matrix computed: shape {pooled_corr.shape}\")\n",
    "        print(f\"  Features: {len(all_features)}\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Task 4: Visualize Correlation Matrix\n",
    "        # ============================================================================\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Task 4: Visualize Correlation Matrix\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig, ax = plt.subplots(figsize=(14, 12))\n",
    "        \n",
    "        # Create mask for upper triangle (optional, to show only lower triangle)\n",
    "        mask = np.triu(np.ones_like(pooled_corr, dtype=bool), k=1)\n",
    "        \n",
    "        # Apply mask to correlation matrix (set upper triangle to NaN)\n",
    "        corr_masked = pooled_corr.copy()\n",
    "        corr_masked[mask] = np.nan\n",
    "        \n",
    "        # Plot heatmap using matplotlib imshow\n",
    "        im = ax.imshow(corr_masked, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1, \n",
    "                      interpolation='nearest')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "        cbar.set_label('Correlation', rotation=270, labelpad=20)\n",
    "        \n",
    "        # Add annotations\n",
    "        for i in range(len(corr_masked.index)):\n",
    "            for j in range(len(corr_masked.columns)):\n",
    "                if not mask[i, j]:  # Only annotate lower triangle\n",
    "                    value = corr_masked.iloc[i, j]\n",
    "                    if not np.isnan(value):\n",
    "                        text = ax.text(j, i, f'{value:.2f}',\n",
    "                                     ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "        \n",
    "        # Set ticks and labels\n",
    "        ax.set_xticks(np.arange(len(corr_masked.columns)))\n",
    "        ax.set_yticks(np.arange(len(corr_masked.index)))\n",
    "        ax.set_xticklabels(corr_masked.columns, rotation=45, ha='right', fontsize=10)\n",
    "        ax.set_yticklabels(corr_masked.index, fontsize=10)\n",
    "        \n",
    "        # Add grid lines\n",
    "        ax.set_xticks(np.arange(len(corr_masked.columns)) - 0.5, minor=True)\n",
    "        ax.set_yticks(np.arange(len(corr_masked.index)) - 0.5, minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"gray\", linestyle='-', linewidth=0.5)\n",
    "        \n",
    "        ax.set_title('Pooled Pearson Correlation Matrix\\n(Continuous Variables & Target)', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig_filename = 'Correlation_Matrix_LightGBM.png'\n",
    "        plt.savefig(fig_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  [OK] Saved: {fig_filename}\")\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Task 5: Extract and Report Key Correlations\n",
    "        # ============================================================================\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Task 5: Extract and Report Key Correlations\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get correlations with target variable\n",
    "        if target_name in pooled_corr.columns:\n",
    "            target_correlations = pooled_corr[target_name].drop(target_name).sort_values(ascending=False, key=abs)\n",
    "            \n",
    "            print(f\"\\nTop 10 Features Most Correlated with {target_name}:\")\n",
    "            print(\"-\"*60)\n",
    "            print(f\"{'Feature':<35} {'Correlation':<15}\")\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            for feat_name, corr_value in target_correlations.head(10).items():\n",
    "                print(f\"{feat_name:<35} {corr_value:>14.4f}\")\n",
    "            \n",
    "            # Save correlation with target\n",
    "            target_corr_df = pd.DataFrame({\n",
    "                'Feature': target_correlations.index,\n",
    "                'Correlation_with_Target': target_correlations.values\n",
    "            })\n",
    "            target_corr_df = target_corr_df.sort_values('Correlation_with_Target', ascending=False, key=lambda x: x.abs())\n",
    "            \n",
    "            target_corr_filename = 'Correlation_with_Target_LightGBM.csv'\n",
    "            target_corr_df.to_csv(target_corr_filename, index=False, encoding='utf-8')\n",
    "            print(f\"\\n[OK] Correlations with target saved: {target_corr_filename}\")\n",
    "        \n",
    "        # Save full correlation matrix\n",
    "        corr_matrix_filename = 'Pooled_Correlation_Matrix_LightGBM.csv'\n",
    "        pooled_corr.to_csv(corr_matrix_filename, index=True, encoding='utf-8')\n",
    "        print(f\"[OK] Full correlation matrix saved: {corr_matrix_filename}\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Task 6: Output Summary\n",
    "        # ============================================================================\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Task 6: Output Summary\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\n[OK] Correlation Matrix Analysis completed!\")\n",
    "        print(f\"  Processed {M} MICE datasets\")\n",
    "        print(f\"  Analyzed {len(continuous_features)} continuous features\")\n",
    "        print(f\"  Generated correlation heatmap\")\n",
    "        print(f\"  Extracted correlations with target variable\")\n",
    "        \n",
    "        import os\n",
    "        files = [\n",
    "            ('Correlation_Matrix_LightGBM.png', 'Correlation Heatmap'),\n",
    "            ('Pooled_Correlation_Matrix_LightGBM.csv', 'Full Correlation Matrix'),\n",
    "            ('Correlation_with_Target_LightGBM.csv', 'Correlations with Target')\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nGenerated Files:\")\n",
    "        for filename, description in files:\n",
    "            if os.path.exists(filename):\n",
    "                if filename.endswith('.png'):\n",
    "                    file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "                    print(f\"  {filename}\")\n",
    "                    print(f\"    Description: {description}\")\n",
    "                    print(f\"    Size: {file_size:.2f} MB\")\n",
    "                else:\n",
    "                    file_size = os.path.getsize(filename) / 1024\n",
    "                    print(f\"  {filename}\")\n",
    "                    print(f\"    Description: {description}\")\n",
    "                    print(f\"    Size: {file_size:.2f} KB\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"[OK] Appendix - Correlation Matrix Analysis completed!\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n[ERROR] Could not compute correlation matrices from MICE datasets\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n[ERROR] Cannot proceed: MICE datasets not loaded successfully.\")\n",
    "    print(f\"  Expected {M} datasets, but loaded {len(mice_X_datasets)} X datasets and {len(mice_Y_datasets)} Y datasets\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
